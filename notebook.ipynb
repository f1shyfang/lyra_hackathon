{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Embeddings + XGBoost Engagement Prediction Model\n",
    "\n",
    "This notebook trains an XGBoost regression model to predict post engagement metrics using:\n",
    "- **LLM embeddings** (Google Gemini) for post text semantic understanding\n",
    "- **Persona metadata** (job role, affiliation, account age)\n",
    "- **Context metadata** (audience size, baseline engagement, time window)\n",
    "\n",
    "**Target Variables:**\n",
    "- % positive reactions\n",
    "- % negative reactions  \n",
    "- Comment sentiment distribution\n",
    "- Engagement velocity (early vs late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Google AI for embeddings\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    GEMINI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: google-generativeai not installed. Install with: pip install google-generativeai\")\n",
    "    GEMINI_AVAILABLE = False\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_linkedin_post_data(posts: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Clean and validate LinkedIn post data.\n",
    "    \n",
    "    Args:\n",
    "        posts: List of post dictionaries from JSON files\n",
    "        \n",
    "    Returns:\n",
    "        List of cleaned post dictionaries\n",
    "    \"\"\"\n",
    "    cleaned_posts = []\n",
    "    seen_urns = set()\n",
    "    \n",
    "    issues = {\n",
    "        'missing_text': 0,\n",
    "        'empty_text': 0,\n",
    "        'missing_stats': 0,\n",
    "        'missing_author': 0,\n",
    "        'missing_timestamp': 0,\n",
    "        'invalid_stats': 0,\n",
    "        'missing_source_company': 0,\n",
    "        'duplicate_urns': 0\n",
    "    }\n",
    "    \n",
    "    for post in posts:\n",
    "        # Check for duplicates by URN\n",
    "        urn = post.get('activity_urn') or post.get('full_urn')\n",
    "        if urn and urn in seen_urns:\n",
    "            issues['duplicate_urns'] += 1\n",
    "            continue\n",
    "        if urn:\n",
    "            seen_urns.add(urn)\n",
    "        \n",
    "        # Validate required fields\n",
    "        if 'text' not in post or not post['text']:\n",
    "            issues['missing_text'] += 1\n",
    "            continue\n",
    "        \n",
    "        if not post['text'].strip():\n",
    "            issues['empty_text'] += 1\n",
    "            continue\n",
    "        \n",
    "        if 'stats' not in post or not post['stats']:\n",
    "            issues['missing_stats'] += 1\n",
    "            continue\n",
    "        \n",
    "        if 'author' not in post or not post['author']:\n",
    "            issues['missing_author'] += 1\n",
    "            continue\n",
    "        \n",
    "        if 'posted_at' not in post or not post['posted_at'] or not post['posted_at'].get('timestamp'):\n",
    "            issues['missing_timestamp'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Validate and normalize stats\n",
    "        stats = post['stats']\n",
    "        if not isinstance(stats, dict):\n",
    "            issues['invalid_stats'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Ensure all stat fields exist with defaults\n",
    "        required_stats = ['total_reactions', 'like', 'love', 'celebrate', 'support', 'insight', 'comments', 'reposts']\n",
    "        for stat in required_stats:\n",
    "            if stat not in stats:\n",
    "                stats[stat] = 0\n",
    "        \n",
    "        # Ensure source_company exists\n",
    "        if 'source_company' not in post or not post['source_company']:\n",
    "            issues['missing_source_company'] += 1\n",
    "            # Try to infer from author name\n",
    "            if post.get('author', {}).get('name'):\n",
    "                post['source_company'] = post['author']['name'].lower()\n",
    "            else:\n",
    "                post['source_company'] = 'unknown'\n",
    "        \n",
    "        # Clean the post\n",
    "        cleaned_post = {\n",
    "            'activity_urn': post.get('activity_urn'),\n",
    "            'full_urn': post.get('full_urn'),\n",
    "            'post_url': post.get('post_url'),\n",
    "            'text': post['text'].strip(),  # Trim whitespace\n",
    "            'posted_at': post['posted_at'],\n",
    "            'post_language_code': post.get('post_language_code', 'en'),\n",
    "            'post_type': post.get('post_type', 'regular'),\n",
    "            'author': post['author'],\n",
    "            'stats': stats,\n",
    "            'media': post.get('media'),\n",
    "            'source_company': post['source_company']\n",
    "        }\n",
    "        \n",
    "        # Only include document field if it's not null\n",
    "        if post.get('document') is not None:\n",
    "            cleaned_post['document'] = post['document']\n",
    "        \n",
    "        cleaned_posts.append(cleaned_post)\n",
    "    \n",
    "    # Print cleaning summary\n",
    "    if any(count > 0 for count in issues.values()):\n",
    "        print(\"Data cleaning issues found:\")\n",
    "        for issue, count in issues.items():\n",
    "            if count > 0:\n",
    "                print(f\"  - {issue}: {count}\")\n",
    "    \n",
    "    print(f\"✓ Cleaned {len(cleaned_posts)}/{len(posts)} posts\")\n",
    "    return cleaned_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up API keys and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: Yes (length: 39)\n",
      "✓ Gemini API configured\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "# Load API key from .env file (loaded via dotenv in imports cell)\n",
    "GEMINI_API_KEY = \"\"\n",
    "print(f\"API Key loaded: {'Yes' if GEMINI_API_KEY else 'No'} (length: {len(GEMINI_API_KEY)})\")\n",
    "EMBEDDING_MODEL = 'models/gemini-embedding-001'\n",
    "DATA_PATH = 'data/'  # Path to your training data directory, or None to simulate\n",
    "\n",
    "# Configure Gemini API if available\n",
    "if GEMINI_AVAILABLE and GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"✓ Gemini API configured\")\n",
    "else:\n",
    "    print(\"⚠ Gemini API not configured. Embeddings will be simulated.\")\n",
    "    if not GEMINI_API_KEY:\n",
    "        print(\"   Tip: Make sure GEMINI_API_KEY is set in your .env file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "Functions to load and transform LinkedIn post data from JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "def load_linkedin_json_data(data_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and transform LinkedIn post JSON files from the data folder.\n",
    "    \n",
    "    Args:\n",
    "        data_folder: Path to folder containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with transformed LinkedIn post data matching expected format\n",
    "    \"\"\"\n",
    "    # Find all company post JSON files\n",
    "    json_pattern = os.path.join(data_folder, '*company-posts*.json')\n",
    "    json_files = glob.glob(json_pattern)\n",
    "    \n",
    "    if not json_files:\n",
    "        raise ValueError(f\"No company post JSON files found in {data_folder}\")\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON file(s) to load...\")\n",
    "    \n",
    "    all_posts = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        print(f\"Loading {os.path.basename(json_file)}...\")\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            posts = json.load(f)\n",
    "        \n",
    "        if not isinstance(posts, list):\n",
    "            posts = [posts]\n",
    "        \n",
    "        print(f\"  Found {len(posts)} posts\")\n",
    "        \n",
    "        # Clean the posts before adding to all_posts\n",
    "        cleaned_posts = clean_linkedin_post_data(posts)\n",
    "        all_posts.extend(cleaned_posts)\n",
    "    \n",
    "    print(f\"Total posts loaded and cleaned: {len(all_posts)}\")\n",
    "    print(\"Transforming data...\")\n",
    "    \n",
    "    transformed_data = []\n",
    "    \n",
    "    for post in all_posts:\n",
    "        # Skip posts without required fields\n",
    "        if not post.get('text') or not post.get('stats'):\n",
    "            continue\n",
    "        \n",
    "        # Extract basic fields\n",
    "        post_text = post.get('text', '')\n",
    "        stats = post.get('stats', {})\n",
    "        author = post.get('author', {})\n",
    "        posted_at = post.get('posted_at', {})\n",
    "        \n",
    "        # Calculate engagement metrics\n",
    "        total_reactions = stats.get('total_reactions', 0)\n",
    "        like_count = stats.get('like', 0)\n",
    "        love_count = stats.get('love', 0)\n",
    "        celebrate_count = stats.get('celebrate', 0)\n",
    "        support_count = stats.get('support', 0)\n",
    "        insight_count = stats.get('insight', 0)\n",
    "        comments_count = stats.get('comments', 0)\n",
    "        reposts_count = stats.get('reposts', 0)\n",
    "        \n",
    "        # Calculate pct_positive (positive reactions / total reactions * 100)\n",
    "        positive_reactions = like_count + love_count + celebrate_count + support_count\n",
    "        if total_reactions > 0:\n",
    "            pct_positive = (positive_reactions / total_reactions) * 100\n",
    "            pct_negative = (insight_count / total_reactions) * 100\n",
    "        else:\n",
    "            pct_positive = 0.0\n",
    "            pct_negative = 0.0\n",
    "        \n",
    "        # Audience size from follower count\n",
    "        audience_size = author.get('follower_count', 0)\n",
    "        if audience_size == 0:\n",
    "            audience_size = 1000  # Default minimum\n",
    "        \n",
    "        # Calculate baseline engagement\n",
    "        total_engagement = total_reactions + comments_count + reposts_count\n",
    "        baseline_engagement = total_engagement / max(audience_size, 1)\n",
    "        \n",
    "        # Calculate comment sentiment distribution (normalized -1 to 1)\n",
    "        if total_reactions > 0:\n",
    "            comment_sentiment_dist = (pct_positive - pct_negative) / 100\n",
    "            comment_sentiment_dist = np.clip(comment_sentiment_dist, -1, 1)\n",
    "        else:\n",
    "            comment_sentiment_dist = 0.0\n",
    "        \n",
    "        # Extract time window from timestamp\n",
    "        timestamp = posted_at.get('timestamp', 0)\n",
    "        if timestamp > 0:\n",
    "            # Convert timestamp (milliseconds) to datetime\n",
    "            post_datetime = datetime.fromtimestamp(timestamp / 1000)\n",
    "            hour = post_datetime.hour\n",
    "            weekday = post_datetime.weekday()  # 0 = Monday, 6 = Sunday\n",
    "            \n",
    "            if weekday >= 5:  # Saturday or Sunday\n",
    "                time_window = 'weekend'\n",
    "            elif hour < 12:\n",
    "                time_window = 'morning'\n",
    "            elif hour < 17:\n",
    "                time_window = 'afternoon'\n",
    "            else:\n",
    "                time_window = 'evening'\n",
    "        else:\n",
    "            time_window = 'afternoon'  # Default\n",
    "        \n",
    "        # Extract affiliation from source_company\n",
    "        affiliation = post.get('source_company', 'Unknown')\n",
    "        if affiliation and isinstance(affiliation, str):\n",
    "            affiliation = affiliation.capitalize()\n",
    "        else:\n",
    "            affiliation = 'Unknown'\n",
    "        \n",
    "        # Infer job_role from company name or use default\n",
    "        # For company posts, we'll use a default role based on the company\n",
    "        company_name = affiliation.lower()\n",
    "        if 'google' in company_name or 'microsoft' in company_name or 'apple' in company_name:\n",
    "            job_role = 'Software Engineer'\n",
    "        elif 'meta' in company_name or 'facebook' in company_name:\n",
    "            job_role = 'Product Manager'\n",
    "        else:\n",
    "            job_role = 'Product Manager'  # Default for company posts\n",
    "        \n",
    "        # Account age - use default since we don't have account creation date\n",
    "        # For company accounts, use a reasonable default (e.g., 5 years)\n",
    "        account_age = 1825  # 5 years in days\n",
    "        \n",
    "        # Calculate engagement velocity\n",
    "        # Estimate based on post age and engagement rate\n",
    "        # Newer posts with high engagement = high velocity\n",
    "        if timestamp > 0:\n",
    "            # Calculate post age in days\n",
    "            current_time = datetime.now().timestamp() * 1000\n",
    "            post_age_days = (current_time - timestamp) / (1000 * 60 * 60 * 24)\n",
    "            \n",
    "            # High engagement rate + recent post = high velocity\n",
    "            engagement_rate = baseline_engagement\n",
    "            if post_age_days < 1:  # Less than 1 day old\n",
    "                velocity_factor = 0.9\n",
    "            elif post_age_days < 7:  # Less than 1 week old\n",
    "                velocity_factor = 0.7\n",
    "            else:\n",
    "                velocity_factor = 0.5\n",
    "            \n",
    "            engagement_velocity = min(engagement_rate * 10 * velocity_factor, 1.0)\n",
    "            engagement_velocity = max(engagement_velocity, 0.0)\n",
    "        else:\n",
    "            engagement_velocity = 0.5  # Default\n",
    "        \n",
    "        transformed_data.append({\n",
    "            'post_text': post_text,\n",
    "            'job_role': job_role,\n",
    "            'affiliation': affiliation,\n",
    "            'account_age': account_age,\n",
    "            'audience_size': audience_size,\n",
    "            'baseline_engagement': baseline_engagement,\n",
    "            'time_window': time_window,\n",
    "            'pct_positive': pct_positive,\n",
    "            'pct_negative': pct_negative,\n",
    "            'comment_sentiment_dist': comment_sentiment_dist,\n",
    "            'engagement_velocity': engagement_velocity\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(transformed_data)\n",
    "    print(f\"✓ Transformed {len(df)} posts\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 JSON file(s) to convert...\n",
      "\n",
      "✓ Conversion complete!\n"
     ]
    }
   ],
   "source": [
    "# Convert all JSON files to CSV\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "data_folder = 'data'\n",
    "json_pattern = os.path.join(data_folder, '*.json')\n",
    "json_files = glob.glob(json_pattern)\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON file(s) to convert...\\n\")\n",
    "\n",
    "for json_file in sorted(json_files):\n",
    "    try:\n",
    "        # Load and transform using existing function\n",
    "        print(f\"Processing {os.path.basename(json_file)}...\")\n",
    "        \n",
    "        # Load JSON\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            posts = json.load(f)\n",
    "        \n",
    "        if not isinstance(posts, list):\n",
    "            posts = [posts]\n",
    "        \n",
    "        print(f\"  Loaded {len(posts)} posts\")\n",
    "        \n",
    "        # Clean posts\n",
    "        cleaned_posts = clean_linkedin_post_data(posts)\n",
    "        print(f\"  Cleaned: {len(cleaned_posts)} posts\")\n",
    "        \n",
    "        # Transform to DataFrame using existing function\n",
    "        # We'll use a simplified version that processes one file\n",
    "        transformed_data = []\n",
    "        \n",
    "        for post in cleaned_posts:\n",
    "            if not post.get('text') or not post.get('stats'):\n",
    "                continue\n",
    "            \n",
    "            post_text = post.get('text', '')\n",
    "            stats = post.get('stats', {})\n",
    "            author = post.get('author', {})\n",
    "            posted_at = post.get('posted_at', {})\n",
    "            \n",
    "            total_reactions = stats.get('total_reactions', 0)\n",
    "            like_count = stats.get('like', 0)\n",
    "            love_count = stats.get('love', 0)\n",
    "            celebrate_count = stats.get('celebrate', 0)\n",
    "            support_count = stats.get('support', 0)\n",
    "            insight_count = stats.get('insight', 0)\n",
    "            comments_count = stats.get('comments', 0)\n",
    "            reposts_count = stats.get('reposts', 0)\n",
    "            \n",
    "            positive_reactions = like_count + love_count + celebrate_count + support_count\n",
    "            if total_reactions > 0:\n",
    "                pct_positive = (positive_reactions / total_reactions) * 100\n",
    "                pct_negative = (insight_count / total_reactions) * 100\n",
    "            else:\n",
    "                pct_positive = 0.0\n",
    "                pct_negative = 0.0\n",
    "            \n",
    "            audience_size = author.get('follower_count', 0)\n",
    "            if audience_size == 0:\n",
    "                audience_size = 1000\n",
    "            \n",
    "            total_engagement = total_reactions + comments_count + reposts_count\n",
    "            baseline_engagement = total_engagement / max(audience_size, 1)\n",
    "            \n",
    "            if total_reactions > 0:\n",
    "                comment_sentiment_dist = (pct_positive - pct_negative) / 100\n",
    "                comment_sentiment_dist = np.clip(comment_sentiment_dist, -1, 1)\n",
    "            else:\n",
    "                comment_sentiment_dist = 0.0\n",
    "            \n",
    "            timestamp = posted_at.get('timestamp', 0)\n",
    "            if timestamp > 0:\n",
    "                from datetime import datetime\n",
    "                post_datetime = datetime.fromtimestamp(timestamp / 1000)\n",
    "                hour = post_datetime.hour\n",
    "                weekday = post_datetime.weekday()\n",
    "                \n",
    "                if weekday >= 5:\n",
    "                    time_window = 'weekend'\n",
    "                elif hour < 12:\n",
    "                    time_window = 'morning'\n",
    "                elif hour < 17:\n",
    "                    time_window = 'afternoon'\n",
    "                else:\n",
    "                    time_window = 'evening'\n",
    "            else:\n",
    "                time_window = 'afternoon'\n",
    "            \n",
    "            affiliation = post.get('source_company', 'Unknown')\n",
    "            if affiliation and isinstance(affiliation, str):\n",
    "                affiliation = affiliation.capitalize()\n",
    "            else:\n",
    "                affiliation = 'Unknown'\n",
    "            \n",
    "            company_name = affiliation.lower()\n",
    "            if 'google' in company_name or 'microsoft' in company_name or 'apple' in company_name:\n",
    "                job_role = 'Software Engineer'\n",
    "            elif 'meta' in company_name or 'facebook' in company_name:\n",
    "                job_role = 'Product Manager'\n",
    "            else:\n",
    "                job_role = 'Product Manager'\n",
    "            \n",
    "            account_age = 1825\n",
    "            \n",
    "            if timestamp > 0:\n",
    "                from datetime import datetime\n",
    "                current_time = datetime.now().timestamp() * 1000\n",
    "                post_age_days = (current_time - timestamp) / (1000 * 60 * 60 * 24)\n",
    "                engagement_rate = baseline_engagement\n",
    "                if post_age_days < 1:\n",
    "                    velocity_factor = 0.9\n",
    "                elif post_age_days < 7:\n",
    "                    velocity_factor = 0.7\n",
    "                else:\n",
    "                    velocity_factor = 0.5\n",
    "                engagement_velocity = min(engagement_rate * 10 * velocity_factor, 1.0)\n",
    "                engagement_velocity = max(engagement_velocity, 0.0)\n",
    "            else:\n",
    "                engagement_velocity = 0.5\n",
    "            \n",
    "            transformed_data.append({\n",
    "                'post_text': post_text,\n",
    "                'job_role': job_role,\n",
    "                'affiliation': affiliation,\n",
    "                'account_age': account_age,\n",
    "                'audience_size': audience_size,\n",
    "                'baseline_engagement': baseline_engagement,\n",
    "                'time_window': time_window,\n",
    "                'pct_positive': pct_positive,\n",
    "                'pct_negative': pct_negative,\n",
    "                'comment_sentiment_dist': comment_sentiment_dist,\n",
    "                'engagement_velocity': engagement_velocity\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(transformed_data)\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_file = json_file.replace('.json', '.csv')\n",
    "        df.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "        print(f\"  ✓ Saved to {os.path.basename(csv_file)} ({len(df)} rows, {len(df.columns)} columns)\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\\n\")\n",
    "\n",
    "print(\"✓ Conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from CSV/JSON file or directory of JSON files.\n",
    "    \n",
    "    Expected columns:\n",
    "    - post_text: string\n",
    "    - job_role: string (persona metadata)\n",
    "    - affiliation: string (persona metadata)\n",
    "    - account_age: int (days, persona metadata)\n",
    "    - audience_size: int (context metadata)\n",
    "    - baseline_engagement: float (context metadata)\n",
    "    - time_window: string (context metadata)\n",
    "    - pct_positive: float (target, 0-100)\n",
    "    - pct_negative: float (target, 0-100)\n",
    "    - comment_sentiment_dist: float (target, -1 to 1)\n",
    "    - engagement_velocity: float (target, 0-1)\n",
    "    \"\"\"\n",
    "    if not data_path:\n",
    "        raise ValueError(\"data_path is required. Please provide a path to your data directory or file.\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise ValueError(f\"Data path does not exist: {data_path}\")\n",
    "    \n",
    "    # Check if it's a directory (for loading multiple JSON files)\n",
    "    if os.path.isdir(data_path):\n",
    "        print(f\"Loading data from directory: {data_path}\")\n",
    "        df = load_linkedin_json_data(data_path)\n",
    "        print(f\"✓ Loaded {len(df)} rows from {data_path}\")\n",
    "        return df\n",
    "    \n",
    "    # Otherwise, treat as a single file\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    if data_path.endswith('.csv'):\n",
    "        df = pd.read_csv(data_path)\n",
    "    elif data_path.endswith('.json'):\n",
    "        df = pd.read_json(data_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {data_path}\")\n",
    "    print(f\"✓ Loaded {len(df)} rows from {data_path}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No company post JSON files found in data/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-376263138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data from JSON files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_linkedin_json_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nDataset shape: {df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nFirst few rows:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3080344914.py\u001b[0m in \u001b[0;36mload_linkedin_json_data\u001b[0;34m(data_folder)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjson_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No company post JSON files found in {data_folder}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {len(json_files)} JSON file(s) to load...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No company post JSON files found in data/"
     ]
    }
   ],
   "source": [
    "# Load data from JSON files\n",
    "df = load_linkedin_json_data(DATA_PATH)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Load data from the `data/` folder containing JSON files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-930656506.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data cleaning and exploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset Info:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMissing values:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Data cleaning and exploration\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nTarget variable statistics:\")\n",
    "print(df[['pct_positive', 'pct_negative', 'comment_sentiment_dist', 'engagement_velocity']].describe())\n",
    "\n",
    "# Handle missing values (if any)\n",
    "df = df.dropna()\n",
    "\n",
    "# Basic text normalization (optional - embeddings handle this well)\n",
    "df['post_text_clean'] = df['post_text'].str.lower().str.strip()\n",
    "\n",
    "print(f\"\\n✓ Cleaned dataset: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Embedding Generation\n",
    "\n",
    "Generate text embeddings using Google Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text embeddings...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1542683768.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Generate embeddings for all post texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating text embeddings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post_text_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Embedding shape: {text_embeddings.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Embedding dimension: {text_embeddings.shape[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_embeddings(texts: List[str], model_name: str = EMBEDDING_MODEL, batch_size: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using Google Gemini API.\n",
    "    Includes batch processing, error handling, and fallback to simulated embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    if not GEMINI_AVAILABLE or not GEMINI_API_KEY:\n",
    "        print(\"⚠ Using simulated embeddings (random vectors)\")\n",
    "        # Generate random embeddings with same dimension as Gemini (768 for text-embedding-004, 768/1536/3072 for gemini-embedding-001)\n",
    "        embedding_dim = 768\n",
    "        for text in texts:\n",
    "            # Create deterministic \"embeddings\" based on text hash for reproducibility\n",
    "            np.random.seed(hash(text) % 2**32)\n",
    "            emb = np.random.randn(embedding_dim)\n",
    "            emb = emb / np.linalg.norm(emb)  # Normalize\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(texts)} texts using {model_name}...\")\n",
    "    \n",
    "    # Try different API formats (support both old and new Google AI SDK)\n",
    "    try:\n",
    "        # Try new API format (google-genai package)\n",
    "        try:\n",
    "            from google import genai as genai_new\n",
    "            client = genai_new.Client(api_key=GEMINI_API_KEY)\n",
    "            use_new_api = True\n",
    "        except:\n",
    "            use_new_api = False\n",
    "    except:\n",
    "        use_new_api = False\n",
    "    \n",
    "    # Process in batches to handle rate limits\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_embeddings = []\n",
    "        \n",
    "        for text in batch:\n",
    "            try:\n",
    "                if use_new_api:\n",
    "                    # New API format\n",
    "                    response = client.models.embed_content(\n",
    "                        model=model_name,\n",
    "                        contents=text\n",
    "                    )\n",
    "                    embedding = response.embeddings[0].values if hasattr(response.embeddings[0], 'values') else response.embeddings[0]\n",
    "                else:\n",
    "                    # Old API format (google-generativeai)\n",
    "                    result = genai.embed_content(\n",
    "                        model=model_name,\n",
    "                        content=text,\n",
    "                        task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "                    )\n",
    "                    # Handle different response formats\n",
    "                    if isinstance(result, dict):\n",
    "                        embedding = result.get('embedding', result.get('values', []))\n",
    "                    else:\n",
    "                        embedding = result.embedding if hasattr(result, 'embedding') else result\n",
    "                \n",
    "                batch_embeddings.append(embedding)\n",
    "                \n",
    "                # Small delay to avoid rate limits\n",
    "                time.sleep(0.1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error embedding text {i}: {str(e)}\")\n",
    "                # Fallback: use random embedding\n",
    "                embedding_dim = 768\n",
    "                np.random.seed(hash(text) % 2**32)\n",
    "                emb = np.random.randn(embedding_dim)\n",
    "                emb = emb / np.linalg.norm(emb)\n",
    "                batch_embeddings.append(emb)\n",
    "        \n",
    "        embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        if (i + batch_size) % 50 == 0:\n",
    "            print(f\"  Processed {min(i + batch_size, len(texts))}/{len(texts)} texts...\")\n",
    "    \n",
    "    print(f\"✓ Generated {len(embeddings)} embeddings\")\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings for all post texts\n",
    "print(\"Generating text embeddings...\")\n",
    "text_embeddings = generate_embeddings(df['post_text_clean'].tolist())\n",
    "print(f\"Embedding shape: {text_embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {text_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text embeddings...\n",
      "⚠ Using simulated embeddings (random vectors)\n",
      "Embedding shape: (500, 768)\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "def generate_embeddings(texts: List[str], model_name: str = EMBEDDING_MODEL, batch_size: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using Google Gemini API.\n",
    "    Includes batch processing, error handling, and fallback to simulated embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    if not GEMINI_AVAILABLE or not GEMINI_API_KEY:\n",
    "        print(\"⚠ Using simulated embeddings (random vectors)\")\n",
    "        # Generate random embeddings with same dimension as Gemini (768 for text-embedding-004, 768/1536/3072 for gemini-embedding-001)\n",
    "        embedding_dim = 768\n",
    "        for text in texts:\n",
    "            # Create deterministic \"embeddings\" based on text hash for reproducibility\n",
    "            np.random.seed(hash(text) % 2**32)\n",
    "            emb = np.random.randn(embedding_dim)\n",
    "            emb = emb / np.linalg.norm(emb)  # Normalize\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(texts)} texts using {model_name}...\")\n",
    "    \n",
    "    # Try different API formats (support both old and new Google AI SDK)\n",
    "    try:\n",
    "        # Try new API format (google-genai package)\n",
    "        try:\n",
    "            from google import genai as genai_new\n",
    "            client = genai_new.Client(api_key=GEMINI_API_KEY)\n",
    "            use_new_api = True\n",
    "        except:\n",
    "            use_new_api = False\n",
    "    except:\n",
    "        use_new_api = False\n",
    "    \n",
    "    # Process in batches to handle rate limits\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_embeddings = []\n",
    "        \n",
    "        for text in batch:\n",
    "            try:\n",
    "                if use_new_api:\n",
    "                    # New API format\n",
    "                    response = client.models.embed_content(\n",
    "                        model=model_name,\n",
    "                        contents=text\n",
    "                    )\n",
    "                    embedding = response.embeddings[0].values if hasattr(response.embeddings[0], 'values') else response.embeddings[0]\n",
    "                else:\n",
    "                    # Old API format (google-generativeai)\n",
    "                    result = genai.embed_content(\n",
    "                        model=model_name,\n",
    "                        content=text,\n",
    "                        task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "                    )\n",
    "                    # Handle different response formats\n",
    "                    if isinstance(result, dict):\n",
    "                        embedding = result.get('embedding', result.get('values', []))\n",
    "                    else:\n",
    "                        embedding = result.embedding if hasattr(result, 'embedding') else result\n",
    "                \n",
    "                batch_embeddings.append(embedding)\n",
    "                \n",
    "                # Small delay to avoid rate limits\n",
    "                time.sleep(0.1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error embedding text {i}: {str(e)}\")\n",
    "                # Fallback: use random embedding\n",
    "                embedding_dim = 768\n",
    "                np.random.seed(hash(text) % 2**32)\n",
    "                emb = np.random.randn(embedding_dim)\n",
    "                emb = emb / np.linalg.norm(emb)\n",
    "                batch_embeddings.append(emb)\n",
    "        \n",
    "        embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        if (i + batch_size) % 50 == 0:\n",
    "            print(f\"  Processed {min(i + batch_size, len(texts))}/{len(texts)} texts...\")\n",
    "    \n",
    "    print(f\"✓ Generated {len(embeddings)} embeddings\")\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings for all post texts\n",
    "print(\"Generating text embeddings...\")\n",
    "text_embeddings = generate_embeddings(df['post_text_clean'].tolist())\n",
    "print(f\"Embedding shape: {text_embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {text_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Combine text embeddings with persona and context metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (500, 788)\n",
      "Target matrix shape: (500, 4)\n",
      "\n",
      "Feature breakdown:\n",
      "  - Text embeddings: 768 dimensions\n",
      "  - Categorical features: 17 dimensions\n",
      "  - Numerical features: 3 dimensions\n",
      "  - Total features: 788 dimensions\n",
      "\n",
      "Target variables: pct_positive, pct_negative, comment_sentiment_dist, engagement_velocity\n"
     ]
    }
   ],
   "source": [
    "def prepare_features(df: pd.DataFrame, text_embeddings: np.ndarray) -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Prepare feature matrix by combining embeddings with metadata.\n",
    "    Returns: (X, y, feature_info)\n",
    "    \"\"\"\n",
    "    # Encode categorical features\n",
    "    le_job = LabelEncoder()\n",
    "    le_affiliation = LabelEncoder()\n",
    "    le_time = LabelEncoder()\n",
    "    \n",
    "    job_encoded = le_job.fit_transform(df['job_role'])\n",
    "    affiliation_encoded = le_affiliation.fit_transform(df['affiliation'])\n",
    "    time_encoded = le_time.fit_transform(df['time_window'])\n",
    "    \n",
    "    # One-hot encode categorical features for better representation\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    categorical_features = ohe.fit_transform(\n",
    "        df[['job_role', 'affiliation', 'time_window']]\n",
    "    )\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_features = scaler.fit_transform(\n",
    "        df[['account_age', 'audience_size', 'baseline_engagement']]\n",
    "    )\n",
    "    \n",
    "    # Combine all features: embeddings + categorical + numerical\n",
    "    X = np.hstack([\n",
    "        text_embeddings,  # Text embeddings (e.g., 768 dims)\n",
    "        categorical_features,  # One-hot encoded categoricals\n",
    "        numerical_features  # Normalized numerical features\n",
    "    ])\n",
    "    \n",
    "    # Extract target variables\n",
    "    y = df[['pct_positive', 'pct_negative', 'comment_sentiment_dist', 'engagement_velocity']].values\n",
    "    \n",
    "    feature_info = {\n",
    "        'embedding_dim': text_embeddings.shape[1],\n",
    "        'categorical_dim': categorical_features.shape[1],\n",
    "        'numerical_dim': numerical_features.shape[1],\n",
    "        'total_dim': X.shape[1],\n",
    "        'label_encoders': {\n",
    "            'job_role': le_job,\n",
    "            'affiliation': le_affiliation,\n",
    "            'time_window': le_time\n",
    "        },\n",
    "        'one_hot_encoder': ohe,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    \n",
    "    return X, y, feature_info\n",
    "\n",
    "# Prepare features\n",
    "X, y, feature_info = prepare_features(df, text_embeddings)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target matrix shape: {y.shape}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - Text embeddings: {feature_info['embedding_dim']} dimensions\")\n",
    "print(f\"  - Categorical features: {feature_info['categorical_dim']} dimensions\")\n",
    "print(f\"  - Numerical features: {feature_info['numerical_dim']} dimensions\")\n",
    "print(f\"  - Total features: {feature_info['total_dim']} dimensions\")\n",
    "print(f\"\\nTarget variables: pct_positive, pct_negative, comment_sentiment_dist, engagement_velocity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (500, 788)\n",
      "Target matrix shape: (500, 4)\n",
      "\n",
      "Feature breakdown:\n",
      "  - Text embeddings: 768 dimensions\n",
      "  - Categorical features: 17 dimensions\n",
      "  - Numerical features: 3 dimensions\n",
      "  - Total features: 788 dimensions\n",
      "\n",
      "Target variables: pct_positive, pct_negative, comment_sentiment_dist, engagement_velocity\n"
     ]
    }
   ],
   "source": [
    "def prepare_features(df: pd.DataFrame, text_embeddings: np.ndarray) -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Prepare feature matrix by combining embeddings with metadata.\n",
    "    Returns: (X, y, feature_info)\n",
    "    \"\"\"\n",
    "    # Encode categorical features\n",
    "    le_job = LabelEncoder()\n",
    "    le_affiliation = LabelEncoder()\n",
    "    le_time = LabelEncoder()\n",
    "    \n",
    "    job_encoded = le_job.fit_transform(df['job_role'])\n",
    "    affiliation_encoded = le_affiliation.fit_transform(df['affiliation'])\n",
    "    time_encoded = le_time.fit_transform(df['time_window'])\n",
    "    \n",
    "    # One-hot encode categorical features for better representation\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    categorical_features = ohe.fit_transform(\n",
    "        df[['job_role', 'affiliation', 'time_window']]\n",
    "    )\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_features = scaler.fit_transform(\n",
    "        df[['account_age', 'audience_size', 'baseline_engagement']]\n",
    "    )\n",
    "    \n",
    "    # Combine all features: embeddings + categorical + numerical\n",
    "    X = np.hstack([\n",
    "        text_embeddings,  # Text embeddings (e.g., 768 dims)\n",
    "        categorical_features,  # One-hot encoded categoricals\n",
    "        numerical_features  # Normalized numerical features\n",
    "    ])\n",
    "    \n",
    "    # Extract target variables\n",
    "    y = df[['pct_positive', 'pct_negative', 'comment_sentiment_dist', 'engagement_velocity']].values\n",
    "    \n",
    "    feature_info = {\n",
    "        'embedding_dim': text_embeddings.shape[1],\n",
    "        'categorical_dim': categorical_features.shape[1],\n",
    "        'numerical_dim': numerical_features.shape[1],\n",
    "        'total_dim': X.shape[1],\n",
    "        'label_encoders': {\n",
    "            'job_role': le_job,\n",
    "            'affiliation': le_affiliation,\n",
    "            'time_window': le_time\n",
    "        },\n",
    "        'one_hot_encoder': ohe,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    \n",
    "    return X, y, feature_info\n",
    "\n",
    "# Prepare features\n",
    "X, y, feature_info = prepare_features(df, text_embeddings)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target matrix shape: {y.shape}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - Text embeddings: {feature_info['embedding_dim']} dimensions\")\n",
    "print(f\"  - Categorical features: {feature_info['categorical_dim']} dimensions\")\n",
    "print(f\"  - Numerical features: {feature_info['numerical_dim']} dimensions\")\n",
    "print(f\"  - Total features: {feature_info['total_dim']} dimensions\")\n",
    "print(f\"\\nTarget variables: pct_positive, pct_negative, comment_sentiment_dist, engagement_velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Train XGBoost multi-output regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 400 samples\n",
      "Test set: 100 samples\n",
      "\n",
      "Training model...\n",
      "✓ Model training complete\n",
      "\n",
      "Predictions shape: (100, 4)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Create XGBoost model with MultiOutputRegressor wrapper\n",
    "# This trains one XGBoost model per target variable\n",
    "base_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model = MultiOutputRegressor(base_model)\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"✓ Model training complete\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"\\nPredictions shape: {y_test_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Calculate metrics and analyze performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Metrics:\n",
      "                target      MAE     RMSE       R²  Correlation\n",
      "          pct_positive 1.025742 1.336643 0.986629     0.995914\n",
      "          pct_negative 0.399778 0.526746 0.986503     0.996368\n",
      "comment_sentiment_dist 0.009412 0.012375 0.989928     0.996738\n",
      "   engagement_velocity 0.009688 0.013141 0.986181     0.996036\n",
      "\n",
      "Test Set Metrics:\n",
      "                target      MAE      RMSE        R²  Correlation\n",
      "          pct_positive 9.738217 12.188792  0.015307     0.281487\n",
      "          pct_negative 3.924400  4.847199 -0.437728    -0.059668\n",
      "comment_sentiment_dist 0.109485  0.135864 -0.064070     0.205078\n",
      "   engagement_velocity 0.122085  0.144606 -0.050543     0.159297\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, target_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for each target variable.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, target_name in enumerate(target_names):\n",
    "        y_true_i = y_true[:, i]\n",
    "        y_pred_i = y_pred[:, i]\n",
    "        \n",
    "        mae = mean_absolute_error(y_true_i, y_pred_i)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_i, y_pred_i))\n",
    "        r2 = r2_score(y_true_i, y_pred_i)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation = np.corrcoef(y_true_i, y_pred_i)[0, 1]\n",
    "        \n",
    "        results.append({\n",
    "            'target': target_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2,\n",
    "            'Correlation': correlation\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate on train and test sets\n",
    "target_names = ['pct_positive', 'pct_negative', 'comment_sentiment_dist', 'engagement_velocity']\n",
    "\n",
    "train_metrics = evaluate_model(y_train, y_train_pred, target_names)\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, target_names)\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(train_metrics.to_string(index=False))\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(test_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Metrics:\n",
      "                target      MAE     RMSE       R²  Correlation\n",
      "          pct_positive 1.025742 1.336643 0.986629     0.995914\n",
      "          pct_negative 0.399778 0.526746 0.986503     0.996368\n",
      "comment_sentiment_dist 0.009412 0.012375 0.989928     0.996738\n",
      "   engagement_velocity 0.009688 0.013141 0.986181     0.996036\n",
      "\n",
      "Test Set Metrics:\n",
      "                target      MAE      RMSE        R²  Correlation\n",
      "          pct_positive 9.738217 12.188792  0.015307     0.281487\n",
      "          pct_negative 3.924400  4.847199 -0.437728    -0.059668\n",
      "comment_sentiment_dist 0.109485  0.135864 -0.064070     0.205078\n",
      "   engagement_velocity 0.122085  0.144606 -0.050543     0.159297\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, target_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for each target variable.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, target_name in enumerate(target_names):\n",
    "        y_true_i = y_true[:, i]\n",
    "        y_pred_i = y_pred[:, i]\n",
    "        \n",
    "        mae = mean_absolute_error(y_true_i, y_pred_i)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_i, y_pred_i))\n",
    "        r2 = r2_score(y_true_i, y_pred_i)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation = np.corrcoef(y_true_i, y_pred_i)[0, 1]\n",
    "        \n",
    "        results.append({\n",
    "            'target': target_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2,\n",
    "            'Correlation': correlation\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate on train and test sets\n",
    "target_names = ['pct_positive', 'pct_negative', 'comment_sentiment_dist', 'engagement_velocity']\n",
    "\n",
    "train_metrics = evaluate_model(y_train, y_train_pred, target_names)\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, target_names)\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(train_metrics.to_string(index=False))\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(test_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Quantification\n",
    "\n",
    "Estimate prediction uncertainty and identify failure modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainty Metrics (Test Set):\n",
      "                target  mean_abs_error  std_abs_error  max_abs_error  mean_rel_error_pct  high_uncertainty_threshold\n",
      "          pct_positive        9.738217       7.330333      37.036650           29.637312                   20.164888\n",
      "          pct_negative        3.924400       2.845069      13.191705           95.163070                    8.104736\n",
      "comment_sentiment_dist        0.109485       0.080448       0.443569           64.028137                    0.217105\n",
      "   engagement_velocity        0.122085       0.077499       0.326937           32.904771                    0.239766\n",
      "\n",
      "Identifying high-uncertainty predictions...\n",
      "High-uncertainty predictions: 27 / 100 (27.0%)\n",
      "\n",
      "These are potential failure modes where the model is less confident.\n"
     ]
    }
   ],
   "source": [
    "def calculate_uncertainty(y_true: np.ndarray, y_pred: np.ndarray, target_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate prediction errors and uncertainty metrics.\n",
    "    \"\"\"\n",
    "    uncertainty_data = []\n",
    "    \n",
    "    for i, target_name in enumerate(target_names):\n",
    "        y_true_i = y_true[:, i]\n",
    "        y_pred_i = y_pred[:, i]\n",
    "        \n",
    "        # Absolute error\n",
    "        abs_error = np.abs(y_true_i - y_pred_i)\n",
    "        \n",
    "        # Relative error (percentage)\n",
    "        # Avoid division by zero\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            rel_error = np.where(\n",
    "                y_true_i != 0,\n",
    "                abs_error / np.abs(y_true_i) * 100,\n",
    "                abs_error\n",
    "            )\n",
    "        \n",
    "        uncertainty_data.append({\n",
    "            'target': target_name,\n",
    "            'mean_abs_error': np.mean(abs_error),\n",
    "            'std_abs_error': np.std(abs_error),\n",
    "            'max_abs_error': np.max(abs_error),\n",
    "            'mean_rel_error_pct': np.mean(rel_error[rel_error != np.inf]),\n",
    "            'high_uncertainty_threshold': np.percentile(abs_error, 90)  # 90th percentile\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(uncertainty_data)\n",
    "\n",
    "# Calculate uncertainty metrics\n",
    "test_uncertainty = calculate_uncertainty(y_test, y_test_pred, target_names)\n",
    "print(\"Uncertainty Metrics (Test Set):\")\n",
    "print(test_uncertainty.to_string(index=False))\n",
    "\n",
    "# Identify high-uncertainty predictions (potential failure modes)\n",
    "print(\"\\nIdentifying high-uncertainty predictions...\")\n",
    "abs_errors = np.abs(y_test - y_test_pred)\n",
    "high_uncertainty_thresholds = test_uncertainty['high_uncertainty_threshold'].values\n",
    "\n",
    "high_uncertainty_mask = np.any(\n",
    "    abs_errors > high_uncertainty_thresholds.reshape(1, -1),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"High-uncertainty predictions: {np.sum(high_uncertainty_mask)} / {len(y_test)} ({np.sum(high_uncertainty_mask)/len(y_test)*100:.1f}%)\")\n",
    "print(\"\\nThese are potential failure modes where the model is less confident.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
