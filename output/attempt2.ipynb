{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfb9897",
   "metadata": {},
   "source": [
    "# LinkedIn PR Sentiment Classification Model\n",
    "\n",
    "Using Gemini LLM embeddings and XGBoost to predict whether a post will result in positive or negative PR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74fe4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install -q google-generativeai xgboost pandas numpy scikit-learn vaderSentiment matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9519d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Gemini API\n",
    "import google.generativeai as genai\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bcdc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Gemini API configured\n"
     ]
    }
   ],
   "source": [
    "# Configure Gemini API\n",
    "# You'll need to set your API key here or as an environment variable\n",
    "GEMINI_API_KEY =  # Set your API key here or export GEMINI_API_KEY=\"your-key\"\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"âœ“ Gemini API configured\")\n",
    "else:\n",
    "    print(\"âš  Warning: GEMINI_API_KEY not set. You'll need to set it before generating embeddings.\")\n",
    "    print(\"  Set it with: export GEMINI_API_KEY='your-api-key'\")\n",
    "    print(\"  Or in the notebook: GEMINI_API_KEY = 'your-api-key'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487145b9",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data\n",
    "\n",
    "### Setup for Google Colab (if needed)\n",
    "\n",
    "**If using Google Colab**, run this cell first to upload your data files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78ad0445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”µ Running in Google Colab environment\n",
      "\n",
      "ðŸ“¤ Data files not found. Please upload them:\n",
      "   1. dataset_linkedin-company-posts_2025-12-14_07-04-03-569.json\n",
      "   2. dataset_linkedin-post-comments-replies-engagements-scraper-no-cookies_2025-12-14_04-25-32-524.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-004dc2fb-76f7-4976-b45a-2ecbedfb4bf9\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-004dc2fb-76f7-4976-b45a-2ecbedfb4bf9\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2182119089.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   2. dataset_linkedin-post-comments-replies-engagements-scraper-no-cookies_2025-12-14_04-25-32-524.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Google Colab Setup - Upload data files (OPTIONAL)\n",
    "# This cell checks if data files exist. If using Colab AND files don't exist, it will prompt for upload.\n",
    "# Otherwise, it will use existing local files.\n",
    "\n",
    "# Define filenames we need\n",
    "posts_filename = 'dataset_linkedin-company-posts_2025-12-14_07-04-03-569.json'\n",
    "comments_filename = 'dataset_linkedin-post-comments-replies-engagements-scraper-no-cookies_2025-12-14_04-25-32-524.json'\n",
    "\n",
    "# Check if files already exist locally\n",
    "local_paths_to_check = [\n",
    "    'data/',\n",
    "    '/home/micha/dev/lyra_hackathon/data/',\n",
    "    os.path.join(os.getcwd(), 'data/'),\n",
    "]\n",
    "\n",
    "files_exist_locally = False\n",
    "for path in local_paths_to_check:\n",
    "    if os.path.exists(os.path.join(path, posts_filename)) and os.path.exists(os.path.join(path, comments_filename)):\n",
    "        files_exist_locally = True\n",
    "        print(f\"âœ“ Data files found in: {path}\")\n",
    "        break\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"ðŸ”µ Running in Google Colab environment\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ðŸ’» Running in local environment\")\n",
    "\n",
    "# Only prompt for upload if in Colab AND files don't exist\n",
    "if IN_COLAB and not files_exist_locally:\n",
    "    from google.colab import files\n",
    "    print(\"\\nðŸ“¤ Data files not found. Please upload them:\")\n",
    "    print(\"   1. dataset_linkedin-company-posts_2025-12-14_07-04-03-569.json\")\n",
    "    print(\"   2. dataset_linkedin-post-comments-replies-engagements-scraper-no-cookies_2025-12-14_04-25-32-524.json\")\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if not os.path.exists('/content/data'):\n",
    "        os.makedirs('/content/data')\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        import shutil\n",
    "        shutil.move(filename, f'/content/data/{filename}')\n",
    "        print(f\"âœ“ Moved {filename} to /content/data/\")\n",
    "else:\n",
    "    if files_exist_locally:\n",
    "        print(\"âœ“ Using existing local data files - no upload needed\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Will attempt to locate files in next cell\")\n",
    "\n",
    "print(\"\\nâœ“ Setup complete - proceed to next cell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files - works for both Colab and local environments\n",
    "posts_filename = 'dataset_linkedin-company-posts_2025-12-14_07-04-03-569.json'\n",
    "comments_filename = 'dataset_linkedin-post-comments-replies-engagements-scraper-no-cookies_2025-12-14_04-25-32-524.json'\n",
    "\n",
    "# Define possible paths\n",
    "possible_paths = [\n",
    "    '/content/data/',  # Google Colab\n",
    "    'data/',  # Local relative\n",
    "    '/home/micha/dev/lyra_hackathon/data/',  # Local absolute\n",
    "    '/content/',  # Colab root\n",
    "    ''  # Current directory\n",
    "]\n",
    "\n",
    "# Find posts file\n",
    "posts_path = None\n",
    "for base_path in possible_paths:\n",
    "    test_path = os.path.join(base_path, posts_filename)\n",
    "    if os.path.exists(test_path):\n",
    "        posts_path = test_path\n",
    "        break\n",
    "\n",
    "if not posts_path:\n",
    "    raise FileNotFoundError(f\"Cannot find {posts_filename}. Please upload the file or check the path.\")\n",
    "\n",
    "print(f\"ðŸ“‚ Loading posts from: {posts_path}\")\n",
    "\n",
    "# Load posts file\n",
    "with open(posts_path, 'r', encoding='utf-8') as f:\n",
    "    posts_raw = json.load(f)\n",
    "\n",
    "print(f\"âœ“ Loaded {len(posts_raw)} posts\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "posts_df = pd.DataFrame(posts_raw)\n",
    "print(f\"  Posts DataFrame shape: {posts_df.shape}\")\n",
    "print(f\"  Columns: {list(posts_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56487c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comments file\n",
    "comments_path = None\n",
    "for base_path in possible_paths:\n",
    "    test_path = os.path.join(base_path, comments_filename)\n",
    "    if os.path.exists(test_path):\n",
    "        comments_path = test_path\n",
    "        break\n",
    "\n",
    "if not comments_path:\n",
    "    raise FileNotFoundError(f\"Cannot find {comments_filename}. Please upload the file or check the path.\")\n",
    "\n",
    "print(f\"ðŸ“‚ Loading comments from: {comments_path}\")\n",
    "\n",
    "# Load comments file\n",
    "with open(comments_path, 'r', encoding='utf-8') as f:\n",
    "    comments_raw = json.load(f)\n",
    "\n",
    "print(f\"âœ“ Loaded {len(comments_raw)} comments\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "comments_df = pd.DataFrame(comments_raw)\n",
    "print(f\"  Comments DataFrame shape: {comments_df.shape}\")\n",
    "print(f\"  Columns: {list(comments_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore posts structure\n",
    "print(\"=\" * 60)\n",
    "print(\"POSTS DATA EXPLORATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFirst post sample:\")\n",
    "print(f\"Text: {posts_df['text'].iloc[0][:200]}...\")\n",
    "print(f\"\\nPost types: {posts_df['post_type'].value_counts().to_dict()}\")\n",
    "print(f\"\\nEngagement stats:\")\n",
    "posts_df['stats'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc4a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten stats dictionary into columns\n",
    "stats_df = pd.json_normalize(posts_df['stats'])\n",
    "posts_df = pd.concat([posts_df.drop('stats', axis=1), stats_df], axis=1)\n",
    "\n",
    "print(\"Flattened engagement columns:\")\n",
    "print(posts_df[['total_reactions', 'like', 'comments', 'reposts']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63637f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten author and posted_at information\n",
    "author_df = pd.json_normalize(posts_df['author'])\n",
    "author_df.columns = ['author_' + col for col in author_df.columns]\n",
    "\n",
    "posted_at_df = pd.json_normalize(posts_df['posted_at'])\n",
    "posted_at_df.columns = ['posted_' + col for col in posted_at_df.columns]\n",
    "\n",
    "posts_df = pd.concat([\n",
    "    posts_df.drop(['author', 'posted_at'], axis=1),\n",
    "    author_df,\n",
    "    posted_at_df\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\nUpdated posts DataFrame shape: {posts_df.shape}\")\n",
    "print(f\"Key columns: {[c for c in posts_df.columns if c in ['text', 'activity_urn', 'total_reactions', 'comments', 'reposts', 'posted_date', 'author_name']]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore comments structure\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMMENTS DATA EXPLORATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFirst comment text: {comments_df['text'].iloc[0][:200]}...\")\n",
    "print(f\"\\nPost IDs in comments: {comments_df['post_input'].nunique()} unique posts\")\n",
    "print(f\"Comments per post: mean={comments_df.groupby('post_input').size().mean():.1f}, median={comments_df.groupby('post_input').size().median():.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1cd34",
   "metadata": {},
   "source": [
    "# Try to link comments to posts\n",
    "# Comments use post_input field which contains activity URN\n",
    "# Posts have activity_urn field\n",
    "\n",
    "# Extract activity URN from posts for matching\n",
    "posts_df['comment_match_key'] = posts_df['activity_urn']\n",
    "\n",
    "# Check if we can match\n",
    "print(f\"\\nSample post activity_urn: {posts_df['activity_urn'].iloc[0]}\")\n",
    "print(f\"Sample comment post_input: {comments_df['post_input'].iloc[0]}\")\n",
    "\n",
    "# Count matches\n",
    "matched_posts = posts_df[posts_df['activity_urn'].isin(comments_df['post_input'])]\n",
    "print(f\"\\nâœ“ Posts with comments in dataset: {len(matched_posts)} out of {len(posts_df)}\")\n",
    "print(f\"âœ“ {len(matched_posts)/len(posts_df)*100:.1f}% of posts have comments data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment analyzer\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to analyze comment sentiment\n",
    "def analyze_comment_sentiment(text):\n",
    "    \"\"\"Return sentiment score from -1 (negative) to 1 (positive)\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return 0\n",
    "    scores = sentiment_analyzer.polarity_scores(str(text))\n",
    "    return scores['compound']  # compound score ranges from -1 to 1\n",
    "\n",
    "# Test sentiment analyzer\n",
    "test_texts = [\n",
    "    \"This is amazing! I love it!\",\n",
    "    \"This is terrible and disappointing.\",\n",
    "    \"Interesting post, thanks for sharing.\"\n",
    "]\n",
    "for text in test_texts:\n",
    "    print(f\"Text: '{text}' => Sentiment: {analyze_comment_sentiment(text):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08856430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment for all comments\n",
    "print(\"Analyzing sentiment for all comments...\")\n",
    "comments_df['sentiment_score'] = comments_df['text'].apply(analyze_comment_sentiment)\n",
    "print(f\"âœ“ Sentiment analysis complete\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(f\"  Mean: {comments_df['sentiment_score'].mean():.3f}\")\n",
    "print(f\"  Median: {comments_df['sentiment_score'].median():.3f}\")\n",
    "print(f\"  Negative (<0): {(comments_df['sentiment_score'] < 0).sum()} ({(comments_df['sentiment_score'] < 0).sum()/len(comments_df)*100:.1f}%)\")\n",
    "print(f\"  Neutral (0): {(comments_df['sentiment_score'] == 0).sum()} ({(comments_df['sentiment_score'] == 0).sum()/len(comments_df)*100:.1f}%)\")\n",
    "print(f\"  Positive (>0): {(comments_df['sentiment_score'] > 0).sum()} ({(comments_df['sentiment_score'] > 0).sum()/len(comments_df)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8176347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate comment sentiment by post\n",
    "comment_sentiment_by_post = comments_df.groupby('post_input').agg({\n",
    "    'sentiment_score': ['mean', 'median', 'min', 'max', 'count'],\n",
    "    'text': 'count'  # number of comments\n",
    "}).reset_index()\n",
    "\n",
    "comment_sentiment_by_post.columns = ['activity_urn', 'avg_sentiment', 'median_sentiment', \n",
    "                                       'min_sentiment', 'max_sentiment', 'sentiment_count', 'num_comments_analyzed']\n",
    "\n",
    "print(f\"Aggregated sentiment for {len(comment_sentiment_by_post)} posts\")\n",
    "print(f\"\\nSample aggregated sentiment:\")\n",
    "comment_sentiment_by_post.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56da4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge comment sentiment with posts\n",
    "posts_with_sentiment = posts_df.merge(\n",
    "    comment_sentiment_by_post,\n",
    "    left_on='activity_urn',\n",
    "    right_on='activity_urn',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NaN for posts without comments (assume neutral)\n",
    "posts_with_sentiment['avg_sentiment'] = posts_with_sentiment['avg_sentiment'].fillna(0)\n",
    "posts_with_sentiment['median_sentiment'] = posts_with_sentiment['median_sentiment'].fillna(0)\n",
    "posts_with_sentiment['num_comments_analyzed'] = posts_with_sentiment['num_comments_analyzed'].fillna(0)\n",
    "\n",
    "print(f\"âœ“ Merged sentiment data\")\n",
    "print(f\"Posts with sentiment data: {len(posts_with_sentiment)}\")\n",
    "print(f\"Posts with comment sentiment: {(posts_with_sentiment['num_comments_analyzed'] > 0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7760df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total engagement score\n",
    "posts_with_sentiment['total_engagement'] = (\n",
    "    posts_with_sentiment['total_reactions'] + \n",
    "    posts_with_sentiment['comments'] * 2 +  # Comments worth more\n",
    "    posts_with_sentiment['reposts'] * 3     # Reposts worth even more\n",
    ")\n",
    "\n",
    "# Calculate positive reaction ratio\n",
    "positive_reactions = ['like', 'love', 'celebrate', 'support']\n",
    "posts_with_sentiment['positive_reaction_sum'] = 0\n",
    "for col in positive_reactions:\n",
    "    if col in posts_with_sentiment.columns:\n",
    "        posts_with_sentiment['positive_reaction_sum'] += posts_with_sentiment[col].fillna(0)\n",
    "\n",
    "posts_with_sentiment['positive_reaction_ratio'] = (\n",
    "    posts_with_sentiment['positive_reaction_sum'] / \n",
    "    (posts_with_sentiment['total_reactions'] + 1)  # Add 1 to avoid division by zero\n",
    ")\n",
    "\n",
    "print(\"Engagement metrics calculated:\")\n",
    "print(posts_with_sentiment[['total_engagement', 'positive_reaction_ratio', 'avg_sentiment']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate PR labels based on combined criteria\n",
    "# Calculate thresholds\n",
    "engagement_median = posts_with_sentiment['total_engagement'].median()\n",
    "engagement_q25 = posts_with_sentiment['total_engagement'].quantile(0.25)\n",
    "sentiment_threshold = 0.0  # Neutral point\n",
    "\n",
    "print(f\"Engagement median: {engagement_median}\")\n",
    "print(f\"Engagement Q25: {engagement_q25}\")\n",
    "print(f\"Sentiment threshold: {sentiment_threshold}\")\n",
    "\n",
    "# Label generation logic:\n",
    "# Positive PR: High engagement (above median) AND positive sentiment (>0) AND high positive reaction ratio\n",
    "# Negative PR: Low engagement (below Q25) OR negative sentiment (<-0.1) OR low positive reaction ratio\n",
    "\n",
    "def generate_pr_label(row):\n",
    "    \"\"\"\n",
    "    Generate PR label based on engagement and sentiment\n",
    "    Returns: 1 for Positive PR, 0 for Negative PR\n",
    "    \"\"\"\n",
    "    high_engagement = row['total_engagement'] >= engagement_median\n",
    "    low_engagement = row['total_engagement'] <= engagement_q25\n",
    "    positive_sentiment = row['avg_sentiment'] > 0.1  # Slightly positive\n",
    "    negative_sentiment = row['avg_sentiment'] < -0.1  # Clearly negative\n",
    "    good_reaction_ratio = row['positive_reaction_ratio'] > 0.85\n",
    "    \n",
    "    # Positive PR criteria (must meet most)\n",
    "    if high_engagement and positive_sentiment and good_reaction_ratio:\n",
    "        return 1  # Positive PR\n",
    "    \n",
    "    # Negative PR criteria (meeting any is problematic)\n",
    "    if low_engagement or negative_sentiment or (row['positive_reaction_ratio'] < 0.7):\n",
    "        return 0  # Negative PR\n",
    "    \n",
    "    # Middle ground - use engagement as tiebreaker\n",
    "    return 1 if row['total_engagement'] >= engagement_median else 0\n",
    "\n",
    "posts_with_sentiment['pr_label'] = posts_with_sentiment.apply(generate_pr_label, axis=1)\n",
    "\n",
    "print(f\"\\nâœ“ PR labels generated\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(posts_with_sentiment['pr_label'].value_counts())\n",
    "print(f\"\\nPercentage:\")\n",
    "print(posts_with_sentiment['pr_label'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57350215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution with engagement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Engagement by label\n",
    "axes[0].boxplot([\n",
    "    posts_with_sentiment[posts_with_sentiment['pr_label'] == 0]['total_engagement'],\n",
    "    posts_with_sentiment[posts_with_sentiment['pr_label'] == 1]['total_engagement']\n",
    "], labels=['Negative PR', 'Positive PR'])\n",
    "axes[0].set_ylabel('Total Engagement')\n",
    "axes[0].set_title('Engagement by PR Label')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sentiment by label\n",
    "axes[1].boxplot([\n",
    "    posts_with_sentiment[posts_with_sentiment['pr_label'] == 0]['avg_sentiment'],\n",
    "    posts_with_sentiment[posts_with_sentiment['pr_label'] == 1]['avg_sentiment']\n",
    "], labels=['Negative PR', 'Positive PR'])\n",
    "axes[1].set_ylabel('Average Comment Sentiment')\n",
    "axes[1].set_title('Sentiment by PR Label')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Visualization complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2d8f5",
   "metadata": {},
   "source": [
    "## 3. Generate Gemini Embeddings\n",
    "\n",
    "We'll use Gemini's embedding model to convert post text into dense vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings using Gemini\n",
    "def get_gemini_embedding(text, task_type=\"RETRIEVAL_DOCUMENT\"):\n",
    "    \"\"\"\n",
    "    Generate embedding for text using Gemini API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not text or pd.isna(text):\n",
    "            return None\n",
    "        result = genai.embed_content(\n",
    "            model=\"models/embedding-001\",\n",
    "            content=str(text),\n",
    "            task_type=task_type\n",
    "        )\n",
    "        return result['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test embedding generation (only if API key is set)\n",
    "if GEMINI_API_KEY:\n",
    "    test_text = \"This is a test post about technology and innovation.\"\n",
    "    test_embedding = get_gemini_embedding(test_text)\n",
    "    if test_embedding:\n",
    "        print(f\"âœ“ Test embedding generated successfully\")\n",
    "        print(f\"  Embedding dimension: {len(test_embedding)}\")\n",
    "        print(f\"  Sample values: {test_embedding[:5]}\")\n",
    "    else:\n",
    "        print(\"âš  Failed to generate test embedding\")\n",
    "else:\n",
    "    print(\"âš  Skipping embedding test - API key not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd39554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all posts\n",
    "# Note: This will make API calls and may take a few minutes\n",
    "\n",
    "if GEMINI_API_KEY:\n",
    "    print(f\"Generating embeddings for {len(posts_with_sentiment)} posts...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    embeddings_list = []\n",
    "    failed_indices = []\n",
    "    \n",
    "    for idx, text in enumerate(posts_with_sentiment['text']):\n",
    "        if idx % 20 == 0:\n",
    "            print(f\"  Progress: {idx}/{len(posts_with_sentiment)}\")\n",
    "        \n",
    "        embedding = get_gemini_embedding(text)\n",
    "        if embedding:\n",
    "            embeddings_list.append(embedding)\n",
    "        else:\n",
    "            embeddings_list.append([0] * 768)  # Fallback to zero vector\n",
    "            failed_indices.append(idx)\n",
    "    \n",
    "    print(f\"âœ“ Embeddings generated\")\n",
    "    print(f\"  Success: {len(embeddings_list) - len(failed_indices)}/{len(posts_with_sentiment)}\")\n",
    "    if failed_indices:\n",
    "        print(f\"  Failed: {len(failed_indices)} posts\")\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    embeddings_array = np.array(embeddings_list)\n",
    "    print(f\"\\nEmbeddings shape: {embeddings_array.shape}\")\n",
    "    \n",
    "    # Save embeddings to avoid re-generating\n",
    "    np.save('post_embeddings.npy', embeddings_array)\n",
    "    print(\"âœ“ Embeddings saved to post_embeddings.npy\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš  Skipping embedding generation - API key not set\")\n",
    "    print(\"  Creating placeholder embeddings for demonstration...\")\n",
    "    # Create random embeddings as placeholders\n",
    "    embeddings_array = np.random.randn(len(posts_with_sentiment), 768)\n",
    "    print(f\"  Created placeholder embeddings: {embeddings_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f7b9a",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Extract metadata features from posts to complement text embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text-based features\n",
    "def count_emojis(text):\n",
    "    \"\"\"Count emoji characters in text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return len(emoji_pattern.findall(text))\n",
    "\n",
    "def count_urls(text):\n",
    "    \"\"\"Count URLs in text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return len(url_pattern.findall(str(text)))\n",
    "\n",
    "def count_hashtags(text):\n",
    "    \"\"\"Count hashtags in text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    return len(re.findall(r'#\\w+', str(text)))\n",
    "\n",
    "def count_mentions(text):\n",
    "    \"\"\"Count @ mentions in text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    return len(re.findall(r'@\\w+', str(text)))\n",
    "\n",
    "# Apply text feature extraction\n",
    "posts_with_sentiment['text_length'] = posts_with_sentiment['text'].apply(lambda x: len(str(x)) if not pd.isna(x) else 0)\n",
    "posts_with_sentiment['emoji_count'] = posts_with_sentiment['text'].apply(count_emojis)\n",
    "posts_with_sentiment['url_count'] = posts_with_sentiment['text'].apply(count_urls)\n",
    "posts_with_sentiment['hashtag_count'] = posts_with_sentiment['text'].apply(count_hashtags)\n",
    "posts_with_sentiment['mention_count'] = posts_with_sentiment['text'].apply(count_mentions)\n",
    "\n",
    "print(\"âœ“ Text features extracted\")\n",
    "print(f\"  Text length: mean={posts_with_sentiment['text_length'].mean():.0f}, median={posts_with_sentiment['text_length'].median():.0f}\")\n",
    "print(f\"  Emojis: mean={posts_with_sentiment['emoji_count'].mean():.1f}\")\n",
    "print(f\"  URLs: mean={posts_with_sentiment['url_count'].mean():.1f}\")\n",
    "print(f\"  Hashtags: mean={posts_with_sentiment['hashtag_count'].mean():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d5308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temporal features\n",
    "posts_with_sentiment['posted_datetime'] = pd.to_datetime(posts_with_sentiment['posted_date'], errors='coerce')\n",
    "posts_with_sentiment['post_hour'] = posts_with_sentiment['posted_datetime'].dt.hour\n",
    "posts_with_sentiment['post_day_of_week'] = posts_with_sentiment['posted_datetime'].dt.dayofweek\n",
    "posts_with_sentiment['post_month'] = posts_with_sentiment['posted_datetime'].dt.month\n",
    "\n",
    "print(\"âœ“ Temporal features extracted\")\n",
    "print(f\"  Most common posting hour: {posts_with_sentiment['post_hour'].mode().values[0] if len(posts_with_sentiment['post_hour'].mode()) > 0 else 'N/A'}\")\n",
    "print(f\"  Most common day of week: {posts_with_sentiment['post_day_of_week'].mode().values[0] if len(posts_with_sentiment['post_day_of_week'].mode()) > 0 else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36992c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract media features\n",
    "posts_with_sentiment['has_media'] = posts_with_sentiment['media'].apply(lambda x: 0 if pd.isna(x) or x is None else 1)\n",
    "\n",
    "def get_media_type(media):\n",
    "    if pd.isna(media) or media is None:\n",
    "        return 'none'\n",
    "    if isinstance(media, dict):\n",
    "        return media.get('type', 'none')\n",
    "    return 'none'\n",
    "\n",
    "def get_media_count(media):\n",
    "    if pd.isna(media) or media is None:\n",
    "        return 0\n",
    "    if isinstance(media, dict) and 'items' in media:\n",
    "        items = media['items']\n",
    "        if isinstance(items, list):\n",
    "            return len(items)\n",
    "    return 0\n",
    "\n",
    "posts_with_sentiment['media_type'] = posts_with_sentiment['media'].apply(get_media_type)\n",
    "posts_with_sentiment['media_count'] = posts_with_sentiment['media'].apply(get_media_count)\n",
    "\n",
    "print(\"âœ“ Media features extracted\")\n",
    "print(f\"  Media types: {posts_with_sentiment['media_type'].value_counts().to_dict()}\")\n",
    "print(f\"  Average media count: {posts_with_sentiment['media_count'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Post type encoding\n",
    "post_type_encoder = LabelEncoder()\n",
    "posts_with_sentiment['post_type_encoded'] = post_type_encoder.fit_transform(posts_with_sentiment['post_type'].fillna('regular'))\n",
    "\n",
    "# Media type encoding\n",
    "media_type_encoder = LabelEncoder()\n",
    "posts_with_sentiment['media_type_encoded'] = media_type_encoder.fit_transform(posts_with_sentiment['media_type'])\n",
    "\n",
    "print(\"âœ“ Categorical features encoded\")\n",
    "print(f\"  Post types: {dict(enumerate(post_type_encoder.classes_))}\")\n",
    "print(f\"  Media types: {dict(enumerate(media_type_encoder.classes_))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9de28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select metadata features for model\n",
    "metadata_features = [\n",
    "    'text_length', 'emoji_count', 'url_count', 'hashtag_count', 'mention_count',\n",
    "    'post_hour', 'post_day_of_week', 'post_month',\n",
    "    'has_media', 'media_count', 'media_type_encoded', 'post_type_encoded',\n",
    "    'author_follower_count',\n",
    "    'avg_sentiment', 'median_sentiment', 'num_comments_analyzed'\n",
    "]\n",
    "\n",
    "# Check which features exist\n",
    "available_features = [f for f in metadata_features if f in posts_with_sentiment.columns]\n",
    "print(f\"âœ“ Selected {len(available_features)} metadata features:\")\n",
    "for feat in available_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "# Create metadata feature matrix\n",
    "metadata_X = posts_with_sentiment[available_features].fillna(0).values\n",
    "print(f\"\\nMetadata features shape: {metadata_X.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837928b1",
   "metadata": {},
   "source": [
    "## 5. Combine Features and Train XGBoost Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8eed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine embeddings and metadata features\n",
    "print(\"Combining features...\")\n",
    "print(f\"  Embeddings shape: {embeddings_array.shape}\")\n",
    "print(f\"  Metadata shape: {metadata_X.shape}\")\n",
    "\n",
    "# Concatenate embeddings and metadata\n",
    "X_combined = np.concatenate([embeddings_array, metadata_X], axis=1)\n",
    "print(f\"âœ“ Combined features shape: {X_combined.shape}\")\n",
    "\n",
    "# Get labels\n",
    "y = posts_with_sentiment['pr_label'].values\n",
    "print(f\"  Labels shape: {y.shape}\")\n",
    "print(f\"  Label distribution: {np.bincount(y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(\"âœ“ Data split complete\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"  Train label distribution: {np.bincount(y_train)}\")\n",
    "print(f\"  Test label distribution: {np.bincount(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8634a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for better performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"âœ“ Features scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ed7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "class_counts = np.bincount(y_train)\n",
    "scale_pos_weight = class_counts[0] / class_counts[1] if class_counts[1] > 0 else 1.0\n",
    "print(f\"Class imbalance ratio: {scale_pos_weight:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost classifier\n",
    "print(\"Training XGBoost model...\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Train with validation set\n",
    "xgb_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_test_scaled, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929170f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = xgb_model.predict(X_train_scaled)\n",
    "y_pred_test = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "y_pred_proba_train = xgb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_pred_proba_test = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"âœ“ Predictions generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and preprocessing objects\n",
    "import joblib\n",
    "\n",
    "joblib.dump(xgb_model, 'pr_classifier_model.pkl')\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "joblib.dump(post_type_encoder, 'post_type_encoder.pkl')\n",
    "joblib.dump(media_type_encoder, 'media_type_encoder.pkl')\n",
    "\n",
    "print(\"âœ“ Model and preprocessors saved\")\n",
    "print(\"  - pr_classifier_model.pkl\")\n",
    "print(\"  - feature_scaler.pkl\")\n",
    "print(\"  - post_type_encoder.pkl\")\n",
    "print(\"  - media_type_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45edbda5",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training set metrics\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "train_f1 = f1_score(y_train, y_pred_train)\n",
    "train_precision, train_recall, _, _ = precision_recall_fscore_support(y_train, y_pred_train, average='binary')\n",
    "\n",
    "print(\"\\nTRAINING SET:\")\n",
    "print(f\"  Accuracy:  {train_accuracy:.4f}\")\n",
    "print(f\"  Precision: {train_precision:.4f}\")\n",
    "print(f\"  Recall:    {train_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {train_f1:.4f}\")\n",
    "\n",
    "# Test set metrics\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "test_precision, test_recall, _, _ = precision_recall_fscore_support(y_test, y_pred_test, average='binary')\n",
    "\n",
    "print(\"\\nTEST SET:\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {test_f1:.4f}\")\n",
    "\n",
    "# ROC AUC\n",
    "try:\n",
    "    train_auc = roc_auc_score(y_train, y_pred_proba_train)\n",
    "    test_auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "    print(f\"\\n  Train ROC AUC: {train_auc:.4f}\")\n",
    "    print(f\"  Test ROC AUC:  {test_auc:.4f}\")\n",
    "except:\n",
    "    print(\"\\n  ROC AUC: Unable to calculate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nClass 0 = Negative PR\")\n",
    "print(\"Class 1 = Positive PR\\n\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Negative PR', 'Positive PR']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6991ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Negative PR', 'Positive PR'],\n",
    "            yticklabels=['Negative PR', 'Positive PR'])\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix - PR Classification', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(f\"  True Negatives:  {cm[0,0]}\")\n",
    "print(f\"  False Positives: {cm[0,1]}\")\n",
    "print(f\"  False Negatives: {cm[1,0]}\")\n",
    "print(f\"  True Positives:  {cm[1,1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b475484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Create feature names\n",
    "embedding_features = [f'embedding_{i}' for i in range(embeddings_array.shape[1])]\n",
    "all_feature_names = embedding_features + available_features\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Show top metadata features (exclude embeddings for clarity)\n",
    "metadata_importance = importance_df[importance_df['feature'].isin(available_features)]\n",
    "print(\"\\nTop 15 Metadata Features:\")\n",
    "print(metadata_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Aggregate embedding importance\n",
    "embedding_importance_sum = importance_df[importance_df['feature'].str.startswith('embedding_')]['importance'].sum()\n",
    "print(f\"\\nTotal Embedding Importance: {embedding_importance_sum:.4f}\")\n",
    "print(f\"Total Metadata Importance: {metadata_importance['importance'].sum():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Top 20 features overall\n",
    "top_features = importance_df.head(20)\n",
    "sns.barplot(data=top_features, y='feature', x='importance', ax=ax, palette='viridis')\n",
    "ax.set_xlabel('Importance Score', fontsize=12)\n",
    "ax.set_ylabel('Feature', fontsize=12)\n",
    "ax.set_title('Top 20 Most Important Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca16f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metadata features only\n",
    "if len(metadata_importance) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    top_metadata = metadata_importance.head(15)\n",
    "    sns.barplot(data=top_metadata, y='feature', x='importance', ax=ax, palette='coolwarm')\n",
    "    ax.set_xlabel('Importance Score', fontsize=12)\n",
    "    ax.set_ylabel('Feature', fontsize=12)\n",
    "    ax.set_title('Top Metadata Features by Importance', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b70c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions with explanations\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get some test samples\n",
    "sample_indices = np.random.choice(len(X_test), size=min(5, len(X_test)), replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    original_idx = y_test.index[idx] if hasattr(y_test, 'index') else idx\n",
    "    \n",
    "    # Get the post data (need to match back to original dataframe)\n",
    "    # This is a bit tricky since we've done train/test split\n",
    "    test_indices = np.arange(len(y))[np.isin(np.arange(len(y)), \n",
    "                                               np.where(np.isin(y, y_test))[0])]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sample {i+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"True Label:      {'Positive PR' if y_test[idx] == 1 else 'Negative PR'}\")\n",
    "    print(f\"Predicted Label: {'Positive PR' if y_pred_test[idx] == 1 else 'Negative PR'}\")\n",
    "    print(f\"Confidence:      {y_pred_proba_test[idx]:.2%}\")\n",
    "    print(f\"Correct:         {'âœ“' if y_test[idx] == y_pred_test[idx] else 'âœ—'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6bb1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nâœ“ Model Type: XGBoost Classifier\")\n",
    "print(f\"âœ“ Features: {X_combined.shape[1]} total ({embeddings_array.shape[1]} embeddings + {len(available_features)} metadata)\")\n",
    "print(f\"âœ“ Training samples: {len(X_train)}\")\n",
    "print(f\"âœ“ Test samples: {len(X_test)}\")\n",
    "print(f\"\\nâœ“ Test Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"âœ“ Test F1 Score: {test_f1:.2%}\")\n",
    "print(f\"âœ“ Test Precision: {test_precision:.2%}\")\n",
    "print(f\"âœ“ Test Recall: {test_recall:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. The model successfully classifies posts as positive or negative PR\")\n",
    "print(\"2. Gemini embeddings capture semantic content of posts\")\n",
    "print(\"3. Metadata features (engagement, sentiment, media type) provide additional signal\")\n",
    "print(f\"4. Top predictive features include: {', '.join(metadata_importance.head(3)['feature'].values)}\")\n",
    "print(\"\\n5. Use this model to:\")\n",
    "print(\"   - Predict PR impact before posting\")\n",
    "print(\"   - Identify what makes content resonate positively\")\n",
    "print(\"   - Optimize content strategy for better PR outcomes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1f25c",
   "metadata": {},
   "source": [
    "## Next Steps and Usage\n",
    "\n",
    "To use this model for new posts:\n",
    "\n",
    "```python\n",
    "# 1. Generate embedding for new post text\n",
    "new_post_text = \"Your new post text here...\"\n",
    "new_embedding = get_gemini_embedding(new_post_text)\n",
    "\n",
    "# 2. Extract metadata features (text_length, emoji_count, etc.)\n",
    "# 3. Combine embedding + metadata\n",
    "# 4. Scale using saved scaler\n",
    "# 5. Predict: xgb_model.predict(scaled_features)\n",
    "```\n",
    "\n",
    "Model files saved:\n",
    "- `pr_classifier_model.pkl` - Trained XGBoost model\n",
    "- `feature_scaler.pkl` - Feature scaler\n",
    "- `post_embeddings.npy` - Cached embeddings\n",
    "- Encoders for categorical features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
