{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Embeddings + XGBoost Engagement Prediction Model\n",
    "\n",
    "This notebook trains an XGBoost regression model to predict post engagement metrics using:\n",
    "- **LLM embeddings** (Google Gemini) for post text semantic understanding\n",
    "- **Persona metadata** (job role, affiliation, account age)\n",
    "- **Context metadata** (audience size, baseline engagement, time window)\n",
    "\n",
    "**Target Variables:**\n",
    "- % positive reactions\n",
    "- % negative reactions  \n",
    "- Comment sentiment distribution\n",
    "- Engagement velocity (early vs late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Google AI for embeddings\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    GEMINI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: google-generativeai not installed. Install with: pip install google-generativeai\")\n",
    "    GEMINI_AVAILABLE = False\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up API keys and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', '')\n",
    "EMBEDDING_MODEL = 'models/text-embedding-004'  # or 'models/gemini-embedding-001'\n",
    "DATA_PATH = 'data/'  # Path to your training data directory, or None to simulate\n",
    "\n",
    "# Configure Gemini API if available\n",
    "if GEMINI_AVAILABLE and GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"✓ Gemini API configured\")\n",
    "else:\n",
    "    print(\"⚠ Gemini API not configured. Embeddings will be simulated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset Generation\n",
    "\n",
    "Standalone functions to generate realistic synthetic datasets for Model A (individual-level) and Model B (post-level) with options to save to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_to_file(df: pd.DataFrame, filepath: str, format: str = 'csv') -> None:\n",
    "    \"\"\"\n",
    "    Save a dataset to file in specified format.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        filepath: Path to save file\n",
    "        format: 'csv' or 'json'\n",
    "    \"\"\"\n",
    "    if format.lower() == 'csv':\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"✓ Saved dataset to {filepath} (CSV, {len(df)} rows)\")\n",
    "    elif format.lower() == 'json':\n",
    "        df.to_json(filepath, orient='records', indent=2)\n",
    "        print(f\"✓ Saved dataset to {filepath} (JSON, {len(df)} rows)\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {format}. Use 'csv' or 'json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comment_text(intent: str, role: str, post_topic: str, sentiment: float) -> str:\n",
    "    \"\"\"\n",
    "    Generate realistic comment text based on intent, role, topic, and sentiment.\n",
    "    \"\"\"\n",
    "    comment_templates = {\n",
    "        'praise': [\n",
    "            \"Great post! {topic} is exactly what we need right now.\",\n",
    "            \"Love this perspective on {topic}. Spot on!\",\n",
    "            \"Excellent insights on {topic}. Thanks for sharing!\",\n",
    "            \"This is why I follow {role}s like you. Great {topic} content!\",\n",
    "            \"Absolutely agree. {topic} is crucial for the industry.\"\n",
    "        ],\n",
    "        'agreement': [\n",
    "            \"Completely agree. {topic} is a game-changer.\",\n",
    "            \"This resonates. We've seen similar patterns with {topic}.\",\n",
    "            \"Spot on. {topic} requires exactly this approach.\",\n",
    "            \"Couldn't agree more. {topic} needs more attention.\",\n",
    "            \"100% this. {topic} is the future.\"\n",
    "        ],\n",
    "        'question': [\n",
    "            \"Interesting take on {topic}. How do you handle [specific aspect]?\",\n",
    "            \"Great post! Curious about your thoughts on [related topic]?\",\n",
    "            \"Love this. Have you considered [alternative approach] for {topic}?\",\n",
    "            \"This is helpful. What's your experience with [related issue]?\",\n",
    "            \"Thanks for sharing. How does {topic} relate to [other topic]?\"\n",
    "        ],\n",
    "        'criticism': [\n",
    "            \"I see your point, but {topic} also has [counterpoint].\",\n",
    "            \"Interesting perspective, though I'd argue {topic} is more nuanced.\",\n",
    "            \"Respectfully disagree. {topic} needs [different approach].\",\n",
    "            \"I think {topic} requires considering [other factor].\",\n",
    "            \"Good points, but {topic} isn't that simple.\"\n",
    "        ],\n",
    "        'insight_addition': [\n",
    "            \"Adding to this: {topic} also benefits from [additional insight].\",\n",
    "            \"Great post! From my experience, {topic} works best when [insight].\",\n",
    "            \"This is spot on. I'd add that {topic} requires [insight].\",\n",
    "            \"Excellent points. Another angle: {topic} is enhanced by [insight].\",\n",
    "            \"Love this. One more thing: {topic} is improved with [insight].\"\n",
    "        ],\n",
    "        'none': ''\n",
    "    }\n",
    "    \n",
    "    if intent == 'none' or intent not in comment_templates:\n",
    "        return ''\n",
    "    \n",
    "    templates = comment_templates[intent]\n",
    "    template = np.random.choice(templates)\n",
    "    \n",
    "    # Replace placeholders\n",
    "    comment = template.format(topic=post_topic, role=role)\n",
    "    \n",
    "    # Adjust tone based on sentiment\n",
    "    if sentiment < -0.3:\n",
    "        comment = comment.replace(\"Great\", \"Interesting\").replace(\"Love\", \"See\")\n",
    "    elif sentiment > 0.7:\n",
    "        comment = comment.replace(\"Interesting\", \"Excellent\").replace(\"See\", \"Love\")\n",
    "    \n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_role_post_alignment(role: str, post_topic: str, post_author_role: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate alignment score between a role and a post (0-1).\n",
    "    Higher score means more likely to engage.\n",
    "    \"\"\"\n",
    "    alignment = 0.5  # Base alignment\n",
    "    \n",
    "    # Role-topic alignment\n",
    "    tech_roles = ['Software Engineer', 'Data Scientist', 'CTO', 'Product Manager']\n",
    "    leadership_roles = ['CEO', 'Founder', 'CTO']\n",
    "    business_roles = ['CEO', 'Founder', 'Product Manager', 'Marketing Director']\n",
    "    \n",
    "    if post_topic in ['AI', 'technology', 'data science']:\n",
    "        if role in tech_roles:\n",
    "            alignment += 0.3\n",
    "        if role == 'CTO':\n",
    "            alignment += 0.2\n",
    "    elif post_topic in ['startups', 'leadership']:\n",
    "        if role in leadership_roles:\n",
    "            alignment += 0.3\n",
    "        if role == 'Founder':\n",
    "            alignment += 0.2\n",
    "    elif post_topic in ['productivity', 'innovation']:\n",
    "        if role in business_roles:\n",
    "            alignment += 0.2\n",
    "    elif post_topic == 'marketing':\n",
    "        if role == 'Marketing Director':\n",
    "            alignment += 0.3\n",
    "    \n",
    "    # Author-role alignment (people engage more with similar roles)\n",
    "    if role == post_author_role:\n",
    "        alignment += 0.15\n",
    "    \n",
    "    # Seniority bonus (executives engage less frequently but more thoughtfully)\n",
    "    if role in ['CEO', 'Founder', 'CTO']:\n",
    "        alignment += 0.1  # But lower base reaction probability\n",
    "    \n",
    "    return min(alignment, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_model_a_data(\n",
    "    n_posts: int = 100,\n",
    "    interactions_per_post: int = 20,\n",
    "    save_path: Optional[str] = None,\n",
    "    seed: int = 42,\n",
    "    realism_level: float = 0.8\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic individual-level data (Model A format).\n",
    "    \n",
    "    Args:\n",
    "        n_posts: Number of unique posts to generate\n",
    "        interactions_per_post: Number of person interactions per post\n",
    "        save_path: Optional path to save dataset (e.g., 'model_a_data.csv')\n",
    "        seed: Random seed for reproducibility\n",
    "        realism_level: 0-1, controls how realistic vs random (1 = most realistic)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with individual-level interaction data\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    print(f\"Generating synthetic Model A data...\")\n",
    "    print(f\"  Posts: {n_posts}\")\n",
    "    print(f\"  Interactions per post: {interactions_per_post}\")\n",
    "    print(f\"  Total interactions: {n_posts * interactions_per_post}\")\n",
    "    print(f\"  Realism level: {realism_level}\")\n",
    "    \n",
    "    # Post text templates\n",
    "    post_templates = [\n",
    "        \"Excited to share my latest insights on {topic}! {opinion}\",\n",
    "        \"Just finished reading an amazing article about {topic}. Here's what I learned: {insight}\",\n",
    "        \"Hot take: {opinion} What do you think?\",\n",
    "        \"After {experience}, I've realized that {insight}\",\n",
    "        \"Breaking: {news} This changes everything!\",\n",
    "        \"Quick thread on {topic}: 1/5 {point1}\",\n",
    "        \"The future of {topic} is {prediction}. Here's why:\",\n",
    "        \"I disagree with the common wisdom that {common_belief}. Instead, {alternative}\",\n",
    "        \"5 lessons I learned from {experience}:\",\n",
    "        \"Why {topic} matters more than you think: {reason}\"\n",
    "    ]\n",
    "    \n",
    "    topics = [\"AI\", \"startups\", \"leadership\", \"technology\", \"productivity\", \"innovation\", \"data science\", \"marketing\"]\n",
    "    author_roles = [\"Software Engineer\", \"Product Manager\", \"Data Scientist\", \"CEO\", \"CTO\", \"Marketing Director\", \"Founder\"]\n",
    "    time_windows = [\"morning\", \"afternoon\", \"evening\", \"weekend\"]\n",
    "    \n",
    "    # Role archetypes with metadata\n",
    "    role_archetypes = {\n",
    "        'Software Engineer': {'industry': 'Tech', 'seniority': 'Mid'},\n",
    "        'Product Manager': {'industry': 'Tech', 'seniority': 'Mid'},\n",
    "        'Data Scientist': {'industry': 'Tech', 'seniority': 'Mid'},\n",
    "        'CEO': {'industry': 'Tech', 'seniority': 'Exec'},\n",
    "        'CTO': {'industry': 'Tech', 'seniority': 'Exec'},\n",
    "        'Founder': {'industry': 'Tech', 'seniority': 'Exec'},\n",
    "        'Recruiter': {'industry': 'Tech', 'seniority': 'Mid'},\n",
    "        'Marketing Director': {'industry': 'Tech', 'seniority': 'Senior'},\n",
    "        'Designer': {'industry': 'Tech', 'seniority': 'Mid'},\n",
    "        'Consultant': {'industry': 'Consulting', 'seniority': 'Senior'},\n",
    "        'Student': {'industry': 'Education', 'seniority': 'Junior'}\n",
    "    }\n",
    "    \n",
    "    reaction_types = ['like', 'love', 'insightful', 'celebrate', 'none']\n",
    "    comment_intents = ['praise', 'agreement', 'question', 'criticism', 'insight_addition', 'none']\n",
    "    \n",
    "    # Generate posts\n",
    "    posts = []\n",
    "    for i in range(n_posts):\n",
    "        template = np.random.choice(post_templates)\n",
    "        topic = np.random.choice(topics)\n",
    "        opinion = f\"Game-changing innovation in {topic}\"\n",
    "        insight = f\"{topic} is revolutionizing the industry\"\n",
    "        \n",
    "        post_text = template.format(\n",
    "            topic=topic,\n",
    "            opinion=opinion,\n",
    "            insight=insight,\n",
    "            experience=f\"working with {topic}\",\n",
    "            news=f\"Major breakthrough in {topic}\",\n",
    "            point1=f\"{topic} is evolving rapidly\",\n",
    "            prediction=\"bright\",\n",
    "            common_belief=f\"{topic} is overhyped\",\n",
    "            alternative=f\"{topic} is just getting started\",\n",
    "            reason=f\"{topic} drives innovation\"\n",
    "        )\n",
    "        \n",
    "        author_role = np.random.choice(author_roles)\n",
    "        time_posted = np.random.choice(time_windows)\n",
    "        \n",
    "        posts.append({\n",
    "            'post_text': post_text,\n",
    "            'post_topic': topic,\n",
    "            'post_author_role': author_role,\n",
    "            'time_posted': time_posted\n",
    "        })\n",
    "    \n",
    "    # Generate individual interactions\n",
    "    individual_data = []\n",
    "    \n",
    "    for post in posts:\n",
    "        post_text = post['post_text']\n",
    "        post_topic = post['post_topic']\n",
    "        post_author_role = post['post_author_role']\n",
    "        time_posted = post['time_posted']\n",
    "        \n",
    "        # Sample role archetypes for this post\n",
    "        available_roles = list(role_archetypes.keys())\n",
    "        sampled_roles = np.random.choice(\n",
    "            available_roles,\n",
    "            size=min(interactions_per_post, len(available_roles)),\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for role in sampled_roles:\n",
    "            role_info = role_archetypes[role]\n",
    "            \n",
    "            # Calculate role-post alignment\n",
    "            alignment = calculate_role_post_alignment(role, post_topic, post_author_role)\n",
    "            \n",
    "            # Adjust probabilities based on realism level\n",
    "            base_reaction_prob = 0.5 + (alignment - 0.5) * realism_level\n",
    "            \n",
    "            # Reaction type\n",
    "            if np.random.random() < base_reaction_prob:\n",
    "                # Weight reaction types based on alignment\n",
    "                if alignment > 0.7:\n",
    "                    reaction_weights = [0.3, 0.3, 0.25, 0.1, 0.05]  # More positive reactions\n",
    "                elif alignment > 0.5:\n",
    "                    reaction_weights = [0.4, 0.2, 0.2, 0.1, 0.1]\n",
    "                else:\n",
    "                    reaction_weights = [0.5, 0.1, 0.1, 0.05, 0.25]  # More 'none'\n",
    "                \n",
    "                reaction_type = np.random.choice(['like', 'love', 'insightful', 'celebrate', 'none'],\n",
    "                                                p=reaction_weights)\n",
    "            else:\n",
    "                reaction_type = 'none'\n",
    "            \n",
    "            # Comment probability (correlated with reaction and alignment)\n",
    "            if reaction_type == 'none':\n",
    "                comment_prob = 0.01 * realism_level\n",
    "            elif reaction_type in ['love', 'celebrate']:\n",
    "                comment_prob = 0.25 * realism_level\n",
    "            elif reaction_type == 'insightful':\n",
    "                comment_prob = 0.35 * realism_level  # Insightful reactions often lead to comments\n",
    "            else:\n",
    "                comment_prob = 0.15 * realism_level\n",
    "            \n",
    "            # Executives comment less frequently but more thoughtfully\n",
    "            if role in ['CEO', 'Founder', 'CTO']:\n",
    "                comment_prob *= 0.6\n",
    "            \n",
    "            commented = np.random.random() < comment_prob\n",
    "            \n",
    "            # Comment sentiment and intent\n",
    "            if commented:\n",
    "                # Sentiment based on reaction type\n",
    "                if reaction_type in ['love', 'celebrate']:\n",
    "                    comment_sentiment = np.random.uniform(0.6, 1.0)\n",
    "                    intent_weights = [0.5, 0.3, 0.1, 0.05, 0.05, 0.0]  # More praise/agreement\n",
    "                elif reaction_type == 'insightful':\n",
    "                    comment_sentiment = np.random.uniform(0.3, 0.8)\n",
    "                    intent_weights = [0.1, 0.2, 0.2, 0.05, 0.4, 0.05]  # More insight addition\n",
    "                elif reaction_type == 'like':\n",
    "                    comment_sentiment = np.random.uniform(0.0, 0.7)\n",
    "                    intent_weights = [0.2, 0.4, 0.2, 0.1, 0.05, 0.05]\n",
    "                else:\n",
    "                    comment_sentiment = np.random.uniform(-0.5, 0.5)\n",
    "                    intent_weights = [0.05, 0.1, 0.3, 0.4, 0.1, 0.05]  # More questions/criticism\n",
    "                \n",
    "                # Adjust intent based on role\n",
    "                if role in ['CEO', 'Founder']:\n",
    "                    # Executives more likely to add insights or ask strategic questions\n",
    "                    intent_weights = [0.1, 0.2, 0.3, 0.1, 0.25, 0.05]\n",
    "                elif role in ['Software Engineer', 'Data Scientist']:\n",
    "                    # Engineers more likely to ask technical questions\n",
    "                    intent_weights = [0.15, 0.25, 0.35, 0.1, 0.1, 0.05]\n",
    "                \n",
    "                comment_intent = np.random.choice(comment_intents, p=intent_weights)\n",
    "                comment_text = generate_comment_text(comment_intent, role, post_topic, comment_sentiment)\n",
    "            else:\n",
    "                comment_sentiment = 0.0\n",
    "                comment_intent = 'none'\n",
    "                comment_text = ''\n",
    "            \n",
    "            # Post metadata\n",
    "            post_length = len(post_text.split())\n",
    "            has_hashtags = '#' in post_text\n",
    "            has_mentions = '@' in post_text\n",
    "            has_urls = 'http' in post_text.lower()\n",
    "            \n",
    "            individual_data.append({\n",
    "                # Post features\n",
    "                'post_text': post_text,\n",
    "                'post_topic': post_topic,\n",
    "                'post_author_role': post_author_role,\n",
    "                'post_length': post_length,\n",
    "                'hashtags': has_hashtags,\n",
    "                'mentions': has_mentions,\n",
    "                'urls_present': has_urls,\n",
    "                'time_posted': time_posted,\n",
    "                \n",
    "                # Person features\n",
    "                'commenter_role': role,\n",
    "                'commenter_industry': role_info['industry'],\n",
    "                'commenter_seniority': role_info['seniority'],\n",
    "                \n",
    "                # Labels\n",
    "                'reaction_type': reaction_type,\n",
    "                'commented': int(commented),\n",
    "                'comment_text': comment_text,\n",
    "                'comment_sentiment': comment_sentiment,\n",
    "                'comment_intent': comment_intent\n",
    "            })\n",
    "    \n",
    "    df_individual = pd.DataFrame(individual_data)\n",
    "    \n",
    "    print(f\"\\\\n✓ Generated {len(df_individual)} individual interactions\")\n",
    "    print(f\"  Reactions: {df_individual['reaction_type'].value_counts().to_dict()}\")\n",
    "    print(f\"  Comments: {df_individual['commented'].sum()} ({df_individual['commented'].mean():.1%})\")\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        save_dataset_to_file(df_individual, save_path)\n",
    "    \n",
    "    return df_individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_model_b_data(\n",
    "    n_posts: int = 100,\n",
    "    save_path: Optional[str] = None,\n",
    "    seed: int = 42,\n",
    "    realism_level: float = 0.8\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate realistic synthetic post-level data with aggregated features (Model B format).\n",
    "    \n",
    "    This simulates what Model A would produce when aggregated, plus post-level targets.\n",
    "    \n",
    "    Args:\n",
    "        n_posts: Number of posts to generate\n",
    "        save_path: Optional path to save dataset (e.g., 'model_b_data.csv')\n",
    "        seed: Random seed for reproducibility\n",
    "        realism_level: 0-1, controls how realistic vs random (1 = most realistic)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with post-level data including aggregated features and targets\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    print(f\"Generating synthetic Model B data...\")\n",
    "    print(f\"  Posts: {n_posts}\")\n",
    "    print(f\"  Realism level: {realism_level}\")\n",
    "    \n",
    "    # Post text templates (same as Model A)\n",
    "    post_templates = [\n",
    "        \"Excited to share my latest insights on {topic}! {opinion}\",\n",
    "        \"Just finished reading an amazing article about {topic}. Here's what I learned: {insight}\",\n",
    "        \"Hot take: {opinion} What do you think?\",\n",
    "        \"After {experience}, I've realized that {insight}\",\n",
    "        \"Breaking: {news} This changes everything!\",\n",
    "        \"Quick thread on {topic}: 1/5 {point1}\",\n",
    "        \"The future of {topic} is {prediction}. Here's why:\",\n",
    "        \"I disagree with the common wisdom that {common_belief}. Instead, {alternative}\",\n",
    "        \"5 lessons I learned from {experience}:\",\n",
    "        \"Why {topic} matters more than you think: {reason}\"\n",
    "    ]\n",
    "    \n",
    "    topics = [\"AI\", \"startups\", \"leadership\", \"technology\", \"productivity\", \"innovation\", \"data science\", \"marketing\"]\n",
    "    author_roles = [\"Software Engineer\", \"Product Manager\", \"Data Scientist\", \"CEO\", \"CTO\", \"Marketing Director\", \"Founder\"]\n",
    "    time_windows = [\"morning\", \"afternoon\", \"evening\", \"weekend\"]\n",
    "    \n",
    "    # Role archetypes for aggregation\n",
    "    engineer_roles = ['Software Engineer', 'Data Scientist', 'CTO']\n",
    "    pm_roles = ['Product Manager']\n",
    "    founder_roles = ['Founder', 'CEO']\n",
    "    all_roles = engineer_roles + pm_roles + founder_roles + ['Recruiter', 'Marketing Director', 'Designer', 'Consultant', 'Student']\n",
    "    \n",
    "    reaction_types = ['like', 'love', 'insightful', 'celebrate', 'none']\n",
    "    \n",
    "    post_data = []\n",
    "    \n",
    "    for i in range(n_posts):\n",
    "        # Generate post\n",
    "        template = np.random.choice(post_templates)\n",
    "        topic = np.random.choice(topics)\n",
    "        opinion = f\"Game-changing innovation in {topic}\"\n",
    "        insight = f\"{topic} is revolutionizing the industry\"\n",
    "        \n",
    "        post_text = template.format(\n",
    "            topic=topic,\n",
    "            opinion=opinion,\n",
    "            insight=insight,\n",
    "            experience=f\"working with {topic}\",\n",
    "            news=f\"Major breakthrough in {topic}\",\n",
    "            point1=f\"{topic} is evolving rapidly\",\n",
    "            prediction=\"bright\",\n",
    "            common_belief=f\"{topic} is overhyped\",\n",
    "            alternative=f\"{topic} is just getting started\",\n",
    "            reason=f\"{topic} drives innovation\"\n",
    "        )\n",
    "        \n",
    "        author_role = np.random.choice(author_roles)\n",
    "        time_posted = np.random.choice(time_windows)\n",
    "        post_length = len(post_text.split())\n",
    "        has_hashtags = '#' in post_text\n",
    "        has_mentions = '@' in post_text\n",
    "        has_urls = 'http' in post_text.lower()\n",
    "        \n",
    "        # Simulate aggregated Model A predictions\n",
    "        # Base engagement depends on topic popularity and author credibility\n",
    "        topic_popularity = 0.8 if topic in ['AI', 'technology', 'startups'] else 0.6\n",
    "        author_credibility = 0.9 if author_role in ['CEO', 'Founder', 'CTO'] else 0.7\n",
    "        \n",
    "        base_engagement = (topic_popularity + author_credibility) / 2 * realism_level\n",
    "        \n",
    "        # Role engagement percentages (simulate what Model A would predict)\n",
    "        # Engineers more likely to engage with tech posts\n",
    "        if topic in ['AI', 'technology', 'data science']:\n",
    "            engineer_engagement = base_engagement + np.random.uniform(0.1, 0.3) * realism_level\n",
    "            pm_engagement = base_engagement + np.random.uniform(-0.1, 0.2) * realism_level\n",
    "            founder_engagement = base_engagement + np.random.uniform(-0.1, 0.1) * realism_level\n",
    "        elif topic in ['startups', 'leadership']:\n",
    "            engineer_engagement = base_engagement + np.random.uniform(-0.1, 0.1) * realism_level\n",
    "            pm_engagement = base_engagement + np.random.uniform(0.0, 0.2) * realism_level\n",
    "            founder_engagement = base_engagement + np.random.uniform(0.2, 0.4) * realism_level\n",
    "        else:\n",
    "            engineer_engagement = base_engagement + np.random.uniform(-0.1, 0.1) * realism_level\n",
    "            pm_engagement = base_engagement + np.random.uniform(-0.1, 0.1) * realism_level\n",
    "            founder_engagement = base_engagement + np.random.uniform(-0.1, 0.1) * realism_level\n",
    "        \n",
    "        pct_engineers_reacting = np.clip(engineer_engagement, 0, 1)\n",
    "        pct_pms_commenting = np.clip(pm_engagement * 0.3, 0, 1)  # Commenting is rarer\n",
    "        pct_founders_engaging = np.clip(founder_engagement, 0, 1)\n",
    "        \n",
    "        # Reaction distribution\n",
    "        total_people = np.random.randint(50, 500)  # Simulated audience size\n",
    "        reactions_by_type = {}\n",
    "        for rt in reaction_types:\n",
    "            if rt == 'none':\n",
    "                reactions_by_type[rt] = total_people * (1 - base_engagement)\n",
    "            else:\n",
    "                # Weight reactions based on engagement level\n",
    "                if base_engagement > 0.7:\n",
    "                    weights = {'like': 0.3, 'love': 0.3, 'insightful': 0.25, 'celebrate': 0.15}\n",
    "                elif base_engagement > 0.5:\n",
    "                    weights = {'like': 0.4, 'love': 0.2, 'insightful': 0.2, 'celebrate': 0.1}\n",
    "                else:\n",
    "                    weights = {'like': 0.5, 'love': 0.1, 'insightful': 0.1, 'celebrate': 0.05}\n",
    "                \n",
    "                reactions_by_type[rt] = total_people * base_engagement * weights.get(rt, 0.1)\n",
    "        \n",
    "        # Normalize reaction distribution\n",
    "        total_reactions = sum(reactions_by_type.values())\n",
    "        reaction_distribution = {k: v / total_reactions if total_reactions > 0 else 0 \n",
    "                                for k, v in reactions_by_type.items()}\n",
    "        \n",
    "        # Comment volume (correlated with engagement)\n",
    "        comment_volume = total_people * base_engagement * np.random.uniform(0.05, 0.25) * realism_level\n",
    "        \n",
    "        # Average sentiment by role (simulate)\n",
    "        avg_sentiment_engineers = np.random.uniform(0.2, 0.8) if pct_engineers_reacting > 0.3 else 0.0\n",
    "        avg_sentiment_pms = np.random.uniform(0.3, 0.9) if pct_pms_commenting > 0.2 else 0.0\n",
    "        avg_sentiment_founders = np.random.uniform(0.4, 0.9) if pct_founders_engaging > 0.3 else 0.0\n",
    "        \n",
    "        # Calculate targets\n",
    "        total_reactions_count = int(total_people * base_engagement)\n",
    "        comment_volume_count = int(comment_volume)\n",
    "        \n",
    "        # Role distribution (percentage of each role in audience)\n",
    "        role_dist = {\n",
    "            'engineer_pct': np.random.uniform(0.3, 0.5),\n",
    "            'pm_pct': np.random.uniform(0.1, 0.2),\n",
    "            'founder_pct': np.random.uniform(0.05, 0.15)\n",
    "        }\n",
    "        # Normalize to sum to reasonable total\n",
    "        total_role_pct = sum(role_dist.values())\n",
    "        role_dist = {k: v / total_role_pct * 0.7 for k, v in role_dist.items()}  # 70% of audience\n",
    "        \n",
    "        # Engagement quality score\n",
    "        avg_sentiment = (avg_sentiment_engineers + avg_sentiment_pms + avg_sentiment_founders) / 3\n",
    "        engagement_quality = (\n",
    "            (total_reactions_count / total_people) * 0.3 +\n",
    "            (comment_volume_count / total_people) * 0.3 +\n",
    "            max(0, avg_sentiment) * 0.4\n",
    "        )\n",
    "        \n",
    "        post_data.append({\n",
    "            # Post features\n",
    "            'post_text': post_text,\n",
    "            'post_topic': topic,\n",
    "            'post_author_role': author_role,\n",
    "            'post_length': post_length,\n",
    "            'hashtags': has_hashtags,\n",
    "            'mentions': has_mentions,\n",
    "            'urls_present': has_urls,\n",
    "            'time_posted': time_posted,\n",
    "            \n",
    "            # Aggregated Model A features (simulated)\n",
    "            'pct_engineers_reacting': pct_engineers_reacting,\n",
    "            'pct_pms_commenting': pct_pms_commenting,\n",
    "            'pct_founders_engaging': pct_founders_engaging,\n",
    "            'predicted_comment_volume': comment_volume,\n",
    "            'reaction_like_pct': reaction_distribution.get('like', 0),\n",
    "            'reaction_love_pct': reaction_distribution.get('love', 0),\n",
    "            'reaction_insightful_pct': reaction_distribution.get('insightful', 0),\n",
    "            'reaction_celebrate_pct': reaction_distribution.get('celebrate', 0),\n",
    "            'reaction_none_pct': reaction_distribution.get('none', 0),\n",
    "            'avg_sentiment_engineers': avg_sentiment_engineers,\n",
    "            'avg_sentiment_pms': avg_sentiment_pms,\n",
    "            'avg_sentiment_founders': avg_sentiment_founders,\n",
    "            \n",
    "            # Target variables\n",
    "            'total_reactions': total_reactions_count,\n",
    "            'comment_volume': comment_volume_count,\n",
    "            'engineer_pct': role_dist['engineer_pct'],\n",
    "            'pm_pct': role_dist['pm_pct'],\n",
    "            'founder_pct': role_dist['founder_pct'],\n",
    "            'engagement_quality': engagement_quality\n",
    "        })\n",
    "    \n",
    "    df_post = pd.DataFrame(post_data)\n",
    "    \n",
    "    print(f\"\\\\n✓ Generated {len(df_post)} posts with aggregated features\")\n",
    "    print(f\"  Average reactions per post: {df_post['total_reactions'].mean():.1f}\")\n",
    "    print(f\"  Average comments per post: {df_post['comment_volume'].mean():.1f}\")\n",
    "    print(f\"  Average engagement quality: {df_post['engagement_quality'].mean():.3f}\")\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        save_dataset_to_file(df_post, save_path)\n",
    "    \n",
    "    return df_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Save Datasets\n",
    "\n",
    "Use the functions above to generate synthetic datasets for Model A and Model B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for dataset generation\n",
    "DATASET_CONFIG = {\n",
    "    'model_a': {\n",
    "        'n_posts': 200,\n",
    "        'interactions_per_post': 25,\n",
    "        'save_path': 'synthetic_model_a_data.csv',  # Set to None to skip saving\n",
    "        'seed': 42,\n",
    "        'realism_level': 0.8\n",
    "    },\n",
    "    'model_b': {\n",
    "        'n_posts': 200,\n",
    "        'save_path': 'synthetic_model_b_data.csv',  # Set to None to skip saving\n",
    "        'seed': 42,\n",
    "        'realism_level': 0.8\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate Model A dataset\n",
    "print(\"=\" * 70)\n",
    "print(\"GENERATING MODEL A DATASET\")\n",
    "print(\"=\" * 70)\n",
    "df_model_a_synthetic = generate_synthetic_model_a_data(**DATASET_CONFIG['model_a'])\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 70)\n",
    "print(\"GENERATING MODEL B DATASET\")\n",
    "print(\"=\" * 70)\n",
    "df_model_b_synthetic = generate_synthetic_model_b_data(**DATASET_CONFIG['model_b'])\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 70)\n",
    "print(\"DATASET GENERATION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\\\nModel A dataset: {len(df_model_a_synthetic)} rows, {len(df_model_a_synthetic.columns)} columns\")\n",
    "print(f\"Model B dataset: {len(df_model_b_synthetic)} rows, {len(df_model_b_synthetic.columns)} columns\")\n",
    "print(\"\\\\nYou can now use these datasets to train Model A and Model B!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_simulate_data(data_path: Optional[str] = None, n_samples: int = 500) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from CSV/JSON file, directory of JSON files, or generate synthetic training data.\n",
    "    \n",
    "    Expected columns:\n",
    "    - post_text: string\n",
    "    - job_role: string (persona metadata)\n",
    "    - affiliation: string (persona metadata)\n",
    "    - account_age: int (days, persona metadata)\n",
    "    - audience_size: int (context metadata)\n",
    "    - baseline_engagement: float (context metadata)\n",
    "    - time_window: string (context metadata)\n",
    "    - pct_positive: float (target, 0-100)\n",
    "    - pct_negative: float (target, 0-100)\n",
    "    - comment_sentiment_dist: float (target, -1 to 1)\n",
    "    - engagement_velocity: float (target, 0-1)\n",
    "    \"\"\"\n",
    "    # Try to load existing data\n",
    "    if data_path and os.path.exists(data_path):\n",
    "        # Check if it's a directory (for loading multiple JSON files)\n",
    "        if os.path.isdir(data_path):\n",
    "            print(f\"Loading data from directory: {data_path}\")\n",
    "            df = load_linkedin_json_data(data_path)\n",
    "            print(f\"✓ Loaded {len(df)} rows from {data_path}\")\n",
    "            return df\n",
    "        # Otherwise, treat as a single file\n",
    "        print(f\"Loading data from {data_path}...\")\n",
    "        if data_path.endswith('.csv'):\n",
    "            df = pd.read_csv(data_path)\n",
    "        elif data_path.endswith('.json'):\n",
    "            df = pd.read_json(data_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {data_path}\")\n",
    "        print(f\"✓ Loaded {len(df)} rows from {data_path}\")\n",
    "        return df\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    print(f\"Generating {n_samples} synthetic training examples...\")\n",
    "    \n",
    "    # Post text templates\n",
    "    post_templates = [\n",
    "        \"Excited to share my latest insights on {topic}! {opinion}\",\n",
    "        \"Just finished reading an amazing article about {topic}. Here's what I learned: {insight}\",\n",
    "        \"Hot take: {opinion} What do you think?\",\n",
    "        \"After {experience}, I've realized that {insight}\",\n",
    "        \"Breaking: {news} This changes everything!\",\n",
    "        \"Quick thread on {topic}: 1/5 {point1}\",\n",
    "        \"The future of {topic} is {prediction}. Here's why:\",\n",
    "        \"I disagree with the common wisdom that {common_belief}. Instead, {alternative}\",\n",
    "        \"5 lessons I learned from {experience}:\",\n",
    "        \"Why {topic} matters more than you think: {reason}\"\n",
    "    ]\n",
    "    \n",
    "    topics = [\"AI\", \"startups\", \"leadership\", \"technology\", \"productivity\", \"innovation\", \"data science\", \"marketing\"]\n",
    "    job_roles = [\"Software Engineer\", \"Product Manager\", \"Data Scientist\", \"CEO\", \"CTO\", \"Marketing Director\", \"Designer\", \"Consultant\"]\n",
    "    affiliations = [\"Tech Corp\", \"StartupXYZ\", \"BigTech Inc\", \"Consulting Group\", \"Agency\", \"Freelance\", \"Academia\", \"Government\"]\n",
    "    time_windows = [\"morning\", \"afternoon\", \"evening\", \"weekend\"]\n",
    "    \n",
    "    data = []\n",
    "    for i in range(n_samples):\n",
    "        # Generate post text\n",
    "        template = np.random.choice(post_templates)\n",
    "        topic = np.random.choice(topics)\n",
    "        opinion = f\"Game-changing innovation in {topic}\"\n",
    "        insight = f\"{topic} is revolutionizing the industry\"\n",
    "        post_text = template.format(\n",
    "            topic=topic,\n",
    "            opinion=opinion,\n",
    "            insight=insight,\n",
    "            experience=f\"working with {topic}\",\n",
    "            news=f\"Major breakthrough in {topic}\",\n",
    "            point1=f\"{topic} is evolving rapidly\",\n",
    "            prediction=\"bright\",\n",
    "            common_belief=f\"{topic} is overhyped\",\n",
    "            alternative=f\"{topic} is just getting started\",\n",
    "            reason=f\"{topic} drives innovation\"\n",
    "        )\n",
    "        \n",
    "        # Persona metadata\n",
    "        job_role = np.random.choice(job_roles)\n",
    "        affiliation = np.random.choice(affiliations)\n",
    "        account_age = np.random.randint(30, 3650)  # 1 month to 10 years in days\n",
    "        \n",
    "        # Context metadata\n",
    "        audience_size = np.random.randint(100, 100000)\n",
    "        baseline_engagement = np.random.uniform(0.01, 0.15)  # 1% to 15%\n",
    "        time_window = np.random.choice(time_windows)\n",
    "        \n",
    "        # Simulate engagement metrics based on heuristics\n",
    "        # Positive correlation: longer posts, certain topics, experienced accounts\n",
    "        text_length_factor = len(post_text) / 200  # Normalize by average length\n",
    "        account_credibility = min(account_age / 1000, 1.0)  # More credible with age\n",
    "        topic_popularity = 0.7 if topic in [\"AI\", \"technology\", \"startups\"] else 0.5\n",
    "        \n",
    "        # Base engagement with some randomness\n",
    "        base_positive = 20 + text_length_factor * 10 + account_credibility * 15 + topic_popularity * 10\n",
    "        base_positive += np.random.normal(0, 10)\n",
    "        pct_positive = np.clip(base_positive, 0, 100)\n",
    "        \n",
    "        base_negative = 5 + np.random.normal(0, 5)\n",
    "        pct_negative = np.clip(base_negative, 0, 50)\n",
    "        \n",
    "        # Comment sentiment: positive posts get positive comments\n",
    "        comment_sentiment_dist = (pct_positive - pct_negative) / 100\n",
    "        comment_sentiment_dist = np.clip(comment_sentiment_dist, -1, 1)\n",
    "        \n",
    "        # Engagement velocity: how quickly engagement happens (early vs late)\n",
    "        # High-engagement posts get early engagement\n",
    "        engagement_velocity = min(pct_positive / 100, 1.0) * 0.7 + np.random.uniform(0, 0.3)\n",
    "        engagement_velocity = np.clip(engagement_velocity, 0, 1)\n",
    "        \n",
    "        data.append({\n",
    "            'post_text': post_text,\n",
    "            'job_role': job_role,\n",
    "            'affiliation': affiliation,\n",
    "            'account_age': account_age,\n",
    "            'audience_size': audience_size,\n",
    "            'baseline_engagement': baseline_engagement,\n",
    "            'time_window': time_window,\n",
    "            'pct_positive': pct_positive,\n",
    "            'pct_negative': pct_negative,\n",
    "            'comment_sentiment_dist': comment_sentiment_dist,\n",
    "            'engagement_velocity': engagement_velocity\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"✓ Generated {len(df)} synthetic examples\")\n",
    "    return df\n",
    "\n",
    "# Load or simulate data\n",
    "df = load_or_simulate_data(DATA_PATH, n_samples=500)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and exploration\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nTarget variable statistics:\")\n",
    "print(df[['pct_positive', 'pct_negative', 'comment_sentiment_dist', 'engagement_velocity']].describe())\n",
    "\n",
    "# Handle missing values (if any)\n",
    "df = df.dropna()\n",
    "\n",
    "# Basic text normalization (optional - embeddings handle this well)\n",
    "df['post_text_clean'] = df['post_text'].str.lower().str.strip()\n",
    "\n",
    "print(f\"\\n✓ Cleaned dataset: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Embedding Generation\n",
    "\n",
    "Generate embeddings for post text using Google Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts: List[str], model_name: str = EMBEDDING_MODEL, batch_size: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using Google Gemini API.\n",
    "    Includes batch processing, error handling, and fallback to simulated embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    if not GEMINI_AVAILABLE or not GEMINI_API_KEY:\n",
    "        print(\"⚠ Using simulated embeddings (random vectors)\")\n",
    "        # Generate random embeddings with same dimension as Gemini (768 for text-embedding-004, 768/1536/3072 for gemini-embedding-001)\n",
    "        embedding_dim = 768\n",
    "        for text in texts:\n",
    "            # Create deterministic \"embeddings\" based on text hash for reproducibility\n",
    "            np.random.seed(hash(text) % 2**32)\n",
    "            emb = np.random.randn(embedding_dim)\n",
    "            emb = emb / np.linalg.norm(emb)  # Normalize\n",
    "            embeddings.append(emb)\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    print(f\"Generating embeddings for {len(texts)} texts using {model_name}...\")\n",
    "    \n",
    "    # Try different API formats (support both old and new Google AI SDK)\n",
    "    try:\n",
    "        # Try new API format (google-genai package)\n",
    "        try:\n",
    "            from google import genai as genai_new\n",
    "            client = genai_new.Client(api_key=GEMINI_API_KEY)\n",
    "            use_new_api = True\n",
    "        except:\n",
    "            use_new_api = False\n",
    "    except:\n",
    "        use_new_api = False\n",
    "    \n",
    "    # Process in batches to handle rate limits\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_embeddings = []\n",
    "        \n",
    "        for text in batch:\n",
    "            try:\n",
    "                if use_new_api:\n",
    "                    # New API format\n",
    "                    response = client.models.embed_content(\n",
    "                        model=model_name,\n",
    "                        contents=text\n",
    "                    )\n",
    "                    embedding = response.embeddings[0].values if hasattr(response.embeddings[0], 'values') else response.embeddings[0]\n",
    "                else:\n",
    "                    # Old API format (google-generativeai)\n",
    "                    result = genai.embed_content(\n",
    "                        model=model_name,\n",
    "                        content=text,\n",
    "                        task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "                    )\n",
    "                    # Handle different response formats\n",
    "                    if isinstance(result, dict):\n",
    "                        embedding = result.get('embedding', result.get('values', []))\n",
    "                    else:\n",
    "                        embedding = result.embedding if hasattr(result, 'embedding') else result\n",
    "                \n",
    "                batch_embeddings.append(embedding)\n",
    "                \n",
    "                # Small delay to avoid rate limits\n",
    "                time.sleep(0.1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error embedding text {i}: {str(e)}\")\n",
    "                # Fallback: use random embedding\n",
    "                embedding_dim = 768\n",
    "                np.random.seed(hash(text) % 2**32)\n",
    "                emb = np.random.randn(embedding_dim)\n",
    "                emb = emb / np.linalg.norm(emb)\n",
    "                batch_embeddings.append(emb)\n",
    "        \n",
    "        embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        if (i + batch_size) % 50 == 0:\n",
    "            print(f\"  Processed {min(i + batch_size, len(texts))}/{len(texts)} texts...\")\n",
    "    \n",
    "    print(f\"✓ Generated {len(embeddings)} embeddings\")\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings for all post texts\n",
    "print(\"Generating text embeddings...\")\n",
    "text_embeddings = generate_embeddings(df['post_text_clean'].tolist())\n",
    "print(f\"Embedding shape: {text_embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {text_embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Combine text embeddings with persona and context metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "def load_linkedin_json_data(data_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and transform LinkedIn post JSON files from the data folder.\n",
    "    \n",
    "    Args:\n",
    "        data_folder: Path to folder containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with transformed LinkedIn post data matching expected format\n",
    "    \"\"\"\n",
    "    # Find all company post JSON files\n",
    "    json_pattern = os.path.join(data_folder, '*company-posts*.json')\n",
    "    json_files = glob.glob(json_pattern)\n",
    "    \n",
    "    if not json_files:\n",
    "        raise ValueError(f\"No company post JSON files found in {data_folder}\")\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON file(s) to load...\")\n",
    "    \n",
    "    all_posts = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        print(f\"Loading {os.path.basename(json_file)}...\")\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            posts = json.load(f)\n",
    "        \n",
    "        if not isinstance(posts, list):\n",
    "            posts = [posts]\n",
    "        \n",
    "        print(f\"  Found {len(posts)} posts\")\n",
    "        all_posts.extend(posts)\n",
    "    \n",
    "    print(f\"Total posts loaded: {len(all_posts)}\")\n",
    "    print(\"Transforming data...\")\n",
    "    \n",
    "    transformed_data = []\n",
    "    \n",
    "    for post in all_posts:\n",
    "        # Skip posts without required fields\n",
    "        if not post.get('text') or not post.get('stats'):\n",
    "            continue\n",
    "        \n",
    "        # Extract basic fields\n",
    "        post_text = post.get('text', '')\n",
    "        stats = post.get('stats', {})\n",
    "        author = post.get('author', {})\n",
    "        posted_at = post.get('posted_at', {})\n",
    "        \n",
    "        # Calculate engagement metrics\n",
    "        total_reactions = stats.get('total_reactions', 0)\n",
    "        like_count = stats.get('like', 0)\n",
    "        love_count = stats.get('love', 0)\n",
    "        celebrate_count = stats.get('celebrate', 0)\n",
    "        support_count = stats.get('support', 0)\n",
    "        insight_count = stats.get('insight', 0)\n",
    "        comments_count = stats.get('comments', 0)\n",
    "        reposts_count = stats.get('reposts', 0)\n",
    "        \n",
    "        # Calculate pct_positive (positive reactions / total reactions * 100)\n",
    "        positive_reactions = like_count + love_count + celebrate_count + support_count\n",
    "        if total_reactions > 0:\n",
    "            pct_positive = (positive_reactions / total_reactions) * 100\n",
    "            pct_negative = (insight_count / total_reactions) * 100\n",
    "        else:\n",
    "            pct_positive = 0.0\n",
    "            pct_negative = 0.0\n",
    "        \n",
    "        # Audience size from follower count\n",
    "        audience_size = author.get('follower_count', 0)\n",
    "        if audience_size == 0:\n",
    "            audience_size = 1000  # Default minimum\n",
    "        \n",
    "        # Calculate baseline engagement\n",
    "        total_engagement = total_reactions + comments_count + reposts_count\n",
    "        baseline_engagement = total_engagement / max(audience_size, 1)\n",
    "        \n",
    "        # Calculate comment sentiment distribution (normalized -1 to 1)\n",
    "        if total_reactions > 0:\n",
    "            comment_sentiment_dist = (pct_positive - pct_negative) / 100\n",
    "            comment_sentiment_dist = np.clip(comment_sentiment_dist, -1, 1)\n",
    "        else:\n",
    "            comment_sentiment_dist = 0.0\n",
    "        \n",
    "        # Extract time window from timestamp\n",
    "        timestamp = posted_at.get('timestamp', 0)\n",
    "        if timestamp > 0:\n",
    "            # Convert timestamp (milliseconds) to datetime\n",
    "            post_datetime = datetime.fromtimestamp(timestamp / 1000)\n",
    "            hour = post_datetime.hour\n",
    "            weekday = post_datetime.weekday()  # 0 = Monday, 6 = Sunday\n",
    "            \n",
    "            if weekday >= 5:  # Saturday or Sunday\n",
    "                time_window = 'weekend'\n",
    "            elif hour < 12:\n",
    "                time_window = 'morning'\n",
    "            elif hour < 17:\n",
    "                time_window = 'afternoon'\n",
    "            else:\n",
    "                time_window = 'evening'\n",
    "        else:\n",
    "            time_window = 'afternoon'  # Default\n",
    "        \n",
    "        # Extract affiliation from source_company\n",
    "        affiliation = post.get('source_company', 'Unknown')\n",
    "        if affiliation and isinstance(affiliation, str):\n",
    "            affiliation = affiliation.capitalize()\n",
    "        else:\n",
    "            affiliation = 'Unknown'\n",
    "        \n",
    "        # Infer job_role from company name or use default\n",
    "        # For company posts, we'll use a default role based on the company\n",
    "        company_name = affiliation.lower()\n",
    "        if 'google' in company_name or 'microsoft' in company_name or 'apple' in company_name:\n",
    "            job_role = 'Software Engineer'\n",
    "        elif 'meta' in company_name or 'facebook' in company_name:\n",
    "            job_role = 'Product Manager'\n",
    "        else:\n",
    "            job_role = 'Product Manager'  # Default for company posts\n",
    "        \n",
    "        # Account age - use default since we don't have account creation date\n",
    "        # For company accounts, use a reasonable default (e.g., 5 years)\n",
    "        account_age = 1825  # 5 years in days\n",
    "        \n",
    "        # Calculate engagement velocity\n",
    "        # Estimate based on post age and engagement rate\n",
    "        # Newer posts with high engagement = high velocity\n",
    "        if timestamp > 0:\n",
    "            # Calculate post age in days\n",
    "            current_time = datetime.now().timestamp() * 1000\n",
    "            post_age_days = (current_time - timestamp) / (1000 * 60 * 60 * 24)\n",
    "            \n",
    "            # High engagement rate + recent post = high velocity\n",
    "            engagement_rate = baseline_engagement\n",
    "            if post_age_days < 1:  # Less than 1 day old\n",
    "                velocity_factor = 0.9\n",
    "            elif post_age_days < 7:  # Less than 1 week old\n",
    "                velocity_factor = 0.7\n",
    "            else:\n",
    "                velocity_factor = 0.5\n",
    "            \n",
    "            engagement_velocity = min(engagement_rate * 10 * velocity_factor, 1.0)\n",
    "            engagement_velocity = max(engagement_velocity, 0.0)\n",
    "        else:\n",
    "            engagement_velocity = 0.5  # Default\n",
    "        \n",
    "        transformed_data.append({\n",
    "            'post_text': post_text,\n",
    "            'job_role': job_role,\n",
    "            'affiliation': affiliation,\n",
    "            'account_age': account_age,\n",
    "            'audience_size': audience_size,\n",
    "            'baseline_engagement': baseline_engagement,\n",
    "            'time_window': time_window,\n",
    "            'pct_positive': pct_positive,\n",
    "            'pct_negative': pct_negative,\n",
    "            'comment_sentiment_dist': comment_sentiment_dist,\n",
    "            'engagement_velocity': engagement_velocity\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(transformed_data)\n",
    "    print(f\"✓ Transformed {len(df)} posts\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_linkedin_json_data(data_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and transform LinkedIn post JSON files from the data folder.\n",
    "    \n",
    "    Args:\n",
    "        data_folder: Path to folder containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with transformed LinkedIn post data matching expected format\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Find all company post JSON files\n",
    "    json_pattern = os.path.join(data_folder, '*company-posts*.json')\n",
    "    json_files = glob.glob(json_pattern)\n",
    "    \n",
    "    if not json_files:\n",
    "        raise ValueError(f\"No company post JSON files found in {data_folder}\")\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON file(s) to load...\")\n",
    "    \n",
    "    all_posts = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        print(f\"Loading {os.path.basename(json_file)}...\")\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            posts = json.load(f)\n",
    "        \n",
    "        if not isinstance(posts, list):\n",
    "            posts = [posts]\n",
    "        \n",
    "        print(f\"  Found {len(posts)} posts\")\n",
    "        all_posts.extend(posts)\n",
    "    \n",
    "    print(f\"Total posts loaded: {len(all_posts)}\")\n",
    "    print(\"Transforming data...\")\n",
    "    \n",
    "    transformed_data = []\n",
    "    \n",
    "    for post in all_posts:\n",
    "        # Skip posts without required fields\n",
    "        if not post.get('text') or not post.get('stats'):\n",
    "            continue\n",
    "        \n",
    "        # Extract basic fields\n",
    "        post_text = post.get('text', '')\n",
    "        stats = post.get('stats', {})\n",
    "        author = post.get('author', {})\n",
    "        posted_at = post.get('posted_at', {})\n",
    "        \n",
    "        # Calculate engagement metrics\n",
    "        total_reactions = stats.get('total_reactions', 0)\n",
    "        like_count = stats.get('like', 0)\n",
    "        love_count = stats.get('love', 0)\n",
    "        celebrate_count = stats.get('celebrate', 0)\n",
    "        support_count = stats.get('support', 0)\n",
    "        insight_count = stats.get('insight', 0)\n",
    "        comments_count = stats.get('comments', 0)\n",
    "        reposts_count = stats.get('reposts', 0)\n",
    "        \n",
    "        # Calculate pct_positive (positive reactions / total reactions * 100)\n",
    "        positive_reactions = like_count + love_count + celebrate_count + support_count\n",
    "        if total_reactions > 0:\n",
    "            pct_positive = (positive_reactions / total_reactions) * 100\n",
    "            pct_negative = (insight_count / total_reactions) * 100\n",
    "        else:\n",
    "            pct_positive = 0.0\n",
    "            pct_negative = 0.0\n",
    "        \n",
    "        # Audience size from follower count\n",
    "        audience_size = author.get('follower_count', 0)\n",
    "        if audience_size == 0:\n",
    "            audience_size = 1000  # Default minimum\n",
    "        \n",
    "        # Calculate baseline engagement\n",
    "        total_engagement = total_reactions + comments_count + reposts_count\n",
    "        baseline_engagement = total_engagement / max(audience_size, 1)\n",
    "        \n",
    "        # Calculate comment sentiment distribution (normalized -1 to 1)\n",
    "        if total_reactions > 0:\n",
    "            comment_sentiment_dist = (pct_positive - pct_negative) / 100\n",
    "            comment_sentiment_dist = np.clip(comment_sentiment_dist, -1, 1)\n",
    "        else:\n",
    "            comment_sentiment_dist = 0.0\n",
    "        \n",
    "        # Extract time window from timestamp\n",
    "        timestamp = posted_at.get('timestamp', 0)\n",
    "        if timestamp > 0:\n",
    "            # Convert timestamp (milliseconds) to datetime\n",
    "            post_datetime = datetime.fromtimestamp(timestamp / 1000)\n",
    "            hour = post_datetime.hour\n",
    "            weekday = post_datetime.weekday()  # 0 = Monday, 6 = Sunday\n",
    "            \n",
    "            if weekday >= 5:  # Saturday or Sunday\n",
    "                time_window = 'weekend'\n",
    "            elif hour < 12:\n",
    "                time_window = 'morning'\n",
    "            elif hour < 17:\n",
    "                time_window = 'afternoon'\n",
    "            else:\n",
    "                time_window = 'evening'\n",
    "        else:\n",
    "            time_window = 'afternoon'  # Default\n",
    "        \n",
    "        # Extract affiliation from source_company\n",
    "        affiliation = post.get('source_company', 'Unknown')\n",
    "        if affiliation and isinstance(affiliation, str):\n",
    "            affiliation = affiliation.capitalize()\n",
    "        else:\n",
    "            affiliation = 'Unknown'\n",
    "        \n",
    "        # Infer job_role from company name or use default\n",
    "        # For company posts, we'll use a default role based on the company\n",
    "        company_name = affiliation.lower()\n",
    "        if 'google' in company_name or 'microsoft' in company_name or 'apple' in company_name:\n",
    "            job_role = 'Software Engineer'\n",
    "        elif 'meta' in company_name or 'facebook' in company_name:\n",
    "            job_role = 'Product Manager'\n",
    "        else:\n",
    "            job_role = 'Product Manager'  # Default for company posts\n",
    "        \n",
    "        # Account age - use default since we don't have account creation date\n",
    "        # For company accounts, use a reasonable default (e.g., 5 years)\n",
    "        account_age = 1825  # 5 years in days\n",
    "        \n",
    "        # Calculate engagement velocity\n",
    "        # Estimate based on post age and engagement rate\n",
    "        # Newer posts with high engagement = high velocity\n",
    "        if timestamp > 0:\n",
    "            # Calculate post age in days\n",
    "            current_time = datetime.now().timestamp() * 1000\n",
    "            post_age_days = (current_time - timestamp) / (1000 * 60 * 60 * 24)\n",
    "            \n",
    "            # High engagement rate + recent post = high velocity\n",
    "            engagement_rate = baseline_engagement\n",
    "            if post_age_days < 1:  # Less than 1 day old\n",
    "                velocity_factor = 0.9\n",
    "            elif post_age_days < 7:  # Less than 1 week old\n",
    "                velocity_factor = 0.7\n",
    "            else:\n",
    "                velocity_factor = 0.5\n",
    "            \n",
    "            engagement_velocity = min(engagement_rate * 10 * velocity_factor, 1.0)\n",
    "            engagement_velocity = max(engagement_velocity, 0.0)\n",
    "        else:\n",
    "            engagement_velocity = 0.5  # Default\n",
    "        \n",
    "        transformed_data.append({\n",
    "            'post_text': post_text,\n",
    "            'job_role': job_role,\n",
    "            'affiliation': affiliation,\n",
    "            'account_age': account_age,\n",
    "            'audience_size': audience_size,\n",
    "            'baseline_engagement': baseline_engagement,\n",
    "            'time_window': time_window,\n",
    "            'pct_positive': pct_positive,\n",
    "            'pct_negative': pct_negative,\n",
    "            'comment_sentiment_dist': comment_sentiment_dist,\n",
    "            'engagement_velocity': engagement_velocity\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(transformed_data)\n",
    "    print(f\"✓ Transformed {len(df)} posts\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df: pd.DataFrame, text_embeddings: np.ndarray) -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Prepare feature matrix by combining embeddings with metadata.\n",
    "    Returns: (X, y, feature_info)\n",
    "    \"\"\"\n",
    "    # Encode categorical features\n",
    "    le_job = LabelEncoder()\n",
    "    le_affiliation = LabelEncoder()\n",
    "    le_time = LabelEncoder()\n",
    "    \n",
    "    job_encoded = le_job.fit_transform(df['job_role'])\n",
    "    affiliation_encoded = le_affiliation.fit_transform(df['affiliation'])\n",
    "    time_encoded = le_time.fit_transform(df['time_window'])\n",
    "    \n",
    "    # One-hot encode categorical features for better representation\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    categorical_features = ohe.fit_transform(\n",
    "        df[['job_role', 'affiliation', 'time_window']]\n",
    "    )\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_features = scaler.fit_transform(\n",
    "        df[['account_age', 'audience_size', 'baseline_engagement']]\n",
    "    )\n",
    "    \n",
    "    # Combine all features: embeddings + categorical + numerical\n",
    "    X = np.hstack([\n",
    "        text_embeddings,  # Text embeddings (e.g., 768 dims)\n",
    "        categorical_features,  # One-hot encoded categoricals\n",
    "        numerical_features  # Normalized numerical features\n",
    "    ])\n",
    "    \n",
    "    # Extract target variables\n",
    "    y = df[['pct_positive', 'pct_negative', 'comment_sentiment_dist', 'engagement_velocity']].values\n",
    "    \n",
    "    feature_info = {\n",
    "        'embedding_dim': text_embeddings.shape[1],\n",
    "        'categorical_dim': categorical_features.shape[1],\n",
    "        'numerical_dim': numerical_features.shape[1],\n",
    "        'total_dim': X.shape[1],\n",
    "        'label_encoders': {\n",
    "            'job_role': le_job,\n",
    "            'affiliation': le_affiliation,\n",
    "            'time_window': le_time\n",
    "        },\n",
    "        'one_hot_encoder': ohe,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    \n",
    "    return X, y, feature_info\n",
    "\n",
    "# Prepare features\n",
    "X, y, feature_info = prepare_features(df, text_embeddings)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target matrix shape: {y.shape}\")\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"  - Text embeddings: {feature_info['embedding_dim']} dimensions\")\n",
    "print(f\"  - Categorical features: {feature_info['categorical_dim']} dimensions\")\n",
    "print(f\"  - Numerical features: {feature_info['numerical_dim']} dimensions\")\n",
    "print(f\"  - Total features: {feature_info['total_dim']} dimensions\")\n",
    "print(f\"\\nTarget variables: pct_positive, pct_negative, comment_sentiment_dist, engagement_velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Train XGBoost multi-output regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Create XGBoost model with MultiOutputRegressor wrapper\n",
    "# This trains one XGBoost model per target variable\n",
    "base_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model = MultiOutputRegressor(base_model)\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"✓ Model training complete\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"\\nPredictions shape: {y_test_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Calculate metrics and analyze performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, target_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for each target variable.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, target_name in enumerate(target_names):\n",
    "        y_true_i = y_true[:, i]\n",
    "        y_pred_i = y_pred[:, i]\n",
    "        \n",
    "        mae = mean_absolute_error(y_true_i, y_pred_i)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_i, y_pred_i))\n",
    "        r2 = r2_score(y_true_i, y_pred_i)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation = np.corrcoef(y_true_i, y_pred_i)[0, 1]\n",
    "        \n",
    "        results.append({\n",
    "            'target': target_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2,\n",
    "            'Correlation': correlation\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate on train and test sets\n",
    "target_names = ['pct_positive', 'pct_negative', 'comment_sentiment_dist', 'engagement_velocity']\n",
    "\n",
    "train_metrics = evaluate_model(y_train, y_train_pred, target_names)\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, target_names)\n",
    "\n",
    "print(\"Training Set Metrics:\")\n",
    "print(train_metrics.to_string(index=False))\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(test_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Quantification\n",
    "\n",
    "Estimate prediction uncertainty and identify failure modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_uncertainty(y_true: np.ndarray, y_pred: np.ndarray, target_names: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate prediction errors and uncertainty metrics.\n",
    "    \"\"\"\n",
    "    uncertainty_data = []\n",
    "    \n",
    "    for i, target_name in enumerate(target_names):\n",
    "        y_true_i = y_true[:, i]\n",
    "        y_pred_i = y_pred[:, i]\n",
    "        \n",
    "        # Absolute error\n",
    "        abs_error = np.abs(y_true_i - y_pred_i)\n",
    "        \n",
    "        # Relative error (percentage)\n",
    "        # Avoid division by zero\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            rel_error = np.where(\n",
    "                y_true_i != 0,\n",
    "                abs_error / np.abs(y_true_i) * 100,\n",
    "                abs_error\n",
    "            )\n",
    "        \n",
    "        uncertainty_data.append({\n",
    "            'target': target_name,\n",
    "            'mean_abs_error': np.mean(abs_error),\n",
    "            'std_abs_error': np.std(abs_error),\n",
    "            'max_abs_error': np.max(abs_error),\n",
    "            'mean_rel_error_pct': np.mean(rel_error[rel_error != np.inf]),\n",
    "            'high_uncertainty_threshold': np.percentile(abs_error, 90)  # 90th percentile\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(uncertainty_data)\n",
    "\n",
    "# Calculate uncertainty metrics\n",
    "test_uncertainty = calculate_uncertainty(y_test, y_test_pred, target_names)\n",
    "print(\"Uncertainty Metrics (Test Set):\")\n",
    "print(test_uncertainty.to_string(index=False))\n",
    "\n",
    "# Identify high-uncertainty predictions (potential failure modes)\n",
    "print(\"\\nIdentifying high-uncertainty predictions...\")\n",
    "abs_errors = np.abs(y_test - y_test_pred)\n",
    "high_uncertainty_thresholds = test_uncertainty['high_uncertainty_threshold'].values\n",
    "\n",
    "high_uncertainty_mask = np.any(\n",
    "    abs_errors > high_uncertainty_thresholds.reshape(1, -1),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"High-uncertainty predictions: {np.sum(high_uncertainty_mask)} / {len(y_test)} ({np.sum(high_uncertainty_mask)/len(y_test)*100:.1f}%)\")\n",
    "print(\"\\nThese are potential failure modes where the model is less confident.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Analyze which features contribute most to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from XGBoost models\n",
    "# Since we use MultiOutputRegressor, we have one model per target\n",
    "feature_importance_data = []\n",
    "\n",
    "for i, target_name in enumerate(target_names):\n",
    "    # Get the underlying XGBoost model for this target\n",
    "    xgb_model = model.estimators_[i]\n",
    "    importances = xgb_model.feature_importances_\n",
    "    \n",
    "    # Get top 20 most important features\n",
    "    top_indices = np.argsort(importances)[-20:][::-1]\n",
    "    top_importances = importances[top_indices]\n",
    "    \n",
    "    # Categorize features\n",
    "    feature_categories = []\n",
    "    for idx in top_indices:\n",
    "        if idx < feature_info['embedding_dim']:\n",
    "            feature_categories.append(f'Embedding_{idx}')\n",
    "        elif idx < feature_info['embedding_dim'] + feature_info['categorical_dim']:\n",
    "            feature_categories.append(f'Categorical_{idx - feature_info[\"embedding_dim\"]}')\n",
    "        else:\n",
    "            feature_categories.append(f'Numerical_{idx - feature_info[\"embedding_dim\"] - feature_info[\"categorical_dim\"]}')\n",
    "    \n",
    "    feature_importance_data.append({\n",
    "        'target': target_name,\n",
    "        'top_features': feature_categories,\n",
    "        'top_importances': top_importances\n",
    "    })\n",
    "\n",
    "# Display feature importance summary\n",
    "print(\"Top 10 Most Important Features per Target:\")\n",
    "for data in feature_importance_data:\n",
    "    print(f\"\\n{data['target']}:\")\n",
    "    for feat, imp in zip(data['top_features'][:10], data['top_importances'][:10]):\n",
    "        print(f\"  {feat}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "Create plots to visualize model performance and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prediction vs Actual scatter plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target_name in enumerate(target_names):\n",
    "    ax = axes[i]\n",
    "    y_true_i = y_test[:, i]\n",
    "    y_pred_i = y_test_pred[:, i]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(y_true_i, y_pred_i, alpha=0.6, s=50)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_true_i.min(), y_pred_i.min())\n",
    "    max_val = max(y_true_i.max(), y_pred_i.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Calculate R² for this target\n",
    "    r2 = r2_score(y_true_i, y_pred_i)\n",
    "    ax.set_xlabel(f'Actual {target_name}')\n",
    "    ax.set_ylabel(f'Predicted {target_name}')\n",
    "    ax.set_title(f'{target_name}\\nR² = {r2:.3f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Residual plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target_name in enumerate(target_names):\n",
    "    ax = axes[i]\n",
    "    y_true_i = y_test[:, i]\n",
    "    y_pred_i = y_test_pred[:, i]\n",
    "    residuals = y_true_i - y_pred_i\n",
    "    \n",
    "    ax.scatter(y_pred_i, residuals, alpha=0.6, s=50)\n",
    "    ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    ax.set_xlabel(f'Predicted {target_name}')\n",
    "    ax.set_ylabel('Residuals (Actual - Predicted)')\n",
    "    ax.set_title(f'Residual Plot: {target_name}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Metrics comparison bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(target_names))\n",
    "width = 0.35\n",
    "\n",
    "# Plot R² scores\n",
    "train_r2 = train_metrics['R²'].values\n",
    "test_r2 = test_metrics['R²'].values\n",
    "\n",
    "ax.bar(x - width/2, train_r2, width, label='Train R²', alpha=0.8)\n",
    "ax.bar(x + width/2, test_r2, width, label='Test R²', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Target Variable')\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_title('Model Performance: R² Scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(target_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Error distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target_name in enumerate(target_names):\n",
    "    ax = axes[i]\n",
    "    y_true_i = y_test[:, i]\n",
    "    y_pred_i = y_test_pred[:, i]\n",
    "    abs_errors = np.abs(y_true_i - y_pred_i)\n",
    "    \n",
    "    ax.hist(abs_errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(np.mean(abs_errors), color='r', linestyle='--', lw=2, label=f'Mean: {np.mean(abs_errors):.2f}')\n",
    "    ax.set_xlabel('Absolute Error')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Error Distribution: {target_name}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Correlation heatmap of predictions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Actual values correlation\n",
    "actual_df = pd.DataFrame(y_test, columns=target_names)\n",
    "corr_actual = actual_df.corr()\n",
    "sns.heatmap(corr_actual, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, ax=ax1, cbar_kws={'label': 'Correlation'})\n",
    "ax1.set_title('Correlation: Actual Values')\n",
    "\n",
    "# Predicted values correlation\n",
    "pred_df = pd.DataFrame(y_test_pred, columns=target_names)\n",
    "corr_pred = pred_df.corr()\n",
    "sns.heatmap(corr_pred, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, ax=ax2, cbar_kws={'label': 'Correlation'})\n",
    "ax2.set_title('Correlation: Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature importance visualization (for first target as example)\n",
    "target_idx = 0\n",
    "xgb_model = model.estimators_[target_idx]\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# Get top 15 features\n",
    "top_n = 15\n",
    "top_indices = np.argsort(importances)[-top_n:][::-1]\n",
    "top_importances = importances[top_indices]\n",
    "\n",
    "# Create feature labels\n",
    "feature_labels = []\n",
    "for idx in top_indices:\n",
    "    if idx < feature_info['embedding_dim']:\n",
    "        feature_labels.append(f'Embedding[{idx}]')\n",
    "    elif idx < feature_info['embedding_dim'] + feature_info['categorical_dim']:\n",
    "        feature_labels.append(f'Categorical[{idx - feature_info[\"embedding_dim\"]}]')\n",
    "    else:\n",
    "        num_idx = idx - feature_info['embedding_dim'] - feature_info['categorical_dim']\n",
    "        num_features = ['account_age', 'audience_size', 'baseline_engagement']\n",
    "        feature_labels.append(num_features[num_idx] if num_idx < len(num_features) else f'Num[{num_idx}]')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.barh(range(len(top_importances)), top_importances)\n",
    "ax.set_yticks(range(len(top_importances)))\n",
    "ax.set_yticklabels(feature_labels)\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title(f'Top {top_n} Features: {target_names[target_idx]}')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Model Architecture: Individual & Post-Level Prediction\n",
    "\n",
    "This section implements a two-model pipeline that provides more interpretable and granular predictions:\n",
    "\n",
    "**Model A (Individual-Level)**: Predicts how specific role archetypes react to posts\n",
    "- One row = one person × one post interaction\n",
    "- Predicts: reaction type, comment presence, comment sentiment, comment intent\n",
    "- Focuses on role archetypes (Software Engineer, PM, Founder, etc.), not individuals\n",
    "\n",
    "**Model B (Post-Level)**: Aggregates Model A predictions to forecast overall post performance\n",
    "- One row = one post\n",
    "- Uses aggregated Model A predictions as features\n",
    "- Predicts: total reactions, comment volume, role distribution, engagement quality\n",
    "\n",
    "**Pipeline Flow**: Post → Model A (role predictions) → Aggregate → Model B → Post-level metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model A: Individual-Level Reaction & Comment Model\n",
    "\n",
    "Model A predicts how different role archetypes will react to a post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_individual_level_data(\n",
    "    post_level_df: pd.DataFrame,\n",
    "    interactions_per_post: int = 20,\n",
    "    individual_data_path: Optional[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load or simulate individual-level interaction data (person × post).\n",
    "    \n",
    "    Expected columns if loading from file:\n",
    "    - Post features: post_text, post_topic, post_author_role, post_length, hashtags, mentions, urls_present, time_posted\n",
    "    - Person features: commenter_role, commenter_industry, commenter_seniority\n",
    "    - Labels: reaction_type, commented, comment_text, comment_sentiment, comment_intent\n",
    "    \n",
    "    If data not available, simulates individual interactions from post-level data.\n",
    "    \"\"\"\n",
    "    # Try to load existing individual-level data\n",
    "    if individual_data_path and os.path.exists(individual_data_path):\n",
    "        print(f\"Loading individual-level data from {individual_data_path}...\")\n",
    "        if individual_data_path.endswith('.csv'):\n",
    "            df_individual = pd.read_csv(individual_data_path)\n",
    "        elif individual_data_path.endswith('.json'):\n",
    "            df_individual = pd.read_json(individual_data_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {individual_data_path}\")\n",
    "        print(f\"✓ Loaded {len(df_individual)} individual interactions\")\n",
    "        return df_individual\n",
    "    \n",
    "    # Simulate individual-level data from post-level data\n",
    "    print(f\"Simulating individual-level interactions ({interactions_per_post} per post)...\")\n",
    "    \n",
    "    # Role archetypes with normalized buckets\n",
    "    role_archetypes = {\n",
    "        'Software Engineer': {'industry': 'Tech', 'seniority': 'Mid'},\n",
    "        'Product Manager': {'industry': 'Tech', 'seniority': 'Mid'},\n",
    "        'Data Scientist': {'industry': 'Tech', 'seniority': 'Mid'},\n",
    "        'CEO': {'industry': 'Tech', 'seniority': 'Exec'},\n",
    "        'CTO': {'industry': 'Tech', 'seniority': 'Exec'},\n",
    "        'Founder': {'industry': 'Tech', 'seniority': 'Exec'},\n",
    "        'Recruiter': {'industry': 'Tech', 'seniority': 'Mid'},\n",
    "        'Marketing Director': {'industry': 'Tech', 'seniority': 'Senior'},\n",
    "        'Designer': {'industry': 'Tech', 'seniority': 'Mid'},\n",
    "        'Consultant': {'industry': 'Consulting', 'seniority': 'Senior'},\n",
    "        'Student': {'industry': 'Education', 'seniority': 'Junior'}\n",
    "    }\n",
    "    \n",
    "    industries = ['Tech', 'Healthcare', 'Finance', 'Education', 'Consulting']\n",
    "    seniorities = ['Junior', 'Mid', 'Senior', 'Exec']\n",
    "    reaction_types = ['like', 'love', 'insightful', 'celebrate', 'none']\n",
    "    comment_intents = ['praise', 'agreement', 'question', 'criticism', 'insight_addition', 'none']\n",
    "    \n",
    "    individual_data = []\n",
    "    \n",
    "    for idx, post_row in post_level_df.iterrows():\n",
    "        post_text = post_row['post_text']\n",
    "        post_topic = 'AI' if 'AI' in post_text or 'artificial intelligence' in post_text.lower() else \\\n",
    "                     'startups' if 'startup' in post_text.lower() else \\\n",
    "                     'technology' if 'tech' in post_text.lower() else 'general'\n",
    "        \n",
    "        # Sample role archetypes for this post\n",
    "        sampled_roles = np.random.choice(list(role_archetypes.keys()), \n",
    "                                        size=min(interactions_per_post, len(role_archetypes)),\n",
    "                                        replace=False)\n",
    "        \n",
    "        for role in sampled_roles:\n",
    "            role_info = role_archetypes[role]\n",
    "            \n",
    "            # Determine reaction based on role-post alignment\n",
    "            # Engineers more likely to react to tech posts, founders to startup posts, etc.\n",
    "            base_reaction_prob = 0.7\n",
    "            if (role in ['Software Engineer', 'Data Scientist', 'CTO'] and post_topic in ['AI', 'technology']) or \\\n",
    "               (role == 'Founder' and post_topic == 'startups') or \\\n",
    "               (role == 'CEO' and post_topic in ['startups', 'technology']):\n",
    "                base_reaction_prob = 0.85\n",
    "            \n",
    "            # Reaction type\n",
    "            if np.random.random() < base_reaction_prob:\n",
    "                reaction_type = np.random.choice(['like', 'love', 'insightful', 'celebrate'], \n",
    "                                                p=[0.5, 0.2, 0.2, 0.1])\n",
    "            else:\n",
    "                reaction_type = 'none'\n",
    "            \n",
    "            # Comment probability (lower than reaction)\n",
    "            comment_prob = 0.15 if reaction_type != 'none' else 0.02\n",
    "            commented = np.random.random() < comment_prob\n",
    "            \n",
    "            # Comment sentiment (if commented)\n",
    "            if commented:\n",
    "                # Sentiment based on reaction type\n",
    "                if reaction_type in ['love', 'celebrate']:\n",
    "                    comment_sentiment = np.random.uniform(0.6, 1.0)\n",
    "                    comment_intent = np.random.choice(['praise', 'agreement'], p=[0.6, 0.4])\n",
    "                elif reaction_type == 'insightful':\n",
    "                    comment_sentiment = np.random.uniform(0.3, 0.8)\n",
    "                    comment_intent = np.random.choice(['insight_addition', 'agreement', 'question'], p=[0.5, 0.3, 0.2])\n",
    "                elif reaction_type == 'like':\n",
    "                    comment_sentiment = np.random.uniform(0.0, 0.7)\n",
    "                    comment_intent = np.random.choice(['agreement', 'question', 'praise'], p=[0.4, 0.3, 0.3])\n",
    "                else:\n",
    "                    comment_sentiment = np.random.uniform(-0.5, 0.5)\n",
    "                    comment_intent = np.random.choice(['criticism', 'question'], p=[0.4, 0.6])\n",
    "                \n",
    "                comment_text = f\"Sample comment from {role}\"\n",
    "            else:\n",
    "                comment_sentiment = 0.0\n",
    "                comment_intent = 'none'\n",
    "                comment_text = ''\n",
    "            \n",
    "            # Post metadata\n",
    "            post_length = len(post_text.split())\n",
    "            has_hashtags = '#' in post_text\n",
    "            has_mentions = '@' in post_text\n",
    "            has_urls = 'http' in post_text.lower()\n",
    "            \n",
    "            individual_data.append({\n",
    "                # Post features\n",
    "                'post_text': post_text,\n",
    "                'post_topic': post_topic,\n",
    "                'post_author_role': post_row.get('job_role', 'CEO'),\n",
    "                'post_length': post_length,\n",
    "                'hashtags': has_hashtags,\n",
    "                'mentions': has_mentions,\n",
    "                'urls_present': has_urls,\n",
    "                'time_posted': post_row.get('time_window', 'morning'),\n",
    "                \n",
    "                # Person features\n",
    "                'commenter_role': role,\n",
    "                'commenter_industry': role_info['industry'],\n",
    "                'commenter_seniority': role_info['seniority'],\n",
    "                \n",
    "                # Labels\n",
    "                'reaction_type': reaction_type,\n",
    "                'commented': commented,\n",
    "                'comment_text': comment_text,\n",
    "                'comment_sentiment': comment_sentiment,\n",
    "                'comment_intent': comment_intent\n",
    "            })\n",
    "    \n",
    "    df_individual = pd.DataFrame(individual_data)\n",
    "    print(f\"✓ Generated {len(df_individual)} individual interactions from {len(post_level_df)} posts\")\n",
    "    return df_individual\n",
    "\n",
    "# Load individual-level data\n",
    "INDIVIDUAL_DATA_PATH = 'individual_interactions.csv'  # Set to None to simulate\n",
    "df_individual = load_individual_level_data(df, interactions_per_post=15)\n",
    "print(f\"\\\\nIndividual-level dataset shape: {df_individual.shape}\")\n",
    "print(f\"\\\\nFirst few rows:\")\n",
    "df_individual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_a_features(df_individual: pd.DataFrame, text_embeddings_dict: Optional[Dict] = None) -> Tuple[np.ndarray, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Prepare features for Model A (individual-level predictions).\n",
    "    \n",
    "    Features:\n",
    "    - Post text embeddings\n",
    "    - Post metadata (topic, author role, length, hashtags, mentions, URLs, time)\n",
    "    - Person features (role, industry, seniority)\n",
    "    \n",
    "    Returns:\n",
    "    - X: Feature matrix\n",
    "    - y_dict: Dictionary of target variables\n",
    "    - feature_info_a: Feature information (encoders, scalers)\n",
    "    \"\"\"\n",
    "    print(\"Preparing Model A features...\")\n",
    "    \n",
    "    # Generate embeddings for unique posts (cache to avoid recomputation)\n",
    "    if text_embeddings_dict is None:\n",
    "        unique_posts = df_individual[['post_text']].drop_duplicates()\n",
    "        unique_posts['post_text_clean'] = unique_posts['post_text'].str.lower().str.strip()\n",
    "        print(f\"Generating embeddings for {len(unique_posts)} unique posts...\")\n",
    "        post_embeddings = generate_embeddings(unique_posts['post_text_clean'].tolist())\n",
    "        text_embeddings_dict = dict(zip(unique_posts['post_text'], post_embeddings))\n",
    "    \n",
    "    # Map embeddings to each row\n",
    "    df_individual['post_text_clean'] = df_individual['post_text'].str.lower().str.strip()\n",
    "    post_embeddings_list = [text_embeddings_dict[post] for post in df_individual['post_text']]\n",
    "    post_embeddings_array = np.array(post_embeddings_list)\n",
    "    \n",
    "    # Encode post metadata\n",
    "    le_post_topic = LabelEncoder()\n",
    "    le_post_author = LabelEncoder()\n",
    "    le_time = LabelEncoder()\n",
    "    \n",
    "    post_topic_encoded = le_post_topic.fit_transform(df_individual['post_topic'])\n",
    "    post_author_encoded = le_post_author.fit_transform(df_individual['post_author_role'])\n",
    "    time_encoded = le_time.fit_transform(df_individual['time_posted'])\n",
    "    \n",
    "    # One-hot encode post categoricals\n",
    "    ohe_post = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    post_categorical = ohe_post.fit_transform(\n",
    "        df_individual[['post_topic', 'post_author_role', 'time_posted']]\n",
    "    )\n",
    "    \n",
    "    # Post numerical features\n",
    "    post_numerical = df_individual[['post_length', 'hashtags', 'mentions', 'urls_present']].astype(float).values\n",
    "    scaler_post_numerical = StandardScaler()\n",
    "    post_numerical_scaled = scaler_post_numerical.fit_transform(post_numerical)\n",
    "    \n",
    "    # Encode person features\n",
    "    le_role = LabelEncoder()\n",
    "    le_industry = LabelEncoder()\n",
    "    le_seniority = LabelEncoder()\n",
    "    \n",
    "    role_encoded = le_role.fit_transform(df_individual['commenter_role'])\n",
    "    industry_encoded = le_industry.fit_transform(df_individual['commenter_industry'])\n",
    "    seniority_encoded = le_seniority.fit_transform(df_individual['commenter_seniority'])\n",
    "    \n",
    "    # One-hot encode person categoricals\n",
    "    ohe_person = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    person_categorical = ohe_person.fit_transform(\n",
    "        df_individual[['commenter_role', 'commenter_industry', 'commenter_seniority']]\n",
    "    )\n",
    "    \n",
    "    # Combine all features\n",
    "    X = np.hstack([\n",
    "        post_embeddings_array,  # Post text embeddings\n",
    "        post_categorical,  # Post categorical features\n",
    "        post_numerical_scaled,  # Post numerical features\n",
    "        person_categorical  # Person features\n",
    "    ])\n",
    "    \n",
    "    # Prepare target variables\n",
    "    y_dict = {\n",
    "        'reaction_type': df_individual['reaction_type'].values,\n",
    "        'commented': df_individual['commented'].astype(int).values,\n",
    "        'comment_sentiment': df_individual['comment_sentiment'].values,\n",
    "        'comment_intent': df_individual['comment_intent'].values\n",
    "    }\n",
    "    \n",
    "    feature_info_a = {\n",
    "        'embedding_dim': post_embeddings_array.shape[1],\n",
    "        'post_categorical_dim': post_categorical.shape[1],\n",
    "        'post_numerical_dim': post_numerical_scaled.shape[1],\n",
    "        'person_categorical_dim': person_categorical.shape[1],\n",
    "        'total_dim': X.shape[1],\n",
    "        'text_embeddings_dict': text_embeddings_dict,\n",
    "        'encoders': {\n",
    "            'post_topic': le_post_topic,\n",
    "            'post_author': le_post_author,\n",
    "            'time': le_time,\n",
    "            'role': le_role,\n",
    "            'industry': le_industry,\n",
    "            'seniority': le_seniority\n",
    "        },\n",
    "        'one_hot_encoders': {\n",
    "            'post': ohe_post,\n",
    "            'person': ohe_person\n",
    "        },\n",
    "        'scaler_post_numerical': scaler_post_numerical\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Feature matrix shape: {X.shape}\")\n",
    "    print(f\"  - Post embeddings: {post_embeddings_array.shape[1]} dims\")\n",
    "    print(f\"  - Post categorical: {post_categorical.shape[1]} dims\")\n",
    "    print(f\"  - Post numerical: {post_numerical_scaled.shape[1]} dims\")\n",
    "    print(f\"  - Person categorical: {person_categorical.shape[1]} dims\")\n",
    "    \n",
    "    return X, y_dict, feature_info_a\n",
    "\n",
    "# Prepare Model A features\n",
    "X_a, y_dict_a, feature_info_a = prepare_model_a_features(df_individual)\n",
    "print(f\"\\\\nTarget variables:\")\n",
    "for key, values in y_dict_a.items():\n",
    "    if key == 'comment_sentiment':\n",
    "        print(f\"  {key}: {len(values)} values, range [{values.min():.2f}, {values.max():.2f}]\")\n",
    "    else:\n",
    "        unique_vals = np.unique(values)\n",
    "        print(f\"  {key}: {len(unique_vals)} unique values - {list(unique_vals)[:5]}{'...' if len(unique_vals) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for Model A\n",
    "X_a_train, X_a_test, y_a_train, y_a_test = train_test_split(\n",
    "    X_a, y_dict_a, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Model A Training set: {X_a_train.shape[0]} samples\")\n",
    "print(f\"Model A Test set: {X_a_test.shape[0]} samples\")\n",
    "\n",
    "# Train Model A components\n",
    "print(\"\\\\nTraining Model A components...\")\n",
    "\n",
    "# 1. Reaction type classifier (multi-class)\n",
    "print(\"  1. Training reaction type classifier...\")\n",
    "model_a_reaction = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "model_a_reaction.fit(X_a_train, y_a_train['reaction_type'])\n",
    "print(\"     ✓ Reaction type classifier trained\")\n",
    "\n",
    "# 2. Comment presence classifier (binary)\n",
    "print(\"  2. Training comment presence classifier...\")\n",
    "model_a_comment = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "model_a_comment.fit(X_a_train, y_a_train['commented'])\n",
    "print(\"     ✓ Comment presence classifier trained\")\n",
    "\n",
    "# 3. Comment sentiment regressor (only for commented=True)\n",
    "print(\"  3. Training comment sentiment regressor...\")\n",
    "commented_mask_train = y_a_train['commented'] == 1\n",
    "commented_mask_test = y_a_test['commented'] == 1\n",
    "\n",
    "if commented_mask_train.sum() > 0:\n",
    "    model_a_sentiment = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_a_sentiment.fit(X_a_train[commented_mask_train], y_a_train['comment_sentiment'][commented_mask_train])\n",
    "    print(f\"     ✓ Comment sentiment regressor trained ({commented_mask_train.sum()} samples)\")\n",
    "else:\n",
    "    model_a_sentiment = None\n",
    "    print(\"     ⚠ No comments in training data, skipping sentiment model\")\n",
    "\n",
    "# 4. Comment intent classifier (only for commented=True)\n",
    "print(\"  4. Training comment intent classifier...\")\n",
    "if commented_mask_train.sum() > 0:\n",
    "    model_a_intent = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    model_a_intent.fit(X_a_train[commented_mask_train], y_a_train['comment_intent'][commented_mask_train])\n",
    "    print(f\"     ✓ Comment intent classifier trained ({commented_mask_train.sum()} samples)\")\n",
    "else:\n",
    "    model_a_intent = None\n",
    "    print(\"     ⚠ No comments in training data, skipping intent model\")\n",
    "\n",
    "print(\"\\\\n✓ Model A training complete!\")\n",
    "\n",
    "# Store models\n",
    "model_a = {\n",
    "    'reaction': model_a_reaction,\n",
    "    'comment': model_a_comment,\n",
    "    'sentiment': model_a_sentiment,\n",
    "    'intent': model_a_intent\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model A\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL A EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Reaction type evaluation\n",
    "print(\"\\\\n1. Reaction Type Classification:\")\n",
    "y_reaction_pred = model_a_reaction.predict(X_a_test)\n",
    "reaction_accuracy = accuracy_score(y_a_test['reaction_type'], y_reaction_pred)\n",
    "print(f\"   Accuracy: {reaction_accuracy:.3f}\")\n",
    "print(f\"\\\\n   Classification Report:\")\n",
    "print(classification_report(y_a_test['reaction_type'], y_reaction_pred, \n",
    "                          target_names=sorted(np.unique(y_a_test['reaction_type']))))\n",
    "\n",
    "# 2. Comment presence evaluation\n",
    "print(\"\\\\n2. Comment Presence Classification:\")\n",
    "y_comment_pred = model_a_comment.predict(X_a_test)\n",
    "y_comment_proba = model_a_comment.predict_proba(X_a_test)[:, 1]\n",
    "comment_accuracy = accuracy_score(y_a_test['commented'], y_comment_pred)\n",
    "comment_auc = roc_auc_score(y_a_test['commented'], y_comment_proba)\n",
    "print(f\"   Accuracy: {comment_accuracy:.3f}\")\n",
    "print(f\"   ROC-AUC: {comment_auc:.3f}\")\n",
    "print(f\"\\\\n   Classification Report:\")\n",
    "print(classification_report(y_a_test['commented'], y_comment_pred, \n",
    "                          target_names=['No Comment', 'Commented']))\n",
    "\n",
    "# 3. Comment sentiment evaluation (only for commented samples)\n",
    "if model_a_sentiment is not None and commented_mask_test.sum() > 0:\n",
    "    print(\"\\\\n3. Comment Sentiment Regression:\")\n",
    "    y_sentiment_pred = model_a_sentiment.predict(X_a_test[commented_mask_test])\n",
    "    y_sentiment_true = y_a_test['comment_sentiment'][commented_mask_test]\n",
    "    sentiment_mae = mean_absolute_error(y_sentiment_true, y_sentiment_pred)\n",
    "    sentiment_rmse = np.sqrt(mean_squared_error(y_sentiment_true, y_sentiment_pred))\n",
    "    sentiment_r2 = r2_score(y_sentiment_true, y_sentiment_pred)\n",
    "    print(f\"   MAE: {sentiment_mae:.3f}\")\n",
    "    print(f\"   RMSE: {sentiment_rmse:.3f}\")\n",
    "    print(f\"   R²: {sentiment_r2:.3f}\")\n",
    "\n",
    "# 4. Comment intent evaluation (only for commented samples)\n",
    "if model_a_intent is not None and commented_mask_test.sum() > 0:\n",
    "    print(\"\\\\n4. Comment Intent Classification:\")\n",
    "    y_intent_pred = model_a_intent.predict(X_a_test[commented_mask_test])\n",
    "    y_intent_true = y_a_test['comment_intent'][commented_mask_test]\n",
    "    intent_accuracy = accuracy_score(y_intent_true, y_intent_pred)\n",
    "    print(f\"   Accuracy: {intent_accuracy:.3f}\")\n",
    "    print(f\"\\\\n   Classification Report:\")\n",
    "    print(classification_report(y_intent_true, y_intent_pred, \n",
    "                              target_names=sorted(np.unique(y_intent_true))))\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model A performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Reaction type confusion matrix\n",
    "ax1 = axes[0, 0]\n",
    "reaction_types = sorted(np.unique(y_a_test['reaction_type']))\n",
    "cm_reaction = confusion_matrix(y_a_test['reaction_type'], y_reaction_pred, labels=reaction_types)\n",
    "sns.heatmap(cm_reaction, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=reaction_types, yticklabels=reaction_types)\n",
    "ax1.set_xlabel('Predicted', fontsize=12)\n",
    "ax1.set_ylabel('Actual', fontsize=12)\n",
    "ax1.set_title('Reaction Type Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Comment presence distribution\n",
    "ax2 = axes[0, 1]\n",
    "comment_counts = pd.Series(y_a_test['commented']).value_counts().sort_index()\n",
    "comment_pred_counts = pd.Series(y_comment_pred).value_counts().sort_index()\n",
    "x = np.arange(len(comment_counts))\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, comment_counts.values, width, label='Actual', alpha=0.7)\n",
    "ax2.bar(x + width/2, comment_pred_counts.values, width, label='Predicted', alpha=0.7)\n",
    "ax2.set_xlabel('Commented', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Comment Presence: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['No', 'Yes'])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Comment sentiment distribution (if available)\n",
    "ax3 = axes[1, 0]\n",
    "if model_a_sentiment is not None and commented_mask_test.sum() > 0:\n",
    "    ax3.scatter(y_sentiment_true, y_sentiment_pred, alpha=0.6, s=50)\n",
    "    min_val = min(y_sentiment_true.min(), y_sentiment_pred.min())\n",
    "    max_val = max(y_sentiment_true.max(), y_sentiment_pred.max())\n",
    "    ax3.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    ax3.set_xlabel('Actual Sentiment', fontsize=12)\n",
    "    ax3.set_ylabel('Predicted Sentiment', fontsize=12)\n",
    "    ax3.set_title(f'Comment Sentiment Prediction (R² = {sentiment_r2:.3f})', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'No comment data available', ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Comment Sentiment Prediction', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Reaction type distribution by role (sample)\n",
    "ax4 = axes[1, 1]\n",
    "# Get a sample of roles\n",
    "sample_roles = df_individual['commenter_role'].value_counts().head(5).index\n",
    "role_reaction_data = []\n",
    "for role in sample_roles:\n",
    "    role_mask = df_individual['commenter_role'] == role\n",
    "    reactions = df_individual[role_mask]['reaction_type'].value_counts()\n",
    "    for reaction_type, count in reactions.items():\n",
    "        role_reaction_data.append({'role': role, 'reaction': reaction_type, 'count': count})\n",
    "role_reaction_df = pd.DataFrame(role_reaction_data)\n",
    "if len(role_reaction_df) > 0:\n",
    "    role_reaction_pivot = role_reaction_df.pivot(index='role', columns='reaction', values='count').fillna(0)\n",
    "    role_reaction_pivot.plot(kind='bar', stacked=True, ax=ax4, colormap='Set3')\n",
    "    ax4.set_xlabel('Role', fontsize=12)\n",
    "    ax4.set_ylabel('Count', fontsize=12)\n",
    "    ax4.set_title('Reaction Distribution by Role (Sample)', fontsize=14, fontweight='bold')\n",
    "    ax4.legend(title='Reaction Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No role data available', ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Reaction Distribution by Role', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model B: Post-Level Aggregation & Impact Model\n",
    "\n",
    "Model B aggregates Model A predictions to forecast overall post performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_model_a_predictions(\n",
    "    post_text: str,\n",
    "    post_metadata: Dict,\n",
    "    role_archetypes: List[str],\n",
    "    model_a: Dict,\n",
    "    feature_info_a: Dict,\n",
    "    generate_embeddings_func\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Use Model A to predict reactions for different role archetypes, then aggregate.\n",
    "    \n",
    "    Returns aggregated features for Model B:\n",
    "    - Reaction counts by type\n",
    "    - Comment probability by role\n",
    "    - Average sentiment by role\n",
    "    - Role distribution of engagement\n",
    "    \"\"\"\n",
    "    # Prepare features for each role archetype\n",
    "    predictions_by_role = {}\n",
    "    \n",
    "    # Get post embedding\n",
    "    post_text_clean = post_text.lower().strip()\n",
    "    if post_text in feature_info_a['text_embeddings_dict']:\n",
    "        post_embedding = feature_info_a['text_embeddings_dict'][post_text]\n",
    "    else:\n",
    "        post_embedding = generate_embeddings_func([post_text_clean])[0]\n",
    "    \n",
    "    # Prepare post features (same for all roles)\n",
    "    le_post_topic = feature_info_a['encoders']['post_topic']\n",
    "    le_post_author = feature_info_a['encoders']['post_author']\n",
    "    le_time = feature_info_a['encoders']['time']\n",
    "    ohe_post = feature_info_a['one_hot_encoders']['post']\n",
    "    scaler_post = feature_info_a['scaler_post_numerical']\n",
    "    \n",
    "    # Encode post metadata\n",
    "    post_topic_encoded = le_post_topic.transform([post_metadata.get('topic', 'general')])[0]\n",
    "    post_author_encoded = le_post_author.transform([post_metadata.get('author_role', 'CEO')])[0]\n",
    "    time_encoded = le_time.transform([post_metadata.get('time_posted', 'morning')])[0]\n",
    "    \n",
    "    post_categorical = ohe_post.transform([[\n",
    "        post_metadata.get('topic', 'general'),\n",
    "        post_metadata.get('author_role', 'CEO'),\n",
    "        post_metadata.get('time_posted', 'morning')\n",
    "    ]])\n",
    "    \n",
    "    post_length = post_metadata.get('post_length', len(post_text.split()))\n",
    "    has_hashtags = post_metadata.get('hashtags', '#' in post_text)\n",
    "    has_mentions = post_metadata.get('mentions', '@' in post_text)\n",
    "    has_urls = post_metadata.get('urls_present', 'http' in post_text.lower())\n",
    "    \n",
    "    post_numerical = scaler_post.transform([[post_length, has_hashtags, has_mentions, has_urls]])\n",
    "    \n",
    "    # Predict for each role\n",
    "    le_role = feature_info_a['encoders']['role']\n",
    "    le_industry = feature_info_a['encoders']['industry']\n",
    "    le_seniority = feature_info_a['encoders']['seniority']\n",
    "    ohe_person = feature_info_a['one_hot_encoders']['person']\n",
    "    \n",
    "    # Role archetype mappings (simplified - in practice, use actual data)\n",
    "    role_industry_map = {\n",
    "        'Software Engineer': 'Tech', 'Product Manager': 'Tech', 'Data Scientist': 'Tech',\n",
    "        'CEO': 'Tech', 'CTO': 'Tech', 'Founder': 'Tech', 'Recruiter': 'Tech',\n",
    "        'Marketing Director': 'Tech', 'Designer': 'Tech', 'Consultant': 'Consulting',\n",
    "        'Student': 'Education'\n",
    "    }\n",
    "    role_seniority_map = {\n",
    "        'Software Engineer': 'Mid', 'Product Manager': 'Mid', 'Data Scientist': 'Mid',\n",
    "        'CEO': 'Exec', 'CTO': 'Exec', 'Founder': 'Exec', 'Recruiter': 'Mid',\n",
    "        'Marketing Director': 'Senior', 'Designer': 'Mid', 'Consultant': 'Senior',\n",
    "        'Student': 'Junior'\n",
    "    }\n",
    "    \n",
    "    for role in role_archetypes:\n",
    "        try:\n",
    "            # Encode person features\n",
    "            industry = role_industry_map.get(role, 'Tech')\n",
    "            seniority = role_seniority_map.get(role, 'Mid')\n",
    "            \n",
    "            person_categorical = ohe_person.transform([[role, industry, seniority]])\n",
    "            \n",
    "            # Combine features\n",
    "            X_role = np.hstack([\n",
    "                post_embedding.reshape(1, -1),\n",
    "                post_categorical,\n",
    "                post_numerical,\n",
    "                person_categorical\n",
    "            ])\n",
    "            \n",
    "            # Make predictions\n",
    "            reaction_pred = model_a['reaction'].predict(X_role)[0]\n",
    "            reaction_proba = model_a['reaction'].predict_proba(X_role)[0]\n",
    "            comment_pred = model_a['comment'].predict(X_role)[0]\n",
    "            comment_proba = model_a['comment'].predict_proba(X_role)[0, 1]\n",
    "            \n",
    "            sentiment_pred = 0.0\n",
    "            intent_pred = 'none'\n",
    "            if comment_pred == 1 and model_a['sentiment'] is not None:\n",
    "                sentiment_pred = model_a['sentiment'].predict(X_role)[0]\n",
    "                if model_a['intent'] is not None:\n",
    "                    intent_pred = model_a['intent'].predict(X_role)[0]\n",
    "            \n",
    "            predictions_by_role[role] = {\n",
    "                'reaction_type': reaction_pred,\n",
    "                'reaction_proba': reaction_proba,\n",
    "                'commented': comment_pred,\n",
    "                'comment_proba': comment_proba,\n",
    "                'sentiment': sentiment_pred,\n",
    "                'intent': intent_pred\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Skip roles not in training data\n",
    "            continue\n",
    "    \n",
    "    # Aggregate predictions\n",
    "    if len(predictions_by_role) == 0:\n",
    "        # Return defaults if no predictions\n",
    "        return {\n",
    "            'pct_engineers_reacting': 0.0,\n",
    "            'pct_pms_commenting': 0.0,\n",
    "            'pct_founders_engaging': 0.0,\n",
    "            'avg_sentiment_per_role': {},\n",
    "            'predicted_reaction_distribution': {},\n",
    "            'predicted_comment_volume': 0.0\n",
    "        }\n",
    "    \n",
    "    # Calculate role-specific metrics\n",
    "    engineer_roles = ['Software Engineer', 'Data Scientist', 'CTO']\n",
    "    pm_roles = ['Product Manager']\n",
    "    founder_roles = ['Founder', 'CEO']\n",
    "    \n",
    "    engineers_reacting = sum(1 for r in engineer_roles \n",
    "                            if r in predictions_by_role and predictions_by_role[r]['reaction_type'] != 'none')\n",
    "    pct_engineers_reacting = engineers_reacting / len(engineer_roles) if engineer_roles else 0.0\n",
    "    \n",
    "    pms_commenting = sum(1 for r in pm_roles \n",
    "                        if r in predictions_by_role and predictions_by_role[r]['commented'] == 1)\n",
    "    pct_pms_commenting = pms_commenting / len(pm_roles) if pm_roles else 0.0\n",
    "    \n",
    "    founders_engaging = sum(1 for r in founder_roles \n",
    "                           if r in predictions_by_role and predictions_by_role[r]['reaction_type'] != 'none')\n",
    "    pct_founders_engaging = founders_engaging / len(founder_roles) if founder_roles else 0.0\n",
    "    \n",
    "    # Average sentiment by role\n",
    "    avg_sentiment_per_role = {}\n",
    "    for role, preds in predictions_by_role.items():\n",
    "        if preds['commented'] == 1:\n",
    "            avg_sentiment_per_role[role] = preds['sentiment']\n",
    "    \n",
    "    # Reaction distribution\n",
    "    reaction_counts = {}\n",
    "    for preds in predictions_by_role.values():\n",
    "        rt = preds['reaction_type']\n",
    "        reaction_counts[rt] = reaction_counts.get(rt, 0) + 1\n",
    "    total_reactions = sum(reaction_counts.values())\n",
    "    predicted_reaction_distribution = {k: v/total_reactions if total_reactions > 0 else 0 \n",
    "                                      for k, v in reaction_counts.items()}\n",
    "    \n",
    "    # Predicted comment volume (sum of comment probabilities)\n",
    "    predicted_comment_volume = sum(preds['comment_proba'] for preds in predictions_by_role.values())\n",
    "    \n",
    "    return {\n",
    "        'pct_engineers_reacting': pct_engineers_reacting,\n",
    "        'pct_pms_commenting': pct_pms_commenting,\n",
    "        'pct_founders_engaging': pct_founders_engaging,\n",
    "        'avg_sentiment_per_role': avg_sentiment_per_role,\n",
    "        'predicted_reaction_distribution': predicted_reaction_distribution,\n",
    "        'predicted_comment_volume': predicted_comment_volume\n",
    "    }\n",
    "\n",
    "print(\"✓ Aggregation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_b_features(\n",
    "    post_level_df: pd.DataFrame,\n",
    "    model_a: Dict,\n",
    "    feature_info_a: Dict,\n",
    "    generate_embeddings_func\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Prepare features for Model B (post-level predictions).\n",
    "    \n",
    "    Features:\n",
    "    - Post text embeddings\n",
    "    - Post metadata\n",
    "    - Aggregated Model A predictions (role distributions, engagement patterns)\n",
    "    \n",
    "    Returns:\n",
    "    - X_b: Feature matrix\n",
    "    - y_b: Target variables (total reactions, comment volume, role distribution, engagement quality)\n",
    "    - feature_info_b: Feature information\n",
    "    \"\"\"\n",
    "    print(\"Preparing Model B features...\")\n",
    "    print(\"This may take a while as we aggregate Model A predictions for each post...\")\n",
    "    \n",
    "    # Get unique posts\n",
    "    unique_posts = post_level_df[['post_text']].drop_duplicates()\n",
    "    unique_posts['post_text_clean'] = unique_posts['post_text'].str.lower().str.strip()\n",
    "    \n",
    "    # Generate embeddings for unique posts\n",
    "    print(f\"Generating embeddings for {len(unique_posts)} unique posts...\")\n",
    "    post_embeddings = generate_embeddings(unique_posts['post_text_clean'].tolist())\n",
    "    text_embeddings_dict_b = dict(zip(unique_posts['post_text'], post_embeddings))\n",
    "    \n",
    "    # Get all role archetypes from training data\n",
    "    role_archetypes = sorted(df_individual['commenter_role'].unique().tolist())\n",
    "    \n",
    "    # For each post, aggregate Model A predictions\n",
    "    aggregated_features_list = []\n",
    "    post_features_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    for idx, post_row in post_level_df.iterrows():\n",
    "        post_text = post_row['post_text']\n",
    "        \n",
    "        # Get post embedding\n",
    "        post_embedding = text_embeddings_dict_b[post_text]\n",
    "        \n",
    "        # Prepare post metadata\n",
    "        post_topic = 'AI' if 'AI' in post_text or 'artificial intelligence' in post_text.lower() else \\\n",
    "                     'startups' if 'startup' in post_text.lower() else \\\n",
    "                     'technology' if 'tech' in post_text.lower() else 'general'\n",
    "        \n",
    "        post_metadata = {\n",
    "            'topic': post_topic,\n",
    "            'author_role': post_row.get('job_role', 'CEO'),\n",
    "            'time_posted': post_row.get('time_window', 'morning'),\n",
    "            'post_length': len(post_text.split()),\n",
    "            'hashtags': '#' in post_text,\n",
    "            'mentions': '@' in post_text,\n",
    "            'urls_present': 'http' in post_text.lower()\n",
    "        }\n",
    "        \n",
    "        # Aggregate Model A predictions\n",
    "        aggregated = aggregate_model_a_predictions(\n",
    "            post_text=post_text,\n",
    "            post_metadata=post_metadata,\n",
    "            role_archetypes=role_archetypes,\n",
    "            model_a=model_a,\n",
    "            feature_info_a=feature_info_a,\n",
    "            generate_embeddings_func=generate_embeddings_func\n",
    "        )\n",
    "        \n",
    "        # Flatten aggregated features\n",
    "        aggregated_features = [\n",
    "            aggregated['pct_engineers_reacting'],\n",
    "            aggregated['pct_pms_commenting'],\n",
    "            aggregated['pct_founders_engaging'],\n",
    "            aggregated['predicted_comment_volume']\n",
    "        ]\n",
    "        \n",
    "        # Add reaction distribution features\n",
    "        reaction_types = ['like', 'love', 'insightful', 'celebrate', 'none']\n",
    "        for rt in reaction_types:\n",
    "            aggregated_features.append(aggregated['predicted_reaction_distribution'].get(rt, 0.0))\n",
    "        \n",
    "        # Add average sentiment (flatten dict)\n",
    "        role_sentiments = aggregated['avg_sentiment_per_role']\n",
    "        for role in role_archetypes[:5]:  # Top 5 roles\n",
    "            aggregated_features.append(role_sentiments.get(role, 0.0))\n",
    "        \n",
    "        aggregated_features_list.append(aggregated_features)\n",
    "        post_features_list.append(post_embedding)\n",
    "        \n",
    "        # Calculate targets from individual-level data for this post\n",
    "        post_individual = df_individual[df_individual['post_text'] == post_text]\n",
    "        if len(post_individual) > 0:\n",
    "            total_reactions = len(post_individual[post_individual['reaction_type'] != 'none'])\n",
    "            comment_volume = post_individual['commented'].sum()\n",
    "            \n",
    "            # Role distribution (simplified - percentage of each role)\n",
    "            role_counts = post_individual['commenter_role'].value_counts()\n",
    "            total_people = len(post_individual)\n",
    "            engineer_pct = role_counts[role_counts.index.isin(['Software Engineer', 'Data Scientist', 'CTO'])].sum() / total_people if total_people > 0 else 0\n",
    "            pm_pct = role_counts[role_counts.index.isin(['Product Manager'])].sum() / total_people if total_people > 0 else 0\n",
    "            founder_pct = role_counts[role_counts.index.isin(['Founder', 'CEO'])].sum() / total_people if total_people > 0 else 0\n",
    "            \n",
    "            # Engagement quality score (simple heuristic: reactions + comments + positive sentiment)\n",
    "            avg_sentiment = post_individual[post_individual['commented'] == 1]['comment_sentiment'].mean() if post_individual['commented'].sum() > 0 else 0\n",
    "            engagement_quality = (total_reactions / len(post_individual)) * 0.4 + \\\n",
    "                               (comment_volume / len(post_individual)) * 0.4 + \\\n",
    "                               (max(0, avg_sentiment)) * 0.2\n",
    "            \n",
    "            targets_list.append([\n",
    "                total_reactions,\n",
    "                comment_volume,\n",
    "                engineer_pct,\n",
    "                pm_pct,\n",
    "                founder_pct,\n",
    "                engagement_quality\n",
    "            ])\n",
    "        else:\n",
    "            targets_list.append([0, 0, 0, 0, 0, 0])\n",
    "    \n",
    "    # Combine features\n",
    "    post_embeddings_array = np.array(post_features_list)\n",
    "    aggregated_features_array = np.array(aggregated_features_list)\n",
    "    \n",
    "    X_b = np.hstack([post_embeddings_array, aggregated_features_array])\n",
    "    y_b = np.array(targets_list)\n",
    "    \n",
    "    feature_info_b = {\n",
    "        'embedding_dim': post_embeddings_array.shape[1],\n",
    "        'aggregated_dim': aggregated_features_array.shape[1],\n",
    "        'total_dim': X_b.shape[1],\n",
    "        'text_embeddings_dict': text_embeddings_dict_b\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Feature matrix shape: {X_b.shape}\")\n",
    "    print(f\"  - Post embeddings: {post_embeddings_array.shape[1]} dims\")\n",
    "    print(f\"  - Aggregated Model A features: {aggregated_features_array.shape[1]} dims\")\n",
    "    print(f\"\\\\nTarget variables: total_reactions, comment_volume, engineer_pct, pm_pct, founder_pct, engagement_quality\")\n",
    "    \n",
    "    return X_b, y_b, feature_info_b\n",
    "\n",
    "# Prepare Model B features\n",
    "X_b, y_b, feature_info_b = prepare_model_b_features(df, model_a, feature_info_a, generate_embeddings)\n",
    "print(f\"\\\\nTarget matrix shape: {y_b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for Model B\n",
    "X_b_train, X_b_test, y_b_train, y_b_test = train_test_split(\n",
    "    X_b, y_b, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Model B Training set: {X_b_train.shape[0]} samples\")\n",
    "print(f\"Model B Test set: {X_b_test.shape[0]} samples\")\n",
    "\n",
    "# Train Model B\n",
    "print(\"\\\\nTraining Model B (post-level aggregation model)...\")\n",
    "\n",
    "base_model_b = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_b = MultiOutputRegressor(base_model_b)\n",
    "\n",
    "model_b.fit(X_b_train, y_b_train)\n",
    "print(\"✓ Model B training complete\")\n",
    "\n",
    "# Make predictions\n",
    "y_b_train_pred = model_b.predict(X_b_train)\n",
    "y_b_test_pred = model_b.predict(X_b_test)\n",
    "\n",
    "print(f\"\\\\nPredictions shape: {y_b_test_pred.shape}\")\n",
    "\n",
    "# Target names for Model B\n",
    "target_names_b = ['total_reactions', 'comment_volume', 'engineer_pct', 'pm_pct', 'founder_pct', 'engagement_quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model B\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL B EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate metrics for each target\n",
    "b_metrics = []\n",
    "for i, target_name in enumerate(target_names_b):\n",
    "    y_true_i = y_b_test[:, i]\n",
    "    y_pred_i = y_b_test_pred[:, i]\n",
    "    \n",
    "    mae = mean_absolute_error(y_true_i, y_pred_i)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_i, y_pred_i))\n",
    "    r2 = r2_score(y_true_i, y_pred_i)\n",
    "    \n",
    "    b_metrics.append({\n",
    "        'target': target_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2\n",
    "    })\n",
    "\n",
    "b_metrics_df = pd.DataFrame(b_metrics)\n",
    "print(\"\\\\nTest Set Metrics:\")\n",
    "print(b_metrics_df.to_string(index=False))\n",
    "\n",
    "# Visualize Model B performance\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target_name in enumerate(target_names_b):\n",
    "    ax = axes[i]\n",
    "    y_true_i = y_b_test[:, i]\n",
    "    y_pred_i = y_b_test_pred[:, i]\n",
    "    \n",
    "    ax.scatter(y_true_i, y_pred_i, alpha=0.6, s=50)\n",
    "    min_val = min(y_true_i.min(), y_pred_i.min())\n",
    "    max_val = max(y_true_i.max(), y_pred_i.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    r2 = r2_score(y_true_i, y_pred_i)\n",
    "    ax.set_xlabel(f'Actual {target_name}')\n",
    "    ax.set_ylabel(f'Predicted {target_name}')\n",
    "    ax.set_title(f'{target_name}\\\\nR² = {r2:.3f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-End Prediction Pipeline\n",
    "\n",
    "Combine Model A and Model B for complete post engagement prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_post_engagement_pipeline(\n",
    "    post_text: str,\n",
    "    post_metadata: Dict,\n",
    "    model_a: Dict,\n",
    "    model_b: MultiOutputRegressor,\n",
    "    feature_info_a: Dict,\n",
    "    feature_info_b: Dict,\n",
    "    generate_embeddings_func\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    End-to-end prediction pipeline: Model A → Aggregate → Model B.\n",
    "    \n",
    "    Args:\n",
    "        post_text: The post text to analyze\n",
    "        post_metadata: Dictionary with post metadata (topic, author_role, time_posted, etc.)\n",
    "        model_a: Trained Model A dictionary\n",
    "        model_b: Trained Model B (MultiOutputRegressor)\n",
    "        feature_info_a: Feature info from Model A training\n",
    "        feature_info_b: Feature info from Model B training\n",
    "        generate_embeddings_func: Function to generate embeddings\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - individual_predictions: Model A predictions by role\n",
    "        - aggregated_features: Aggregated Model A features\n",
    "        - post_level_predictions: Model B predictions\n",
    "    \"\"\"\n",
    "    # Step 1: Get role archetypes\n",
    "    role_archetypes = sorted(df_individual['commenter_role'].unique().tolist())\n",
    "    \n",
    "    # Step 2: Use Model A to predict for each role\n",
    "    aggregated = aggregate_model_a_predictions(\n",
    "        post_text=post_text,\n",
    "        post_metadata=post_metadata,\n",
    "        role_archetypes=role_archetypes,\n",
    "        model_a=model_a,\n",
    "        feature_info_a=feature_info_a,\n",
    "        generate_embeddings_func=generate_embeddings_func\n",
    "    )\n",
    "    \n",
    "    # Step 3: Prepare features for Model B\n",
    "    post_text_clean = post_text.lower().strip()\n",
    "    \n",
    "    # Get or generate post embedding\n",
    "    if post_text in feature_info_b['text_embeddings_dict']:\n",
    "        post_embedding = feature_info_b['text_embeddings_dict'][post_text]\n",
    "    else:\n",
    "        post_embedding = generate_embeddings_func([post_text_clean])[0]\n",
    "    \n",
    "    # Flatten aggregated features (same as in training)\n",
    "    aggregated_features = [\n",
    "        aggregated['pct_engineers_reacting'],\n",
    "        aggregated['pct_pms_commenting'],\n",
    "        aggregated['pct_founders_engaging'],\n",
    "        aggregated['predicted_comment_volume']\n",
    "    ]\n",
    "    \n",
    "    # Add reaction distribution\n",
    "    reaction_types = ['like', 'love', 'insightful', 'celebrate', 'none']\n",
    "    for rt in reaction_types:\n",
    "        aggregated_features.append(aggregated['predicted_reaction_distribution'].get(rt, 0.0))\n",
    "    \n",
    "    # Add average sentiment\n",
    "    role_sentiments = aggregated['avg_sentiment_per_role']\n",
    "    for role in role_archetypes[:5]:\n",
    "        aggregated_features.append(role_sentiments.get(role, 0.0))\n",
    "    \n",
    "    # Combine features\n",
    "    X_b_input = np.hstack([post_embedding.reshape(1, -1), np.array([aggregated_features])])\n",
    "    \n",
    "    # Step 4: Use Model B to predict post-level metrics\n",
    "    post_level_pred = model_b.predict(X_b_input)[0]\n",
    "    \n",
    "    return {\n",
    "        'individual_predictions': aggregated,\n",
    "        'aggregated_features': aggregated_features,\n",
    "        'post_level_predictions': {\n",
    "            'total_reactions': post_level_pred[0],\n",
    "            'comment_volume': post_level_pred[1],\n",
    "            'engineer_pct': post_level_pred[2],\n",
    "            'pm_pct': post_level_pred[3],\n",
    "            'founder_pct': post_level_pred[4],\n",
    "            'engagement_quality': post_level_pred[5]\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"✓ End-to-end prediction pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pipeline with a sample post\n",
    "test_post = \"Excited to share our latest AI breakthrough! This technology will revolutionize how we work.\"\n",
    "test_metadata = {\n",
    "    'topic': 'AI',\n",
    "    'author_role': 'CEO',\n",
    "    'time_posted': 'morning',\n",
    "    'post_length': len(test_post.split()),\n",
    "    'hashtags': False,\n",
    "    'mentions': False,\n",
    "    'urls_present': False\n",
    "}\n",
    "\n",
    "print(\"Testing end-to-end pipeline...\")\n",
    "print(f\"Post: {test_post}\")\n",
    "print(\"\\\\n\" + \"=\" * 70)\n",
    "\n",
    "result = predict_post_engagement_pipeline(\n",
    "    post_text=test_post,\n",
    "    post_metadata=test_metadata,\n",
    "    model_a=model_a,\n",
    "    model_b=model_b,\n",
    "    feature_info_a=feature_info_a,\n",
    "    feature_info_b=feature_info_b,\n",
    "    generate_embeddings_func=generate_embeddings\n",
    ")\n",
    "\n",
    "print(\"\\\\nINDIVIDUAL-LEVEL PREDICTIONS (Model A):\")\n",
    "print(f\"  Engineers reacting: {result['individual_predictions']['pct_engineers_reacting']:.1%}\")\n",
    "print(f\"  PMs commenting: {result['individual_predictions']['pct_pms_commenting']:.1%}\")\n",
    "print(f\"  Founders engaging: {result['individual_predictions']['pct_founders_engaging']:.1%}\")\n",
    "print(f\"  Predicted comment volume: {result['individual_predictions']['predicted_comment_volume']:.1f}\")\n",
    "print(f\"\\\\n  Reaction distribution:\")\n",
    "for reaction, prob in result['individual_predictions']['predicted_reaction_distribution'].items():\n",
    "    print(f\"    {reaction}: {prob:.1%}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 70)\n",
    "print(\"POST-LEVEL PREDICTIONS (Model B):\")\n",
    "for key, value in result['post_level_predictions'].items():\n",
    "    if 'pct' in key:\n",
    "        print(f\"  {key}: {value:.1%}\")\n",
    "    elif 'quality' in key:\n",
    "        print(f\"  {key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:.1f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the pipeline flow and results\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Create a 2x2 grid\n",
    "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Role-based reaction predictions (Model A output)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "reaction_dist = result['individual_predictions']['predicted_reaction_distribution']\n",
    "reactions = list(reaction_dist.keys())\n",
    "probs = list(reaction_dist.values())\n",
    "colors_map = {'like': '#3498db', 'love': '#e74c3c', 'insightful': '#2ecc71', 'celebrate': '#f39c12', 'none': '#95a5a6'}\n",
    "colors = [colors_map.get(r, '#95a5a6') for r in reactions]\n",
    "ax1.bar(reactions, probs, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Probability', fontsize=12)\n",
    "ax1.set_title('Model A: Predicted Reaction Distribution', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim(0, max(probs) * 1.2)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, (r, p) in enumerate(zip(reactions, probs)):\n",
    "    ax1.text(i, p, f'{p:.1%}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 2. Role engagement percentages\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "role_metrics = {\n",
    "    'Engineers': result['individual_predictions']['pct_engineers_reacting'],\n",
    "    'PMs': result['individual_predictions']['pct_pms_commenting'],\n",
    "    'Founders': result['individual_predictions']['pct_founders_engaging']\n",
    "}\n",
    "roles = list(role_metrics.keys())\n",
    "values = list(role_metrics.values())\n",
    "ax2.barh(roles, values, color=['#3498db', '#2ecc71', '#9b59b6'], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_xlabel('Engagement Percentage', fontsize=12)\n",
    "ax2.set_title('Model A: Role Engagement Rates', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "for i, (role, val) in enumerate(zip(roles, values)):\n",
    "    ax2.text(val, i, f'{val:.1%}', va='center', ha='left', fontsize=11, fontweight='bold', \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 3. Post-level predictions (Model B output)\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "post_preds = result['post_level_predictions']\n",
    "metrics_to_plot = {\n",
    "    'Total Reactions': post_preds['total_reactions'],\n",
    "    'Comment Volume': post_preds['comment_volume'],\n",
    "    'Engagement Quality': post_preds['engagement_quality'] * 100  # Scale for visibility\n",
    "}\n",
    "metric_names = list(metrics_to_plot.keys())\n",
    "metric_values = list(metrics_to_plot.values())\n",
    "colors_metrics = ['#e74c3c', '#3498db', '#2ecc71']\n",
    "bars = ax3.bar(metric_names, metric_values, color=colors_metrics, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax3.set_ylabel('Value', fontsize=12)\n",
    "ax3.set_title('Model B: Post-Level Predictions', fontsize=13, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars, metric_values):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.1f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Role distribution (Model B output)\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "role_dist = {\n",
    "    'Engineers': post_preds['engineer_pct'],\n",
    "    'PMs': post_preds['pm_pct'],\n",
    "    'Founders': post_preds['founder_pct']\n",
    "}\n",
    "role_names = list(role_dist.keys())\n",
    "role_values = list(role_dist.values())\n",
    "# Add \"Others\" to make it sum to 1\n",
    "others = 1 - sum(role_values)\n",
    "if others > 0:\n",
    "    role_names.append('Others')\n",
    "    role_values.append(others)\n",
    "colors_pie = ['#3498db', '#2ecc71', '#9b59b6', '#95a5a6']\n",
    "ax4.pie(role_values, labels=role_names, autopct='%1.1f%%', colors=colors_pie[:len(role_values)],\n",
    "        startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "ax4.set_title('Model B: Predicted Role Distribution', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Two-Model Pipeline: Model A → Model B Predictions', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Two-Model Architecture\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "1. **Interpretability**: Model A shows which roles react how, making predictions explainable\n",
    "2. **Granularity**: Individual-level predictions can be aggregated in different ways\n",
    "3. **Flexibility**: Can adjust role archetypes or add new ones without retraining Model B\n",
    "4. **Scalability**: Model A predictions can be cached and reused for multiple posts\n",
    "\n",
    "**Model A** learns: \"How do different role archetypes react to posts?\"\n",
    "**Model B** learns: \"Which posts generate valuable engagement patterns?\"\n",
    "\n",
    "**Pipeline**: Post → Model A (role predictions) → Aggregate → Model B → Post-level metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Your Draft Post\n",
    "\n",
    "Use this section to test how your draft post might perform! Simply paste your post text below and optionally adjust the metadata. The model will predict engagement metrics based on the trained patterns.\n",
    "\n",
    "**Instructions:**\n",
    "1. Paste your draft post text in the `test_post_text` variable below\n",
    "2. Optionally adjust persona and context metadata (or leave as None to use defaults from training data)\n",
    "3. Run all cells in this section to see predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_post_text = (\n",
    "    \"I've seen the chatter. It’s okay. Conversations like this usually come from people who care about the same ecosystem we do.\\n\\n\"\n",
    "    \"The message I shared recently was meant for founders and builders in Australia. The point wasn’t about working till 11:30pm. \"\n",
    "    \"It was about ambition and what it takes to close the gap between what’s possible here and what’s already happening in the world.\\n\\n\"\n",
    "    \"Lyra was built for people who want to build. Many of our engineers dream of starting companies, and our role is to create a support system \"\n",
    "    \"with revenue, environment, and a community so they can pursue these ambitions (with the highest chance of success).\\n\\n\"\n",
    "    \"I believe drive looks different for everyone. Some want balance, some want to push limits. Both belong here.\\n\\n\"\n",
    "    \"Im proud of what Lyra's building. From engineers mentoring each other weekly, to clients trusting us with complex products, \"\n",
    "    \"to side projects turning into startups.\\n\\n\"\n",
    "    \"Im grateful to everyone who’s part of this journey. The mission stays the same: build, teach, and grow.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your draft post text here\n",
    "# test_post_text = \"Excited to share my latest insights on AI! This technology is revolutionizing the industry and changing how we work.\"\n",
    "\n",
    "# Optional: Persona metadata (set to None to use defaults from training data)\n",
    "test_job_role = None  # e.g., \"Software Engineer\", \"Product Manager\", \"CEO\"\n",
    "test_affiliation = None  # e.g., \"Tech Corp\", \"StartupXYZ\", \"BigTech Inc\"\n",
    "test_account_age = None  # Account age in days (e.g., 365 for 1 year)\n",
    "\n",
    "# Optional: Context metadata (set to None to use defaults from training data)\n",
    "test_audience_size = None  # Typical audience size (e.g., 10000)\n",
    "test_baseline_engagement = None  # Baseline engagement rate (e.g., 0.05 for 5%)\n",
    "test_time_window = None  # Time window: \"morning\", \"afternoon\", \"evening\", \"weekend\"\n",
    "\n",
    "# Calculate defaults from training data if not provided\n",
    "if test_job_role is None:\n",
    "    test_job_role = df['job_role'].mode()[0] if len(df) > 0 else \"Software Engineer\"\n",
    "if test_affiliation is None:\n",
    "    test_affiliation = df['affiliation'].mode()[0] if len(df) > 0 else \"Tech Corp\"\n",
    "if test_account_age is None:\n",
    "    test_account_age = int(df['account_age'].median()) if len(df) > 0 else 365\n",
    "if test_audience_size is None:\n",
    "    test_audience_size = int(df['audience_size'].median()) if len(df) > 0 else 10000\n",
    "if test_baseline_engagement is None:\n",
    "    test_baseline_engagement = float(df['baseline_engagement'].median()) if len(df) > 0 else 0.05\n",
    "if test_time_window is None:\n",
    "    test_time_window = df['time_window'].mode()[0] if len(df) > 0 else \"morning\"\n",
    "\n",
    "print(\"Test Configuration:\")\n",
    "print(f\"  Post text: {test_post_text[:100]}...\" if len(test_post_text) > 100 else f\"  Post text: {test_post_text}\")\n",
    "print(f\"  Job role: {test_job_role}\")\n",
    "print(f\"  Affiliation: {test_affiliation}\")\n",
    "print(f\"  Account age: {test_account_age} days\")\n",
    "print(f\"  Audience size: {test_audience_size:,}\")\n",
    "print(f\"  Baseline engagement: {test_baseline_engagement:.2%}\")\n",
    "print(f\"  Time window: {test_time_window}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_single_post_features(\n",
    "    post_text: str,\n",
    "    job_role: str,\n",
    "    affiliation: str,\n",
    "    account_age: int,\n",
    "    audience_size: int,\n",
    "    baseline_engagement: float,\n",
    "    time_window: str,\n",
    "    feature_info: Dict,\n",
    "    generate_embeddings_func\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare features for a single post using the same preprocessing pipeline as training.\n",
    "    \n",
    "    Args:\n",
    "        post_text: The post text to analyze\n",
    "        job_role: Job role of the poster\n",
    "        affiliation: Affiliation of the poster\n",
    "        account_age: Account age in days\n",
    "        audience_size: Typical audience size\n",
    "        baseline_engagement: Baseline engagement rate\n",
    "        time_window: Time window when post is published\n",
    "        feature_info: Dictionary containing encoders and scalers from training\n",
    "        generate_embeddings_func: Function to generate embeddings\n",
    "    \n",
    "    Returns:\n",
    "        Feature vector ready for model prediction\n",
    "    \"\"\"\n",
    "    # Clean and normalize text\n",
    "    post_text_clean = post_text.lower().strip()\n",
    "    \n",
    "    # Generate embeddings for the post text\n",
    "    text_embeddings = generate_embeddings_func([post_text_clean])\n",
    "    \n",
    "    # Prepare metadata as DataFrame for encoding\n",
    "    metadata_df = pd.DataFrame({\n",
    "        'job_role': [job_role],\n",
    "        'affiliation': [affiliation],\n",
    "        'time_window': [time_window],\n",
    "        'account_age': [account_age],\n",
    "        'audience_size': [audience_size],\n",
    "        'baseline_engagement': [baseline_engagement]\n",
    "    })\n",
    "    \n",
    "    # Handle unseen categorical values\n",
    "    ohe = feature_info['one_hot_encoder']\n",
    "    try:\n",
    "        # Try to encode with existing encoder\n",
    "        categorical_features = ohe.transform(metadata_df[['job_role', 'affiliation', 'time_window']])\n",
    "    except ValueError as e:\n",
    "        # Handle unseen categories by using most common values from training\n",
    "        print(f\"Warning: Unseen category detected. Using defaults from training data.\")\n",
    "        # Get most common values from training data\n",
    "        default_job = df['job_role'].mode()[0]\n",
    "        default_affiliation = df['affiliation'].mode()[0]\n",
    "        default_time = df['time_window'].mode()[0]\n",
    "        \n",
    "        metadata_df['job_role'] = [default_job if job_role not in df['job_role'].values else job_role]\n",
    "        metadata_df['affiliation'] = [default_affiliation if affiliation not in df['affiliation'].values else affiliation]\n",
    "        metadata_df['time_window'] = [default_time if time_window not in df['time_window'].values else time_window]\n",
    "        \n",
    "        categorical_features = ohe.transform(metadata_df[['job_role', 'affiliation', 'time_window']])\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = feature_info['scaler']\n",
    "    numerical_features = scaler.transform(\n",
    "        metadata_df[['account_age', 'audience_size', 'baseline_engagement']]\n",
    "    )\n",
    "    \n",
    "    # Combine all features: embeddings + categorical + numerical\n",
    "    X = np.hstack([\n",
    "        text_embeddings,\n",
    "        categorical_features,\n",
    "        numerical_features\n",
    "    ])\n",
    "    \n",
    "    return X\n",
    "\n",
    "print(\"✓ Preprocessing function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model and feature_info are available\n",
    "try:\n",
    "    if 'model' not in globals() or 'feature_info' not in globals():\n",
    "        raise ValueError(\"Model not trained yet. Please run the training cells first.\")\n",
    "    \n",
    "    # Prepare features for the test post\n",
    "    print(\"Preprocessing test post...\")\n",
    "    X_test_post = prepare_single_post_features(\n",
    "        post_text=test_post_text,\n",
    "        job_role=test_job_role,\n",
    "        affiliation=test_affiliation,\n",
    "        account_age=test_account_age,\n",
    "        audience_size=test_audience_size,\n",
    "        baseline_engagement=test_baseline_engagement,\n",
    "        time_window=test_time_window,\n",
    "        feature_info=feature_info,\n",
    "        generate_embeddings_func=generate_embeddings\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Features prepared: {X_test_post.shape}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\\\nMaking predictions...\")\n",
    "    predictions = model.predict(X_test_post)\n",
    "    \n",
    "    # Extract individual predictions\n",
    "    pct_positive = predictions[0][0]\n",
    "    pct_negative = predictions[0][1]\n",
    "    comment_sentiment_dist = predictions[0][2]\n",
    "    engagement_velocity = predictions[0][3]\n",
    "    \n",
    "    # Clip values to valid ranges\n",
    "    pct_positive = np.clip(pct_positive, 0, 100)\n",
    "    pct_negative = np.clip(pct_negative, 0, 100)\n",
    "    comment_sentiment_dist = np.clip(comment_sentiment_dist, -1, 1)\n",
    "    engagement_velocity = np.clip(engagement_velocity, 0, 1)\n",
    "    \n",
    "    print(\"✓ Predictions complete\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\\\nMake sure you have:\")\n",
    "    print(\"  1. Run all cells up to and including the model training section\")\n",
    "    print(\"  2. The 'model' and 'feature_info' variables are available\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a formatted way\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICTED ENGAGEMENT METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Format and display each metric\n",
    "print(f\"📈 % Positive Reactions: {pct_positive:.1f}%\")\n",
    "if pct_positive >= 40:\n",
    "    print(\"   → Excellent! This post is predicted to receive strong positive engagement.\")\n",
    "elif pct_positive >= 25:\n",
    "    print(\"   → Good! This post should perform well with positive reactions.\")\n",
    "elif pct_positive >= 15:\n",
    "    print(\"   → Moderate positive engagement expected.\")\n",
    "else:\n",
    "    print(\"   → Lower positive engagement predicted. Consider refining the content.\")\n",
    "print()\n",
    "\n",
    "print(f\"📉 % Negative Reactions: {pct_negative:.1f}%\")\n",
    "if pct_negative <= 5:\n",
    "    print(\"   → Very low negative reactions - great!\")\n",
    "elif pct_negative <= 10:\n",
    "    print(\"   → Low negative reactions - acceptable.\")\n",
    "elif pct_negative <= 20:\n",
    "    print(\"   → Moderate negative reactions - some risk of controversy.\")\n",
    "else:\n",
    "    print(\"   → High negative reactions predicted. Content may be controversial.\")\n",
    "print()\n",
    "\n",
    "print(f\"💬 Comment Sentiment Distribution: {comment_sentiment_dist:.3f}\")\n",
    "print(\"   (Range: -1 = very negative, 0 = neutral, +1 = very positive)\")\n",
    "if comment_sentiment_dist >= 0.5:\n",
    "    print(\"   → Comments are predicted to be very positive!\")\n",
    "elif comment_sentiment_dist >= 0.2:\n",
    "    print(\"   → Comments should be generally positive.\")\n",
    "elif comment_sentiment_dist >= -0.2:\n",
    "    print(\"   → Comments expected to be neutral to mixed.\")\n",
    "elif comment_sentiment_dist >= -0.5:\n",
    "    print(\"   → Comments may be somewhat negative.\")\n",
    "else:\n",
    "    print(\"   → Comments predicted to be negative. Consider revising.\")\n",
    "print()\n",
    "\n",
    "print(f\"⚡ Engagement Velocity: {engagement_velocity:.3f}\")\n",
    "print(\"   (Range: 0 = slow/late engagement, 1 = fast/early engagement)\")\n",
    "if engagement_velocity >= 0.7:\n",
    "    print(\"   → High velocity! Engagement is predicted to happen quickly.\")\n",
    "elif engagement_velocity >= 0.5:\n",
    "    print(\"   → Moderate velocity - steady engagement expected.\")\n",
    "elif engagement_velocity >= 0.3:\n",
    "    print(\"   → Lower velocity - engagement may build gradually.\")\n",
    "else:\n",
    "    print(\"   → Slow velocity - engagement may take time to develop.\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Predictions with Actual Results\n",
    "\n",
    "Analyze how the model's predictions compare to the actual engagement from real comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze actual comments from the post\n",
    "# Based on the comments provided, categorize them\n",
    "\n",
    "actual_comments_analysis = {\n",
    "    'positive': [\n",
    "        \"Ankit Talwar - Supportive, understands the context\",\n",
    "        \"Humza Rafiq - Love this perspective, positive\",\n",
    "        \"Brenton Andersen - Supportive of ambition\",\n",
    "        \"Smit Bhatt - Talent grows fastest when culture doesn't shame ambition\",\n",
    "        \"Ryan Zahrai - Strong support, defends founder culture\",\n",
    "        \"Simon Liu - Supporting you, understands timezones\",\n",
    "        \"Harry M.D. Nguyen - Supports Lyra's culture and mission\",\n",
    "        \"Shubham P. - Extraordinary goals require extraordinary efforts\",\n",
    "        \"Edward J Roland III - Keep grinding\",\n",
    "        \"Jerry X. - Bucket list moment\",\n",
    "        \"Jack Ryan - Supportive, understands enjoyment of work\",\n",
    "        \"Ricki Burke - Supportive, not everyone suited for every company\",\n",
    "        \"Frantz Rigaud - Supportive of impact-focused ambition\",\n",
    "        \"Anh Dao (author reply) - Agrees with sentiment\"\n",
    "    ],\n",
    "    'negative': [\n",
    "        \"Cameron Craig - 'You have torched your career in one post'\",\n",
    "        \"Adam R. - Concerned about worker safety and fatigue\",\n",
    "        \"Adam Heathcote - 'You didn't make that clear and now you are backpeddling'\",\n",
    "        \"Richard Forsythe - 'Terrible post. Terrible outlook. Terrible company.'\",\n",
    "        \"Daniel Pyemont - 'LOL this is hilarious' (sarcastic)\",\n",
    "        \"Avi Ben-Moyal - Negative about applicants after 'shit show'\"\n",
    "    ],\n",
    "    'mixed_neutral': [\n",
    "        \"Simon Monk - Balanced: true for founders but risk of burnout\",\n",
    "        \"Josh O'Dea - Concerned about vulnerable employees (high school grad)\",\n",
    "        \"Chris Rickard - Questioning: equity vs contractors working late\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Count comments\n",
    "n_positive = len(actual_comments_analysis['positive'])\n",
    "n_negative = len(actual_comments_analysis['negative'])\n",
    "n_mixed = len(actual_comments_analysis['mixed_neutral'])\n",
    "n_total = n_positive + n_negative + n_mixed\n",
    "\n",
    "# Calculate percentages\n",
    "actual_pct_positive = (n_positive / n_total * 100) if n_total > 0 else 0\n",
    "actual_pct_negative = (n_negative / n_total * 100) if n_total > 0 else 0\n",
    "actual_pct_mixed = (n_mixed / n_total * 100) if n_total > 0 else 0\n",
    "\n",
    "# Estimate sentiment distribution (positive comments = +1, negative = -1, mixed = 0)\n",
    "actual_sentiment = (n_positive - n_negative) / n_total if n_total > 0 else 0\n",
    "\n",
    "# Engagement velocity: Based on comment timing (most comments within 3-4 weeks suggests moderate velocity)\n",
    "# Comments appeared over 3-4 weeks, so moderate velocity\n",
    "actual_velocity = 0.4  # Estimated based on comment spread\n",
    "\n",
    "print(\"ACTUAL ENGAGEMENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total comments analyzed: {n_total}\")\n",
    "print(f\"  Positive: {n_positive} ({actual_pct_positive:.1f}%)\")\n",
    "print(f\"  Negative: {n_negative} ({actual_pct_negative:.1f}%)\")\n",
    "print(f\"  Mixed/Neutral: {n_mixed} ({actual_pct_mixed:.1f}%)\")\n",
    "print(f\"\\\\nComment Sentiment Score: {actual_sentiment:.3f}\")\n",
    "print(f\"Engagement Velocity (estimated): {actual_velocity:.3f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions vs actual\n",
    "print(\"\\\\n\" + \"=\" * 70)\n",
    "print(\"PREDICTED vs ACTUAL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "comparison_data = {\n",
    "    'Metric': ['% Positive Reactions', '% Negative Reactions', 'Comment Sentiment', 'Engagement Velocity'],\n",
    "    'Predicted': [pct_positive, pct_negative, comment_sentiment_dist, engagement_velocity],\n",
    "    'Actual': [actual_pct_positive, actual_pct_negative, actual_sentiment, actual_velocity],\n",
    "    'Difference': [\n",
    "        pct_positive - actual_pct_positive,\n",
    "        pct_negative - actual_pct_negative,\n",
    "        comment_sentiment_dist - actual_sentiment,\n",
    "        engagement_velocity - actual_velocity\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df['Abs_Error'] = comparison_df['Difference'].abs()\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae_positive = abs(pct_positive - actual_pct_positive)\n",
    "mae_negative = abs(pct_negative - actual_pct_negative)\n",
    "mae_sentiment = abs(comment_sentiment_dist - actual_sentiment)\n",
    "mae_velocity = abs(engagement_velocity - actual_velocity)\n",
    "\n",
    "print(\"\\\\nPrediction Accuracy:\")\n",
    "print(f\"  % Positive Error: {mae_positive:.1f} percentage points\")\n",
    "print(f\"  % Negative Error: {mae_negative:.1f} percentage points\")\n",
    "print(f\"  Sentiment Error: {mae_sentiment:.3f}\")\n",
    "print(f\"  Velocity Error: {mae_velocity:.3f}\")\n",
    "\n",
    "# Overall assessment\n",
    "print(\"\\\\n\" + \"=\" * 70)\n",
    "print(\"ASSESSMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if mae_positive < 15 and mae_negative < 10:\n",
    "    print(\"✓ Model predictions are reasonably close to actual results!\")\n",
    "elif mae_positive < 25 and mae_negative < 15:\n",
    "    print(\"⚠ Model predictions are somewhat close, but there's room for improvement.\")\n",
    "else:\n",
    "    print(\"✗ Model predictions differ significantly from actual results.\")\n",
    "\n",
    "print(\"\\\\nKey Observations:\")\n",
    "print(f\"  • Model predicted {pct_positive:.1f}% positive, actual was {actual_pct_positive:.1f}%\")\n",
    "print(f\"  • Model predicted {pct_negative:.1f}% negative, actual was {actual_pct_negative:.1f}%\")\n",
    "print(f\"  • The post generated significant discussion and mixed reactions\")\n",
    "print(f\"  • Some comments were highly polarized (very positive or very negative)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Positive Reactions Comparison\n",
    "ax1 = axes[0, 0]\n",
    "categories = ['Predicted', 'Actual']\n",
    "values = [pct_positive, actual_pct_positive]\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "bars1 = ax1.bar(categories, values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Percentage (%)', fontsize=12)\n",
    "ax1.set_title('% Positive Reactions: Predicted vs Actual', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylim(0, max(values) * 1.2)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars1, values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 2. Negative Reactions Comparison\n",
    "ax2 = axes[0, 1]\n",
    "values2 = [pct_negative, actual_pct_negative]\n",
    "colors2 = ['#3498db', '#e74c3c']\n",
    "bars2 = ax2.bar(categories, values2, color=colors2, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Percentage (%)', fontsize=12)\n",
    "ax2.set_title('% Negative Reactions: Predicted vs Actual', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim(0, max(values2) * 1.2)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars2, values2):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 3. Sentiment Comparison\n",
    "ax3 = axes[1, 0]\n",
    "values3 = [comment_sentiment_dist, actual_sentiment]\n",
    "bars3 = ax3.bar(categories, values3, color=['#3498db', '#9b59b6'], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax3.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax3.set_ylabel('Sentiment Score', fontsize=12)\n",
    "ax3.set_title('Comment Sentiment: Predicted vs Actual', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylim(-1, 1)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars3, values3):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.3f}', ha='center', va='bottom' if val >= 0 else 'top', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 4. Velocity Comparison\n",
    "ax4 = axes[1, 1]\n",
    "values4 = [engagement_velocity, actual_velocity]\n",
    "bars4 = ax4.bar(categories, values4, color=['#3498db', '#f39c12'], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax4.set_ylabel('Velocity Score', fontsize=12)\n",
    "ax4.set_title('Engagement Velocity: Predicted vs Actual', fontsize=13, fontweight='bold')\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars4, values4):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from Actual Comments\n",
    "\n",
    "**Key Themes in Comments:**\n",
    "\n",
    "1. **Supportive Comments** focused on:\n",
    "   - Understanding ambition and drive\n",
    "   - Supporting founder culture\n",
    "   - Recognizing different work styles\n",
    "   - Appreciating the clarification about balance\n",
    "\n",
    "2. **Critical Comments** raised concerns about:\n",
    "   - Worker safety and burnout risks\n",
    "   - Equity vs contractor compensation\n",
    "   - Vulnerable employees (e.g., high school graduates)\n",
    "   - Communication clarity (perception of backpedaling)\n",
    "\n",
    "3. **Mixed/Neutral Comments** provided:\n",
    "   - Balanced perspectives on founder vs employee expectations\n",
    "   - Questions about the business model (contractors vs equity holders)\n",
    "   - Recognition of timezone challenges in global companies\n",
    "\n",
    "**Model Performance Notes:**\n",
    "- The model predicted lower negative reactions (6.6%) than actual (26.1%)\n",
    "- This suggests the model may underestimate controversy in posts that address work culture\n",
    "- The sentiment prediction (0.381) was close to actual (0.348), showing good sentiment understanding\n",
    "- The post generated significant discussion, which the model partially captured through engagement velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Positive vs Negative Reactions\n",
    "ax1 = axes[0, 0]\n",
    "categories = ['Positive', 'Negative']\n",
    "values = [pct_positive, pct_negative]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax1.bar(categories, values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('Percentage (%)', fontsize=12)\n",
    "ax1.set_title('Predicted Reaction Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, max(values) * 1.2)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.1f}%',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 2. Comment Sentiment\n",
    "ax2 = axes[0, 1]\n",
    "sentiment_range = np.linspace(-1, 1, 100)\n",
    "sentiment_colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in sentiment_range]\n",
    "ax2.barh([0], [comment_sentiment_dist], height=0.3, color='#3498db', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax2.axvline(x=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.set_xlim(-1, 1)\n",
    "ax2.set_xlabel('Sentiment Score', fontsize=12)\n",
    "ax2.set_title('Comment Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_yticks([])\n",
    "ax2.text(comment_sentiment_dist, 0, f'{comment_sentiment_dist:.3f}',\n",
    "         ha='center' if abs(comment_sentiment_dist) < 0.3 else ('left' if comment_sentiment_dist > 0 else 'right'),\n",
    "         va='bottom', fontsize=11, fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Engagement Velocity\n",
    "ax3 = axes[1, 0]\n",
    "velocity_range = np.linspace(0, 1, 100)\n",
    "ax3.barh([0], [engagement_velocity], height=0.3, color='#9b59b6', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax3.set_xlim(0, 1)\n",
    "ax3.set_xlabel('Velocity Score', fontsize=12)\n",
    "ax3.set_title('Engagement Velocity', fontsize=14, fontweight='bold')\n",
    "ax3.set_yticks([])\n",
    "ax3.text(engagement_velocity, 0, f'{engagement_velocity:.3f}',\n",
    "         ha='center', va='bottom', fontsize=11, fontweight='bold',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. Summary Metrics Table\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "table_data = [\n",
    "    ['Metric', 'Value', 'Interpretation'],\n",
    "    ['% Positive', f'{pct_positive:.1f}%', 'Higher is better'],\n",
    "    ['% Negative', f'{pct_negative:.1f}%', 'Lower is better'],\n",
    "    ['Sentiment', f'{comment_sentiment_dist:.3f}', 'Range: -1 to +1'],\n",
    "    ['Velocity', f'{engagement_velocity:.3f}', 'Range: 0 to 1']\n",
    "]\n",
    "table = ax4.table(cellText=table_data, cellLoc='left', loc='center',\n",
    "                  colWidths=[0.3, 0.25, 0.45])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "# Style header row\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('#34495e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "# Style data rows\n",
    "for i in range(1, 5):\n",
    "    for j in range(3):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#ecf0f1')\n",
    "ax4.set_title('Summary Metrics', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Model Performance Summary\n",
    "- The model has been trained on {len(X_train)} training examples\n",
    "- Test set performance metrics are shown above\n",
    "- High-uncertainty predictions have been identified as potential failure modes\n",
    "\n",
    "### Key Insights\n",
    "1. **Feature Contributions**: Text embeddings capture semantic meaning, while metadata provides context\n",
    "2. **Uncertainty**: The model surfaces uncertainty through error analysis\n",
    "3. **Failure Modes**: High-uncertainty predictions indicate where the model is less confident\n",
    "\n",
    "### Recommendations\n",
    "- Collect more training data, especially for edge cases\n",
    "- Experiment with different embedding models (e.g., gemini-embedding-001)\n",
    "- Tune hyperparameters further using cross-validation\n",
    "- Consider ensemble methods for improved robustness\n",
    "- Monitor model performance on new data and retrain periodically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
